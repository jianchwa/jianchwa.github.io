<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>IRQ</title>
</head>
<body>
<div>
    <h1>IRQ</h1> 
</div>
<p>
<font size="2">
<a href="#Framework">Framework</a>
<ul>
<li><a href="#IRQ_Domain">IRQ Domain</a>
<li><a href="#IRQ_Affinity">IRQ Affinity</a>
</ul>
</font>
</p>

<h2><a name="Framework">Framework</a></h2>
<hr style="height:5px;border:none;border-top:3px solid black;" />
<p>
<font size="2">
<pre>
struct irq_chip
 - A set of methods describing how to drive the interrupt controller
 - Directly called by core IRQ code
struct irqdomain
 - A pointer to the firmware node for a given interrupt controller (fwnode)
 - A method to convert a firmware description of an IRQ into an ID local to this interrupt controller (hwirq)
 - A way to retrieve the Linux view of an IRQ from the hwirq
struct irq_desc
 - Linux’s view of an interrupt
 - Contains all the core stuff
 - 1:1 mapping to the Linux interrupt number
struct irq_data
 - Contains the data that is relevant to the irq_chip managing this interrupt
 - Both the Linux IRQ number and the hwirq
 - A pointer to the irq_chip
 - Embedded in irq_desc (for now)
</pre>
</font>
</p>
<h3><a name="IRQ_Domain">IRQ Domain</a></h3>
<p>
<font size="2">
There are two main reasons for linux kernel to introduce irq_domain.
<ul>
<li> there could be multiple interrupt controllers in system, the single irq number space cannot ensure non-overlapping accross irq numbers of different controllers
<li> irq number has loose all kind of correspondence to hardware interrupt numbers
</ul>
The irq_domain library adds mapping between hwirq and IRQ numbers on top of the irq_alloc_desc*() API <br/>
<br/>

<strong>Where is the irq number in nowadays kernel ?</strong><br/>
<pre>
nr_irqs = NR_IRQS

static DECLARE_BITMAP(allocated_irqs, IRQ_BITMAP_BITS);

struct irq_desc *irq_to_desc(unsigned int irq)
{
	return radix_tree_lookup(&irq_desc_tree, irq);
}
All the irq_desc structures are stored in a radix tree irq_desc_tree <font color="red">indexed by irq number</font>.

</pre>

<strong>A irq domain example on x86 platform</strong>
<pre>
On some architectures, there may be multiple interrupt controllers
involved in delivering an interrupt from the device to the target CPU.
Let's look at a typical interrupt delivering path on x86 platforms::

  Device --> IOAPIC -> Interrupt remapping Controller -> Local APIC -> CPU

There are three interrupt controllers involved:

1) IOAPIC controller
2) Interrupt remapping controller
3) Local APIC controller

To support such a hardware topology and make software architecture match
hardware architecture, an irq_domain data structure is built for each
interrupt controller and those irq_domains are organized into hierarchy.
When building irq_domain hierarchy, the irq_domain near to the device is
child and the irq_domain near to CPU is parent. So a hierarchy structure
as below will be built for the example above::

	CPU Vector irq_domain (root irq_domain to manage CPU vectors)
		^
		|
	Interrupt Remapping irq_domain (manage irq_remapping entries)
		^
		|
	IOAPIC irq_domain (manage IOAPIC delivery entries/pins)
</pre>
Following is the output of irq debugfs about this:
<pre>
domain:  INTEL-IR-MSI-1-2
 hwirq:   0x100003
 chip:    IR-PCI-MSI
  flags:   0x10
             IRQCHIP_SKIP_SET_WAKE
 parent:
    domain:  INTEL-IR-1
     hwirq:   0x160000
     chip:    INTEL-IR
      flags:   0x0
     parent:
        domain:  VECTOR
         hwirq:   0x80
         chip:    APIC
          flags:   0x0
         Vector:    33
         Target:     3
</pre>

</font>
</p>

<h2><a name="IRQ_Affinity">IRQ Affinity</a></h2>
<hr style="height:5px;border:none;border-top:3px solid black;"/>

<p>
<font size="2">

Before look into the irq affinity, let's look at something about the cpumask.
<pre>
The following particular system cpumasks and operations manage
possible, present, active and online cpus.

cpu_possible_mask- has bit ‘cpu’ set iff cpu is populatable
cpu_present_mask – has bit ‘cpu’ set iff cpu is populated
cpu_online_mask – has bit ‘cpu’ set iff cpu available to scheduler
cpu_active_mask – has bit ‘cpu’ set iff cpu available to migration

If !CONFIG_HOTPLUG_CPU, present == possible, and active == online.

The cpu_possible_mask is fixed at boot time, as the set of CPU id’s
that it is possible might ever be plugged in at anytime during the
life of that system boot. The cpu_present_mask is dynamic(*),
representing which CPUs are currently plugged in. And
cpu_online_mask is the dynamic subset of cpu_present_mask,
indicating those CPUs available for scheduling.

If HOTPLUG is enabled, then cpu_possible_mask is forced to have
all NR_CPUS bits set, otherwise it is just the set of CPUs that
ACPI reports present at boot.

If HOTPLUG is enabled, then cpu_present_mask varies dynamically,
depending on what ACPI reports as currently plugged in, otherwise
cpu_present_mask is just a copy of cpu_possible_mask.

(*) Well, cpu_present_mask is dynamic in the hotplug case. If not
hotplug, it’s a copy of cpu_possible_mask, hence fixed at boot.
</pre>

<br/>
<br/>
<br/>

The path to generate the irq affinity masks.
<pre>
 prev       affinity spreading     post
 |- - - | - - - - - - - - - - - - | - - - -|
 
 resv = prev + post

pci_alloc_irq_vectors_affinity
  -> __pci_enable_msix_range
    -> irq_calc_affinity_vectors
    --
    ret = min_t(int, cpumask_weight(cpu_possible_mask), vecs) + resv;
    <font color="red">by default, we prefer one-to-one mapping between hw queue and cpu</font>
    --
    -> __pci_enable_msix
      -> msix_setup_entries
        -> irq_create_affinity_masks
</pre>

Take an example here.
<pre>

4 irq vectors v0,v1,v2,v3.

cpu topology

               Node0                              Node1
{ [core0, core2], [core1, core3]}  { [core0, core2], [core1, core3]}

[] -> siblings

</pre>
There are two stages to spread the irqs.
<ul>
<li> spread the irq vectors across the NUMA nodes
<pre>
     every node will get two irq vectors
     v0,v1  ->  Node0
     v2,v3  ->  Node1
</pre>
<li> spread the irq vectors that have been assigned to a node across the cpus in
     that node
<pre>
     v0  ->  Node0-core0,core2
     v1  ->  Node0-core1,core3
</pre>
</ul>

Before commit 84676c1f21 ("genirq/affinity: assign vectors to all possible CPUs") <br/>
the irq vectors will be spread across the present cpus. So we may get following
scenarios.

<pre>
               Node0                              Node1
{ [core0, core2], [core1, core3]}  { [core0, core2], [core1, core3]}
                                                        X      X
[] -> siblings
X  -> not present

At most, 6 irq vectors could be allocated due to irq_calc_affinity_vectors
v0,v1,v2,v3,v4,v5

</pre>

Let's look at how will the irq vectors be spread.
<pre>
    for_each_node_mask(n, nodemsk) {
        int ncpus, v, vecs_to_assign, vecs_per_node;

        /* Spread the vectors per node */
        vecs_per_node = (affv - (curvec - affd->pre_vectors)) / nodes;
        <font color="red">2 nodes, 6 vectors, vecs_per_node is 3 </font>
        /* Get the cpus on this node which are in the mask */
        cpumask_and(nmsk, cpu_present_mask, node_to_present_cpumask[n]);

        /* Calculate the number of cpus per vector */
        ncpus = cpumask_weight(nmsk);
        <font color="red">node0 has 4 cpus</font>
        vecs_to_assign = min(vecs_per_node, ncpus);
        <font color="red">vecs_to_assign will be 3 here</font>
        /* Account for rounding errors */
        extra_vecs = ncpus - vecs_to_assign * (ncpus / vecs_to_assign);
        <font color="red">extra_vecs is 1</font>
        
        
        for (v = 0; curvec < last_affv && v < vecs_to_assign;
             curvec++, v++) {
            cpus_per_vec = ncpus / vecs_to_assign;
            <font color="red">cpus_per_vec is 1</font>
            /* Account for extra vectors to compensate rounding errors */
            if (extra_vecs) {
                cpus_per_vec++;
                --extra_vecs;
            }
            <font color="red">node0 has 4 cpus, but 3 vectors are assigned to
                it. so 1 vector will have 2 cpu assigned</font>
            
            irq_spread_init_one(masks + curvec, nmsk, cpus_per_vec);
        }

        if (curvec >= last_affv)
            break;
        --nodes;
        <font color="red">
        There are 3 vectors left, but node1 just have 2 cpu present.
        
        vecs_to_assign = min(vecs_per_node, ncpus);

        Just 2 vectors could be assigned with cpu.
        The left one's masks will get irq_default_affinity.
        </font>

    }
done:
    put_online_cpus();

    /* Fill out vectors at the end that don't need affinity */
    for (; curvec < nvecs; curvec++)
        cpumask_copy(masks + curvec, irq_default_affinity);
    free_node_to_possible_cpumask(node_to_possible_cpumask);
out:
    free_cpumask_var(nmsk);
    return masks

</pre>

Here is one case that a patch I sent to upstream causes a big bug when block layer maintainer tested it. <br/>
<pre>
This is the patch https://lkml.org/lkml/2018/3/13/227
What is the issue ?
It employs "pre_vector=1" when allocate irq vectors, and  assign separate irq vectors for adminq and ioq1.
So nvme io queues will get a irq vector <font color="red">which is equal to its qid</font>. But I missed 
the blk_mq_pci_map_queues() which still thinks the <font color="red">irq vector is equal to qid-1</font>.

To reproduce this issue we must add "possible_cpus=16" on the kernel commandline  and boot it on my machine with
8 cores. Then we will get following irq affinity result.
nvme0q1   nvme0q2   nvme0q3    nvme0q4    nvme0q5    nvme0q6     nvme0q7    nvme0q8
0,4       1,5       2,6        3,7        8-9        10-11       12-13      14-15
We could see irqs of nvme0q5~nvme0q8 are set affinity to not present cpu.
This is due to 84676c1f21 ("genirq/affinity: assign vectors to all possible CPUs")
Leiming has submitted a series patch to fix it.
genirq/affinity: irq vector spread among online CPUs as far as possible

Let's return to our issue.
int blk_mq_pci_map_queues(struct blk_mq_tag_set *set, struct pci_dev *pdev)
{
    const struct cpumask *mask;
    unsigned int queue, cpu;
    /*<font color="red">0 based here, the mask for vector 0 contains all the possible cpus.
    so it will do a wrong mapping between ctxs and hctxs.
    </font>*/
    for (queue = 0; queue < set->nr_hw_queues; queue++) {
            mask = pci_irq_get_affinity(pdev, queue);
            if (!mask)
                goto fallback;

        for_each_cpu(cpu, mask)
            set->mq_map[cpu] = queue;
    }

	return 0;
...
}
We get a mapping as following:                             |
                                                           V
ctxs            14,15     0,4       1,5        2,6        3,7        8,9         10,11      12,13
                hctx0     hctx1     hctx2      hctx3      hctx4      hctx5       hctx6      hctx7
                nvme0q1   nvme0q2   nvme0q3    nvme0q4    nvme0q5    nvme0q6     nvme0q7    nvme0q8
cpu affinitys   0,4       1,5       2,6        3,7        8-9        10-11       12-13      14-15

The issue is:
if we submit IO on cpu3 or cpu7, hctx4/nvme0q5 will handle it. After completion, the irq will be sent to 8,9 which are
not present on my machine.
taskset -c 3 dd if=/dev/nvme0n1 of=/dev/null bs=4k count=1
And get following log in dmesg
[  350.800366] nvme nvme0: I/O 253 QID 5 timeout, completion polled
</pre>
</font>
</p

</body>
</html>
