<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Synchronize Primitives</title>
</head>
<body>
<div>
    <h1>Synchronize Primitives</h1> 
</div>

<p>
<font size="2">
<a href="#Percpu_reference_count">Percpu reference count</a><br/>
<a href="#Sequence_lock">Sequence lock</a><br/>
<a href="#ACCESS_ONCE_Series">ACCESS_ONCE Series</a><br/>
<a href="#RCU">RCU</a>
<ul>
<li><a href="#Rcu_Tree">Rcu Tree</a>
<li><a href="#How_does_RCU_run">How does RCU run</a>
<ul>
<li><a href="#Grace_period_and_Quiescent_state">Grace period and Quiescent state</a><br/>
</ul>
<li><a href="#RCU_Usage">RCU Usage</a>
<ul>
<li><a href="#scsi_host_busy">scsi_host_busy</a></h4>
</ul>
</ul>
</font>
</p>


<h2><a name="Percpu_reference_count">Percpu reference count</a></h2>
<hr style="height:5px;border:none;border-top:3px solid black;" />
<p>
<font size="2">
Reference counting is used by the kernel to know when a data structure is unused and can be freed. Most of the time, reference counts are represented by an atomic_t variable, perhaps wrapped by a structure like a kref. If references are added and removed frequently over an object's lifetime, though, that atomic_t variable can become a performance bottleneck.<br/>
<pre>
The frequent write access to the shared reference count variable could result in heavy cache line bouncing between different cpus. 
On other hand, the atomic operations itself could introduce extra overhead in parallel environment.
</pre>
<strong>How does it work</strong><br/>
The percpu reference count was introduced to the 3.11 kernel created by Kent Overstreet. It is to improve the scalability of reference count in parallel environment. Let's look at how does it work next.<br/>
Typical usage will involve embedding a percpu_ref structure within the data structure being tracked. The counter must be initialized with:
<pre>
    int percpu_ref_init(struct percpu_ref *ref, percpu_ref_release *release);
</pre>
Where release() is the function to be called when the reference count drops to zero:
<pre>
    typedef void (percpu_ref_release)(struct percpu_ref *);
</pre>
The call to percpu_ref_init() will initialize the reference count to one. References are added and removed with:
<pre>
    void percpu_ref_get(struct percpu_ref *ref);
    void percpu_ref_put(struct percpu_ref *ref);
</pre>
These two functions will operate on a per-cpu array of reference counters, so they will not cause cacheline bouncing across the system. But the question is how does percpu_ref_put() determine the reference count dropped to zero and invoke the realse callback?<br/>
There is a basic practice in kernel, the reference count will not be zero as long as the initial one held. So percpu_ref_put() needn't to check it until someone decide to drop the initial reference. It's done thought percpu_ref_kill().
<pre>
    void percpu_ref_kill(struct percpu_ref *ref);
</pre>
After this call, the reference count degrades to the usual model with a single atomic_t counter. This atomic_t counter will be decremented and checked by percpu_ref_put().<br/>
<strong>Look at the source code</strong><br/>
The most interesting part of the percpu_ref is how to switch from percpu variable to atomic variable smoothly.<br/>
<pre>
percpu_ref_kill()
    -> percpu_ref_kill_and_confirm()
        -> __percpu_ref_switch_mode() // set __PERCPU_REF_DEAD
            -> __percpu_ref_switch_to_atomic()

static void __percpu_ref_switch_to_atomic(struct percpu_ref *ref,
                      percpu_ref_func_t *confirm_switch)
{
    if (ref->percpu_count_ptr & __PERCPU_REF_ATOMIC) {
        if (confirm_switch)
            confirm_switch(ref);
        return;
    }

    /* switching from percpu to atomic */
    ref->percpu_count_ptr |= __PERCPU_REF_ATOMIC;

    percpu_ref_get(ref);    /* put after confirmation */
    call_rcu_sched(&ref->rcu, percpu_ref_switch_to_atomic_rcu);
}
</pre>
All the reference get/put interface will operate the reference under rcu lock. For example:
<pre>
static inline bool percpu_ref_tryget_live(struct percpu_ref *ref)
{
	unsigned long __percpu *percpu_count;
	bool ret = false;

	rcu_read_lock_sched();

	if (__ref_is_percpu(ref, &percpu_count)) {
		this_cpu_inc(*percpu_count);
		ret = true;
	} else if (!(ref->percpu_count_ptr & __PERCPU_REF_DEAD)) {
		ret = atomic_long_inc_not_zero(&ref->count);
	}

	rcu_read_unlock_sched();

	return ret;
}
</pre>
percpu_ref_switch_to_atomic_rcu() will be invoked in next grace period. At the moment, all the percpu_ref_get/put() and some other similar interfaces will see the __PERCPU_REF_ATOMIC and __PERCPU_REF_DEAD through __ref_is_percpu().<br/>

But there is one question. The percpu_ref_get/put() could race with the rcu
callback percpu_ref_switch_to_atomic_rcu(). How to handle that ?<br/>

To switch percpu to atomic, we have to sum the reference in percpu variable to 
the atomic variable. During this procedure, the percpu_ref_put() could decrement
the atomic reference variable, even drop it to zero and invoke the release function.
To avoid this, there are two actions taken by percpu_ref.
<ul>
<li>give a basis value to the atomic reference count and decrement it after add the percpu reference sum to it. So nobody could drop the atomic reference to zero before add the percpu reference sum to it.
<li>get one reference before  call_rcu_sched() in __percpu_ref_switch_to_atomic() and put it in percpu_ref_call_confirm_rcu. So if all the reference are gone during this procedure, we could invoke the release callback here.
</ul>
Look at the source code:
<pre>
static void percpu_ref_switch_to_atomic_rcu(struct rcu_head *rcu)
{
    struct percpu_ref *ref = container_of(rcu, struct percpu_ref, rcu);
    unsigned long __percpu *percpu_count = percpu_count_ptr(ref);
    unsigned long count = 0;
    int cpu;

    for_each_possible_cpu(cpu)
        count += *per_cpu_ptr(percpu_count, cpu);

    pr_debug("global %ld percpu %ld",
         atomic_long_read(&ref->count), (long)count);

    /*
     * It's crucial that we sum the percpu counters _before_ adding the sum
     * to &ref->count; since gets could be happening on one cpu while puts
     * happen on another, adding a single cpu's count could cause
     * @ref->count to hit 0 before we've got a consistent value - but the
     * sum of all the counts will be consistent and correct.
     *
     * Subtracting the bias value then has to happen _after_ adding count to
     * &ref->count; we need the bias value to prevent &ref->count from
     * reaching 0 before we add the percpu counts. But doing it at the same
     * time is equivalent and saves us atomic operations:
     */
    atomic_long_add((long)count - PERCPU_COUNT_BIAS, &ref->count);

    WARN_ONCE(atomic_long_read(&ref->count) <= 0,
          "percpu ref (%pf) <= 0 (%ld) after switching to atomic",
          ref->release, atomic_long_read(&ref->count));

    /* @ref is viewed as dead on all CPUs, send out switch confirmation */
    percpu_ref_call_confirm_rcu(rcu);
}
</pre>
<strong>A usage of the percpu_ref</strong><br/>
The request_queue->q_usage_counter is a percpu_ref.
blk_mq_freeze_queue() exploit it to drain the ongoing requests
in blk-mq.
<pre>
blk_mq_freeze_queue()
    -> blk_freeze_queue()
        -> blk_freeze_queue_start()
            -> percpu_ref_kill() 
            // switch to atomic mode then the q->q_usage_counter could be dropped to zero.
        -> blk_mq_freeze_queue_wait()
            -> wait_event(q->mq_freeze_wq, percpu_ref_is_zero(&q->q_usage_counter));
blk_mq_unfreeze_queue()
    -> percpu_ref_reinit() // switch to percpu mode
</pre>
The request_queue->q_usage_counter will be operated in the hot path of block and blk-mq.<br/>
For example:
<pre>
generic_make_request()->blk_queue_enter()
blk_mq_get_request()->blk_queue_enter_live()
</pre>
So it will gain a lot of performance improvement.<br/>
<strong>Reference</strong><br/>
<a href="https://lwn.net/Articles/557478/">https://lwn.net/Articles/557478/</a>
</font>
</p>


<h2><a name="Sequence_lock">Sequence lock</a></h2>
<hr style="height:5px;border:none;border-top:3px solid black;" />
<p>
<font size="2">
Sequence lock is used in linux kernel for the read almost data that must be seen in a consistent state by readers. However, unlike the reader-writer lock, the readers don't exclude the writer, but the reader <strong>_must_</strong> retry the operation if they detect activity from the concurrent writer.<br/>
In fact, in linux kernel, the sequence lock is just a combination of a sequence count and a spinlock which is used to execlude concurrent writers. So sequence lock is nearly equal to sequence count.
<pre>
static inline void write_seqlock(seqlock_t *sl)
{
	spin_lock(&sl->lock);
	write_seqcount_begin(&sl->seqcount);
}

static inline void write_sequnlock(seqlock_t *sl)
{
	write_seqcount_end(&sl->seqcount);
	spin_unlock(&sl->lock);
}
</pre>
Following, let's focus on the implementation of sequence count.<br/>
The basic idea of seqence count is:<br/>
The writer will update the sequence count before and after its updating operation. So if the sequence count is a even value, there is no writer ongoing. If it is a odd value, a writer is updating it. In the view of readers, it need to snapshot the value of seqence count. if get a odd value, it indicates a writing is in progress, if two snapshots differ, it indicates a concurrent update has completed and reader may get inconsistent data and need to discard it.<br/>
<pre>
read_seqcount_begin()
    -> raw_read_seqcount_begin()
        -> __read_seqcount_begin()
        >>>>
repeat:
    ret = READ_ONCE(s->sequence);
    if (unlikely(ret & 1)) {
        cpu_relax();
        goto repeat;
    } // ensure the concurrent update is completed.
    return ret;
        >>>>
        -> smp_rmb() 
    //smp_rmb() has two destinations:
    //1. ensure all read operations is after __read_seqcount_begin() to sharp
    //the boundary of read critical section.
    //2. ensure all the read operations see the updating from the other cpu.
</pre>  
read_seqcount_begin() ensure to return an even value, then it could ensure a concurrent updating has been completed at least when read_seqcount_begin() returns.
<pre>
write_seqcount_begin()

updateing

write_seqcount_end()        read_seqcount_begin()
                            reading  
                            read_seqcount_retry()        
</pre>
Most important, this could avoid the scenario below:<br/>
<pre>          context A                  context B                         
	write_seqcount_begin();
	updating ...
	                          
		                     read_seqcount_begin()
			             reading
				     read_seqcount_retry()
	write_seqcount_end();

</pre>
But this could introduce other issue, look at the following scenario.
<pre>          context A                  context B                         
	write_seqcount_begin();
	updating ...
	                  ---preempt--->           
		                     __read_seqcount_begin()
			                 while (seq & 1)
				             cpurelax();
	write_seqcount_end();
</pre>
If the context B is irq context, a live lock comes up. If context B is a normal task, it has to use up its time slice (ideal time in cfs) then come back to context A to update the sequence count to even value. But if context B is a FIFO task, live lock again.<br/>
To fix this, a irq/preempt disable is needed here.<br/>

The classic usage example in linux kernel, the timekeeping.<br/>
<pre>
void update_wall_time(void)
{
    >>>>
    raw_spin_lock_irqsave(&timekeeper_lock, flags);
    >>>>
    clock_set |= accumulate_nsecs_to_secs(tk);

    write_seqcount_begin(&tk_core.seq);
    /*
     * Update the real timekeeper.
     *
     * We could avoid this memcpy by switching pointers, but that
     * requires changes to all other timekeeper usage sites as
     * well, i.e. move the timekeeper pointer getter into the
     * spinlocked/seqcount protected sections. And we trade this
     * memcpy under the tk_core.seq against one before we start
     * updating.
     */
    timekeeping_update(tk, clock_set);
    memcpy(real_tk, tk, sizeof(*tk));
    /* The memcpy must come last. Do not put anything here! */
    write_seqcount_end(&tk_core.seq);
out:
    raw_spin_unlock_irqrestore(&timekeeper_lock, flags);
    if (clock_set)
        /* Have to call _delayed version, since in irq context*/
        clock_was_set_delayed();

}

ktime_t ktime_get_raw(void)
{
    struct timekeeper *tk = &tk_core.timekeeper;
    unsigned int seq;
    ktime_t base;
    u64 nsecs;

    do {
        seq = read_seqcount_begin(&tk_core.seq);
        base = tk->tkr_raw.base;
        nsecs = timekeeping_get_ns(&tk->tkr_raw);

    } while (read_seqcount_retry(&tk_core.seq, seq));

    return ktime_add_ns(base, nsecs);
}
</pre>
</font>
</p>
<h2><a name="ACCESS_ONCE_Series">ACCESS_ONCE Series</a></h2>
<hr style="height:5px;border:none;border-top:3px solid black;" />
<p>
<font size="2">
There are a lot of ACCESS_ONCE macro in the kernel, what are they for ?<br/>
<a href="https://lwn.net/Articles/508991/">https://lwn.net/Articles/508991/</a>
Its functionality has been actually well described by its name. Its purpose is to ensure the address passed by the parameter to be <strong>accessed exactly once</strong> by the final assembly code generated by the compiler. We may suspect that would the address be accessed twice or non ? Actually, yes. The optimizaion option of the C compiler could do that.<br/>
If not given reasons to the contrary, <strong>assume that there is only one thread of execution in the address space of the program it is compiling</strong>. Concurrency is not built into the C language itself, so mechanisms for dealing with concurrent access must be built on top of the language; ACCESS_ONCE() is one such mechanism.<br/>
Consider the following example:
<pre>
int foo(int x)
{
    x = x*x;
    x = x>>6;
    printf("result is %d\n", x);
    return x;
}

int main()
{
    int x;
    int conti;
    int *p_value = (int *)malloc(sizeof(int));

     for (;;) {
        x = *p_value;
        conti = foo(x);
        if (conti)
            break;
     }

    return 0;
}
</pre>
gcc -O2 test.c -S -o test.asm
<pre>
main:
.LFB39:
    .cfi_startproc
    pushq    %rbx
    .cfi_def_cfa_offset 16
    .cfi_offset 3, -16
    movl    $4, %edi
    call    malloc
    <strong>movl    (%rax), %ebx</strong>
    imull    %ebx, %ebx
    sarl    $6, %ebx
    .p2align 4,,10
    .p2align 3
.L4:
    xorl    %eax, %eax
    movl    %ebx, %edx
    movl    $.LC0, %esi
    movl    $1, %edi
    call    __printf_chk
    testl    %ebx, %ebx
    je    .L4
    xorl    %eax, %eax
    popq    %rbx
    .cfi_def_cfa_offset 8
    ret
</pre>
The actual scenario may be more complicated than above. But the result is similar. To prevent the optimization, we could introduce the ACCESS_ONCE() macro.
<pre>
#define ACCESS_ONCE(x) (*(volatile typeof(x) *)&(x))
</pre>
Then
<pre>
#define ACCESS_ONCE(x) (*(volatile typeof(x) *)&(x))
int foo(int x)
{
    x = x*x;
    x = x>>6;
    printf("result is %d\n", x);
    return x;
}

int main()
{
    int x;
    int conti;
    int *p_value = (int *)malloc(sizeof(int));

     for (;;) {
        x = ACCESS_ONCE(*p_value);
        conti = foo(x);
        if (conti)
            break;
     }

    return 0;
}
</pre>
gcc -O2 test.c -S -o test.asm
<pre>
main:
.LFB39:
    .cfi_startproc
    pushq    %rbp
    .cfi_def_cfa_offset 16
    .cfi_offset 6, -16
    pushq    %rbx
    .cfi_def_cfa_offset 24
    .cfi_offset 3, -24
    movl    $4, %edi
    subq    $8, %rsp
    .cfi_def_cfa_offset 32
    call    malloc
    movq    %rax, %rbp
    .p2align 4,,10
    .p2align 3
.L4:
    <strong>movl    0(%rbp), %ebx</strong>
    xorl    %eax, %eax
    movl    $.LC0, %esi
    movl    $1, %edi
    imull    %ebx, %ebx
    sarl    $6, %ebx
    movl    %ebx, %edx
    call    __printf_chk
    testl    %ebx, %ebx
    je    .L4
    addq    $8, %rsp
    .cfi_def_cfa_offset 24
    xorl    %eax, %eax
</pre>

As it happens, an optimized-out access is not the only peril that this code could encounter. Some processor architectures (x86, for example) are not richly endowed with registers; on such systems, the compiler must make careful choices regarding which values to keep in registers if it is to generate the highest-performing code. Specific values may be pushed out of the register set, then pulled back in later. The address is accessed twice actually, and this may introcue inconsistence into kernel.<br/><br/>

There are two other macro to achieve same functionality, READ_ONCE() and WRITE_ONCE(), the implementation is similar with ACCESS_ONCE<br/>
Another expample is:
<a href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/n4444.html">http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/n4444.html</a>
<pre>
	while (tmp = atomic_load_explicit(a, memory_order_relaxed))
		do_something_with(tmp);
</pre>
The compiler would be permitted to unroll it as follows:
<pre>
	while (tmp = atomic_load_explicit(a, memory_order_relaxed))
		do_something_with(tmp);
		do_something_with(tmp);
		do_something_with(tmp);
		do_something_with(tmp);
	}
</pre>
This would be unacceptable for real-time applications, which need the value to be reloaded from a on each iteration, unrolled or not. The volatile qualifier prevents this transformation. For example, consider the following loop.
<pre>
	while (tmp = READ_ONCE(a))
		do_something_with(tmp);
</pre>
This loop could still be unrolled, but the read would also need to be unrolled, for example, like this:
<pre>
	for (;;) {
		if (!(tmp = READ_ONCE(a)))
			break;
		do_something_with(tmp);
		if (!(tmp = READ_ONCE(a)))
			break;
		do_something_with(tmp);
		if (!(tmp = READ_ONCE(a)))
			break;
		do_something_with(tmp);
		if (!(tmp = READ_ONCE(a)))
			break;
		do_something_with(tmp);
	}
</pre>

Let's look at the implementation of READ_ONCE:
<pre>
#define __READ_ONCE_SIZE						\
({									\
	switch (size) {							\
	case 1: *(__u8 *)res = *(volatile __u8 *)p; break;		\
	case 2: *(__u16 *)res = *(volatile __u16 *)p; break;		\
	case 4: *(__u32 *)res = *(volatile __u32 *)p; break;		\
	case 8: *(__u64 *)res = *(volatile __u64 *)p; break;		\
	default:							\
		barrier();						\
		__builtin_memcpy((void *)res, (const void *)p, size);	\
		barrier();						\
	}								\
})
</pre>
Why we need this __u8,u16,u32... here ?<br/>
At one time, gcc guaranteed that <strong>properly aligned accesses to machine-word-sized variables would be atomic</strong>. Although gcc <strong>_no longer_</strong> documents this guarantee, there is still code in the Linux kernel that relies on it. These accesses could be modeled as non-volatile memory_order_relaxed accesses.
</font>
</p>

<h2><a name="RCU">RCU</a></h2>

<h3><a name="Rcu_Tree">Rcu Tree</a></h3>
<p>
<font size="2">
<pre>
                    rcu_state                 
                       |
    +-----------+-----------+----------+
    |           |           |          |
 rcu_node    rcu_node    rcu_node   rcu_note <,  
                                               \ 
    |           |           |          |      mynode
                                               /
 rcu_data    rcu_data   rcu_data    rcu_data  /

<font color="blue">rcu_state - rcu(sched/preemptible) rcu_sched, rcu_bh</font>    
<font color="blue">rcu_node  - Just RCU TREE Orginazition</font>
<font color="blue">rcu_data  - PER-CPU Indicate a specific CPUs
                               Get it through raw_cpu_ptr(rsp->rda)</font>
</pre>
rcu_node is just for RCU Tree orginazition. We could get the topology from the
following member.<br/>
<pre>
rcu_node->qsmask, qsmaskinit, grpmask
qsmask           The mark that indicate the the CPUs or children nodes in this
                 node has not passed the quiescent state. Per bit indicate one CPU or
                 child node.
qsmaskinit       Use to initialize the qsmask when a new grace period start.
grpmask          The position in parent's qsmaskinit.

rcu_data->grpmask
grpmask          The position in qsmaskinit of the node where local CPU belongs to.
</pre>

A quiescent state is reported through:<br/>
<pre>
   ^                       -> rcu_report_qs_rsp
   |               -> rcu_report_qs_rnp
   |            ...
           -> rcu_report_qs_rnp
   rcu_report_qs_rdp
</pre>

</font>
</p>

<h3><a name="How_does_RCU_run">How does RCU run</a></h3>
<p>
<font size="2">
There are 3 types rcu in kernel.
<ul>
<li> preemptible
<li> sched
<li> bh
</ul>
We just talk abot sched and bh rcu in this section.<br/>
</font>
</p>


<h4><a name="Grace_period_and_Quiescent_state">Grace period and Quiescent state</a></h4>
<p>
<font size="2">
When all the cpu pass quiescent state, we say the system passes grace
period.<br/>
rcu_gp_in_progress() tell us whether a gp is in progress.
<pre>
static int rcu_gp_in_progress(struct rcu_state *rsp)
{
    return READ_ONCE(rsp->completed) != READ_ONCE(rsp->gpnum);
}
</pre>
rcu_start_gp() will start a new grace period.<br/>
<pre>
rcu_start_gp()
    -> rcu_start_gp_advanced()
        -> WRITE_ONCE(rsp->gp_flags, RCU_GP_FLAG_INIT);
rcu_gp_kthread will be wakeup and handle the grace-period start.
This is done in rcu_gp_init().
 - record_gp_stall_check_time(rsp), calculate a new rsp->jiffies_stall which
   will be used in rcu_stall detection.
 - smp_store_release(&rsp->gpnum, rsp->gpnum + 1)
 - iterate all the rnp and rdp:
   - rnp->qsmask = rnp->qsmaskinit
   - WRITE_ONCE(rnp->gpnum, rsp->gpnum);
   - __note_gp_changes()

__note_gp_changes()
----
    if (rdp->gpnum != rnp->gpnum || unlikely(READ_ONCE(rdp->gpwrap))) {
        rdp->gpnum = rnp->gpnum;
        need_gp = !!(rnp->qsmask & rdp->grpmask);
        <font color="red">rdp->cpu_no_qs.b.norm = need_gp;</font>
        <font color="red">rdp->core_needs_qs = need_gp;</font>
    }
----

</pre>
<br/>
<br/>
How to <font color="red">report</font> a cpu has passed qs ?<br/>
<pre>
SOFTIRQ_RCU context
rcu_process_callbacks() //iterate all the rsp
    -> __rcu_process_callbacks()
        -> rcu_check_quiescent_state()
rcu_check_quiescent_state()
----
    <font color="red">if (!rdp->core_needs_qs)</font>
        return;

    <font color="red">if (rdp->cpu_no_qs.b.norm)</font>
        return;

    rcu_report_qs_rdp(rdp->cpu, rsp, rdp);
----
</pre>
<br/>
How to <font color="red">check</font> a cpu has passed qs ?<br/>
<pre>
bh qs
<font color="red">cpu tick irq context</font>
update_process_times()
    -> rcu_check_callbacks()
        -> rcu_bh_qs() // <font color="red">user mode or !in_softirq</font>

Note: local_bh_disable will increase SOFTIRQ part of preempt_count, in_softirq
will return true at the moment.

__do_softirq()
    -> rcu_bh_qs() //<font color="red">after loop of invoking softirq action</font>
</pre>

</font>
</p>

<h3><a name="RCU_Usage">RCU Usage</a></h3>

<h4><a name="scsi_host_busy">scsi_host_busy</a></h4>
<p>
<font size="2">
<pre>
scsi_host_queue_ready
---
    if (scsi_host_in_recovery(shost))
        return 0;

    busy = atomic_inc_return(&shost->host_busy) - 1;
<font color="blue">
    // We add the host_busy here atomically, but this is not\
    // the final result. We could decrement host_busy next due to
    // multiple reasons.
</font>                                                              scsi_host_set_state(shost, SHOST_RECOVERY)    
                                                                     scsi_eh_wakeup
    if (atomic_read(&shost->host_blocked) > 0) {                     ---
        if (busy)                                                    if (atomic_read(&shost->host_busy) == shost->host_failed) {
            goto starved;                                                wake_up_process(shost->ehandler);
                                                                     ---
        if (atomic_dec_return(&shost->host_blocked) > 0)             <font color="blue">The host_busy here is not the final result</font>
            goto out_dec;                                            <font color="blue">operation on host_busy is atomic, but the whole scsi_host_queue_ready is not atomic</font>
    }

    if (shost->can_queue > 0 && busy >= shost->can_queue)
        goto starved;
    if (shost->host_self_blocked)
        goto starved;
    ...
    return 1;

starved:
    spin_lock_irq(shost->host_lock);
    if (list_empty(&sdev->starved_entry))
        list_add_tail(&sdev->starved_entry, &shost->starved_list);
    spin_unlock_irq(shost->host_lock);
out_dec:
    atomic_dec(&shost->host_busy);
    return 0;
---

The worst result could be:
scsi_host_queue_ready return due to busy >= shost->can_queue, and the
scsi_eh_wakeup doesn't wake up the EH kthread due to the race. And then
the requests in EH cannot be handled and shost->can_queue cannot be decremented
and IO hang comes up finally.

The fix could be:
scsi_host_queue_ready                        scsi_eh_scmd_add
---                                          ---
    rcu_read_lock();                         scsi_host_set_state(shost, SHOST_RECOVERY)
    if (scsi_host_in_recovery(shost)) {   
        rcu_read_unlock();
        return 0;
    }
    host_busy operations;
    rcu_read_unlock();
                                             synchronize_rcu();
                                             shost->host_failed++;
                                             scsi_eh_wakeup(shost);
---
Then scsi_eh_wakeup will not race with scsi_host_queue_ready any more due to the
SHOST_RECOVERY state.
scsi_eh_scmd_add is always invoked in process context, so I think this should be
OK.

The fix in upstream is as following:
scsi_host_queue_ready
  -> scsi_dec_host_busy
  ---
    rcu_read_lock();
    atomic_dec(&shost->host_busy);
    if (unlikely(scsi_host_in_recovery(shost))) {
        spin_lock_irqsave(shost->host_lock, flags);
        if (shost->host_failed || shost->host_eh_scheduled)
            scsi_eh_wakeup(shost);
        spin_unlock_irqrestore(shost->host_lock, flags);
    }
    rcu_read_unlock();
  ---

scsi_eh_scmd_add
  -> scsi_host_set_state(shost, SHOST_RECOVERY)
  -> call_rcu(&scmd->rcu, scsi_eh_inc_host_failed);

scsi_eh_inc_host_failed
---
    spin_lock_irqsave(shost->host_lock, flags);
    shost->host_failed++;
    scsi_eh_wakeup(shost);
    spin_unlock_irqrestore(shost->host_lock, flags);
---
Weird solution.

</pre>
</font>
</p>

</body>
</html>
