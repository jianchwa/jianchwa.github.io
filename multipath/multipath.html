<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>multipath</title>
</head>
<body>
<div>
    <h1>Multipath</h1> 
</div>
<p>
<font size="2">
<a href="#Concepts">Concepts</a><br/>
<ul>
<li> <a href="#multiple_path">multiple path</a>
<li> <a href="#a_a_a_p_alua">a/a a/p alua</a>
</ul>
<a href="#Implementation">Implementation</a>
<ul>
<li><a href="#How_to_identify_paths_to_one_LU">How to identify pathes to one LU</a>
</ul>
</font>
</p>

<h2><a name="Concepts">Concepts</a></h2>
<hr style="height:5px;border:none;border-top:3px solid black;" />

<h3><a name="multiple_path">multiple path</a></h3>
<p>
<font size="2">
The storage process could be broken into 3 different functional areas as following:
<pre>
Connecting - Provides the transmission of data between systems and storage
Storing    - A lower-level application with specialized commands and control protocols for 
             system/device interactions.
Filing     - Directs the placement of data objects in storage and is responsible for
             presenting data to applications and users
</pre>

For the iSCSI, the TCP/IP is _connecting_, iSCSI is the _storing_.<br/>
For the NVMe, the NVMe is storing, the _connecting_ could be pcie, fc or rdma.... <br/>
Filing is commonly the filesystem.<br/>
<br/>
We could easily infer that the multipath is in <font color="red">_connecting_</font>, it depends on the redundant IO path between<br/>
system and storage.
<pre>
The connection from the server through the HBA to the storage controller is referred as a path. When multiple paths
exists to a storage device(LUN) on a storage subsystem, it is referred as multipath connectivity. It is a enterprise
level storage capability. Main purpose of multipath connectivity is to provide redundant access to the storage devices,
i.e to have access to the storage device when one or more of the components in a path fail. Another advantage of
multipathing is the increased throughput by way of load balancing.

Note: Multipathing protects against the failure of path(s) and not the failure of a specific storage device.

Common example of multipath is a SAN connected storage device. Usually one or more fibre channel HBAs from the host will be
connected to the fabric switch and the storage controllers will be connected to the same switch.
A simple example of multipath could be: 2 HBAs connected to a switch to which the storage controllers are connected. In this
case the storage controller can be accessed from either of the HBAs and hence we have multipath connectivity.
Following is an example of multipath
</pre>
<pre>
                              +----------------------+
                              |                      |  
                              |        SERVER        |
                              |                      |
                              +-----+----------+-----+
                              |HBA_0|          |HBA_1|
                              +--+--+          +--+--+
                                 |                |
                                 |   +--------+   |
                                 +---| SWITCH |---+ 
                                     +-+----+-+
                                       |    | 
                                 +-----+    +-----+
                                 |                |
                             +---+---+        +---+---+
                             |CONTRL0|        |CONTRL1|
                             +-------+--------+-------+
                             |       STORAGE          |
                             +------------------------+
 
</pre>
</font>
</p>

<h3><a name="a_a_a_p_alua">a/a a/p alua</a></h3>
<p>
<font size="2">
<pre>
<a href="http://kaminario.com/company/blog/active-active-vs-active-passive-storage-controllers/">active/active vs active/passive storage controllers</a>
<a href="http://www.govmlab.com/alua-asymmetric-logic-unit-access-alua-different-aa-ap-array/">What is alua</a>

<strong>Active-Active</strong> storage controllers describe an architecture where both controllers are doing the above functions and the load
 on the storage array is automatically distributed evenly between all the controllers.

<strong>Active-Passive</strong> storage controllers refer to the approach where only one controller is doing the above functions (that’s the
 “active” controller) and the second controller is not doing anything (you guessed it, that’s the “passive” controller).

<strong>Asymmetric Logical Unit Access</strong> or ALUA refers to a hybrid approach. I know that you are thinking, “ALUA is a horrible
 acronym.” In this approach, both controllers are at work, but Logical Unit Numbers (LUNs) have an affinity to a specific controller and
 usually, if you access the LUN from a different controller, you’ll pay a performance price. In addition, the performance per LUN is limited to
 that of a single controller (yes, also in a scale-out architecture). So you need to start manual load balancing of the LUNs between
 controllers. Did I mention it’s a headache?
</pre>
</font>
</p>




<h2><a name="Implementation">Implementation</a></h2>
<hr style="height:5px;border:none;border-top:3px solid black;" />
<p>
<font size="2">
The kernal part of multipath is based on Device mapper and it is the only one <font color="red">request-based</font> dm driver.<br/>
<pre>
                 Filesystem
 ---------------------------------- generic_make_request
                 BIO
 ---------------------------------- blk_make_request
                 Blk-mq
                 Queue
 ---------------------------------- dm_mq_queue_rq
                 Blk-mq 
                 w/o sched
         Queue0         Queue1
 ---------------------------------- scsi_queue_rq
                 SCSI-mq
                 
</pre>

</font>
</p>

<h3><a name="How_to_identify_paths_to_one_LU">How to identify pathes to one LU</a></h3>
<p>
<font size="2">
<pre>
path_discovery
---
    udev_iter = udev_enumerate_new(udev);
    if (!udev_iter)
        return -ENOMEM;

    udev_enumerate_add_match_subsystem(udev_iter, "block");
    udev_enumerate_add_match_is_initialized(udev_iter);
    udev_enumerate_scan_devices(udev_iter);

<font color="red">
    udev_list_entry_foreach(entry,
                udev_enumerate_get_list_entry(udev_iter)) {
</font>
        const char *devtype;
        devpath = udev_list_entry_get_name(entry);
        condlog(4, "Discover device %s", devpath);
        udevice = udev_device_new_from_syspath(udev, devpath);
        if (!udevice) {
            condlog(4, "%s: no udev information", devpath);
            continue;
        }
        devtype = udev_device_get_devtype(udevice);
        if(devtype && !strncmp(devtype, "disk", 4)) {
            total_paths++;
            conf = get_multipath_config();
            pthread_cleanup_push(put_multipath_config, conf);
<font color="red">
            if (path_discover(pathvec, conf,
                      udevice, flag) == PATHINFO_OK)
</font>
                num_paths++;
            pthread_cleanup_pop(1);
        }
        udev_device_unref(udevice);
    }
---

configure
  -> path_discover
    -> store_pathinfo
      -> path_info // collect information.
        -> sysfs_pathinfo
        -> scsi_ioctl_pathinfo
<font color="red">
all the paths will be collected together.
then coalesce the paths to a same target/LU together
a dm-mpath will be built on top of these paths.
</font>
  -> coalesce_paths
    -> add_map_with_path
      -> adopt_paths
---
    vector_foreach_slot (pathvec, pp, i) {
<font color="red">
        if (!strncmp(mpp->wwid, pp->wwid, WWID_SIZE)) {
</font>
            condlog(3, "%s: ownership set to %s",
                pp->dev, mpp->alias);
            pp->mpp = mpp;

            if (!mpp->paths && !(mpp->paths = vector_alloc()))
                return 1;

            if (!find_path_by_dev(mpp->paths, pp->dev) &&
                store_path(mpp->paths, pp))
                    return 1;
            conf = get_multipath_config();
            pthread_cleanup_push(put_multipath_config, conf);
            ret = pathinfo(pp, conf,
                       DI_PRIO | DI_CHECKER);
            pthread_cleanup_pop(1);
            if (ret)
                return 1;
        }
    }

---
</pre>
The <font color="red"><strong>wwid</strong></font> is the flag that identify paths that are connecting the same target/LU.
Where does it come form ?
<pre>
configure //DI_ALL
  -> path_discover
    -> store_pathinfo
      -> path_info //DI_WWID
        -> get_uid //SYSFS_BUS_SCSI
          -> get_vpd_uid
            -> get_vpd_sysfs //0x83
              -> sysfs_get_vpd

read /sys/block/sda/device/vpd_pg83
<font color="red">
vps: vital product data
</font>
scsi_add_lun
  -> scsi_attach_vpd
    -> scsi_update_vpd_page
      -> scsi_get_vpd_buf
        -> scsi_vpd_inquiry
---
<font color="red">
    cmd[0] = INQUIRY;
    cmd[1] = 1;        /* EVPD */
    cmd[2] = page; //0x83
</font>
    cmd[3] = len >> 8;
    cmd[4] = len & 0xff;
    cmd[5] = 0;        /* Control byte */

    /*
     * I'm not convinced we need to try quite this hard to get VPD, but
     * all the existing users tried this hard.
     */
    result = scsi_execute_req(sdev, cmd, DMA_FROM_DEVICE, buffer,
                  len, NULL, 30 * HZ, 3, NULL);

---
SCSI_Command_Manual_Reference

The application client requests the vital product data information by setting the EVPD bit to one 
and specifying the page code of a vital product data.

5.4.11 Device Identification VPD page (83h)
Device identifiers consist of one or more of the following:
<ul>
<li> Logical unit names;
<li> SCSI target port identifiers;
<li> SCSI target port names;
<li> SCSI target device names;
<li> Relative target port identifiers;
<li> SCSI target port group number; or
<li> Logical unit group number.
</ul>
</pre>


</font>
</p>


</body>
</html>
