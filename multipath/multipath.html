<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>multipath</title>
</head>
<body>
<div>
    <h1>Multipath</h1> 
</div>
<p>
<font size="2">
<a href="#Concepts">Concepts</a><br/>
<ul>
<li> <a href="#multiple_path">multiple path</a>
<li> <a href="#a_a_a_p_alua">a/a a/p alua</a>
</ul>
<a href="#Implementation">Implementation</a>
<ul>
<li><a href="#How_to_identify_paths_to_one_LU">How to identify pathes to one LU</a>
<li><a href="#Deliver_request">Deliver request</a>
<li><a href="#path_select">path select</a>
<li><a href="#path_group">path group</a>
<li><a href="#device_handler">device handler</a>
</ul>
</font>
</p>

<h2><a name="Concepts">Concepts</a></h2>
<hr style="height:5px;border:none;border-top:3px solid black;" />

<h3><a name="multiple_path">multiple path</a></h3>
<p>
<font size="2">
The storage process could be broken into 3 different functional areas as following:
<pre>
Connecting - Provides the transmission of data between systems and storage
Storing    - A lower-level application with specialized commands and control protocols for 
             system/device interactions.
Filing     - Directs the placement of data objects in storage and is responsible for
             presenting data to applications and users
</pre>

For the iSCSI, the TCP/IP is _connecting_, iSCSI is the _storing_.<br/>
For the NVMe, the NVMe is storing, the _connecting_ could be pcie, fc or rdma.... <br/>
Filing is commonly the filesystem.<br/>
<br/>
We could easily infer that the multipath is in <font color="red">_connecting_</font>, it depends on the redundant IO path between<br/>
system and storage.
<pre>
The connection from the server through the HBA to the storage controller is referred as a path. When multiple paths
exists to a storage device(LUN) on a storage subsystem, it is referred as multipath connectivity. It is a enterprise
level storage capability. Main purpose of multipath connectivity is to provide redundant access to the storage devices,
i.e to have access to the storage device when one or more of the components in a path fail. Another advantage of
multipathing is the increased throughput by way of load balancing.

Note: Multipathing protects against the failure of path(s) and not the failure of a specific storage device.

Common example of multipath is a SAN connected storage device. Usually one or more fibre channel HBAs from the host will be
connected to the fabric switch and the storage controllers will be connected to the same switch.
A simple example of multipath could be: 2 HBAs connected to a switch to which the storage controllers are connected. In this
case the storage controller can be accessed from either of the HBAs and hence we have multipath connectivity.
Following is an example of multipath
</pre>
<pre>
                              +----------------------+
                              |                      |  
                              |        SERVER        |
                              |                      |
                              +-----+----------+-----+
                              |HBA_0|          |HBA_1|
                              +--+--+          +--+--+
                                 |                |
                                 |   +--------+   |
                                 +---| SWITCH |---+ 
                                     +-+----+-+
                                       |    | 
                                 +-----+    +-----+
                                 |                |
                             +---+---+        +---+---+
                             |CONTRL0|        |CONTRL1|
                             +-------+--------+-------+
                             |       STORAGE          |
                             +------------------------+
 
</pre>
</font>
</p>

<h3><a name="a_a_a_p_alua">a/a a/p alua</a></h3>
<p>
<font size="2">
<pre>
<a href="http://kaminario.com/company/blog/active-active-vs-active-passive-storage-controllers/">active/active vs active/passive storage controllers</a>
<a href="http://www.govmlab.com/alua-asymmetric-logic-unit-access-alua-different-aa-ap-array/">What is alua</a>

<strong>Active-Active</strong> storage controllers describe an architecture where both controllers are doing the above functions and the load
 on the storage array is automatically distributed evenly between all the controllers.

<strong>Active-Passive</strong> storage controllers refer to the approach where only one controller is doing the above functions (that’s the
 “active” controller) and the second controller is not doing anything (you guessed it, that’s the “passive” controller).

<strong>Asymmetric Logical Unit Access</strong> or ALUA refers to a hybrid approach. I know that you are thinking, “ALUA is a horrible
 acronym.” In this approach, both controllers are at work, but Logical Unit Numbers (LUNs) have an affinity to a specific controller and
 usually, if you access the LUN from a different controller, you’ll pay a performance price. In addition, the performance per LUN is limited to
 that of a single controller (yes, also in a scale-out architecture). So you need to start manual load balancing of the LUNs between
 controllers. Did I mention it’s a headache?
</pre>
</font>
</p>




<h2><a name="Implementation">Implementation</a></h2>
<hr style="height:5px;border:none;border-top:3px solid black;" />
<p>
<font size="2">
The kernal part of multipath is based on Device mapper and it is the only one <font color="red">request-based</font> dm driver.<br/>
<pre>
                 Filesystem
 ---------------------------------- generic_make_request
                 BIO
 ---------------------------------- blk_make_request
                 Blk-mq
                 Queue
 ---------------------------------- dm_mq_queue_rq
                 Blk-mq 
                 w/o sched
         Queue0         Queue1
 ---------------------------------- scsi_queue_rq
                 SCSI-mq
                 
</pre>

</font>
</p>

<h3><a name="How_to_identify_paths_to_one_LU">How to identify pathes to one LU</a></h3>
<p>
<font size="2">
<pre>
path_discovery
---
    udev_iter = udev_enumerate_new(udev);
    if (!udev_iter)
        return -ENOMEM;

    udev_enumerate_add_match_subsystem(udev_iter, "block");
    udev_enumerate_add_match_is_initialized(udev_iter);
    udev_enumerate_scan_devices(udev_iter);

<font color="red">
    udev_list_entry_foreach(entry,
                udev_enumerate_get_list_entry(udev_iter)) {
</font>
        const char *devtype;
        devpath = udev_list_entry_get_name(entry);
        condlog(4, "Discover device %s", devpath);
        udevice = udev_device_new_from_syspath(udev, devpath);
        if (!udevice) {
            condlog(4, "%s: no udev information", devpath);
            continue;
        }
        devtype = udev_device_get_devtype(udevice);
        if(devtype && !strncmp(devtype, "disk", 4)) {
            total_paths++;
            conf = get_multipath_config();
            pthread_cleanup_push(put_multipath_config, conf);
<font color="red">
            if (path_discover(pathvec, conf,
                      udevice, flag) == PATHINFO_OK)
</font>
                num_paths++;
            pthread_cleanup_pop(1);
        }
        udev_device_unref(udevice);
    }
---

configure
  -> path_discover
    -> store_pathinfo
      -> path_info // collect information.
        -> sysfs_pathinfo
        -> scsi_ioctl_pathinfo
<font color="red">
all the paths will be collected together.
then coalesce the paths to a same target/LU together
a dm-mpath will be built on top of these paths.
</font>
  -> coalesce_paths
    -> add_map_with_path
      -> adopt_paths
---
    vector_foreach_slot (pathvec, pp, i) {
<font color="red">
        if (!strncmp(mpp->wwid, pp->wwid, WWID_SIZE)) {
</font>
            condlog(3, "%s: ownership set to %s",
                pp->dev, mpp->alias);
            pp->mpp = mpp;

            if (!mpp->paths && !(mpp->paths = vector_alloc()))
                return 1;

            if (!find_path_by_dev(mpp->paths, pp->dev) &&
                store_path(mpp->paths, pp))
                    return 1;
            conf = get_multipath_config();
            pthread_cleanup_push(put_multipath_config, conf);
            ret = pathinfo(pp, conf,
                       DI_PRIO | DI_CHECKER);
            pthread_cleanup_pop(1);
            if (ret)
                return 1;
        }
    }

---
</pre>
The <font color="red"><strong>wwid</strong></font> is the flag that identify paths that are connecting the same target/LU.
Where does it come form ?
<pre>
configure //DI_ALL
  -> path_discover
    -> store_pathinfo
      -> path_info //DI_WWID
        -> get_uid //SYSFS_BUS_SCSI
          -> get_vpd_uid
            -> get_vpd_sysfs //0x83
              -> sysfs_get_vpd

read /sys/block/sda/device/vpd_pg83
<font color="red">
vps: vital product data
</font>
scsi_add_lun
  -> scsi_attach_vpd
    -> scsi_update_vpd_page
      -> scsi_get_vpd_buf
        -> scsi_vpd_inquiry
---
<font color="red">
    cmd[0] = INQUIRY;
    cmd[1] = 1;        /* EVPD */
    cmd[2] = page; //0x83
</font>
    cmd[3] = len >> 8;
    cmd[4] = len & 0xff;
    cmd[5] = 0;        /* Control byte */

    /*
     * I'm not convinced we need to try quite this hard to get VPD, but
     * all the existing users tried this hard.
     */
    result = scsi_execute_req(sdev, cmd, DMA_FROM_DEVICE, buffer,
                  len, NULL, 30 * HZ, 3, NULL);

---
SCSI_Command_Manual_Reference

The application client requests the vital product data information by setting the EVPD bit to one 
and specifying the page code of a vital product data.

5.4.11 Device Identification VPD page (83h)
Device identifiers consist of one or more of the following:
<ul>
<li> Logical unit names;
<li> SCSI target port identifiers;
<li> SCSI target port names;
<li> SCSI target device names;
<li> Relative target port identifiers;
<li> SCSI target port group number; or
<li> Logical unit group number.
</ul>
</pre>

</font>
</p>

<h3><a name="Deliver_request">Deliver request</a></h3>
<p>
<font size="2">
<pre>

  ----------------------------
         Queue of DM

   Merge, Sort, IO scheduling
  ----------------------------
    Path0          Path1           
  Queue of Dev    Queue of Dev
  -----------     ------------
      | issue         | issue
      v               v


All the merge, sort and io scheduling work is done in the queue of DM device.
The requests will only passthrough the queue of target device.
This is done by blk_insert_cloned_request.

blk_insert_cloned_request used to queue the original request to underlying
request_queues' hctx->dispatch and run the hctx directly.
When the underlying driver returns BLK_STS_DEV_RESOURCE/BLK_STS_RESOURCE,
the cloned request will be queued to underlying hctx->dispatch and wait for
the resource. And also, the original request has been issued from the top layer
queue (dm queue) and cannot do any merge on it.

A patch from leiming improve this.
396eaf2 (blk-mq: improve DM's blk-mq IO merging via blk_insert_cloned_request feedback)
---
    Fix this by updating blk_insert_cloned_request() to use
    blk_mq_request_direct_issue().  blk_mq_request_direct_issue() allows a
    request to be issued directly to the underlying queue and returns the
    dispatch feedback (blk_status_t).  If blk_mq_request_direct_issue()
    returns BLK_SYS_RESOURCE the dm-rq driver will now use DM_MAPIO_REQUEUE
    to _not_ destage the request.  Whereby preserving the opportunity to
    merge IO.
---
See map_request()
---
    case DM_MAPIO_REMAPPED:
        if (setup_clone(clone, rq, tio, GFP_ATOMIC)) {
            /* -ENOMEM */
            ti->type->release_clone_rq(clone);
            return DM_MAPIO_REQUEUE;
        }

        /* The target has remapped the I/O so dispatch it */
        trace_block_rq_remap(clone->q, clone, disk_devt(dm_disk(md)),
                     blk_rq_pos(rq));
        ret = dm_dispatch_clone_request(clone, rq);
        if (ret == BLK_STS_RESOURCE || ret == BLK_STS_DEV_RESOURCE) {
            blk_rq_unprep_clone(clone);
            tio->ti->type->release_clone_rq(clone);
            tio->clone = NULL;
<font color="red">
            //release the cloned request and requeue the original request.
</font
            if (!rq->q->mq_ops)
                r = DM_MAPIO_DELAY_REQUEUE;
            else
                r = DM_MAPIO_REQUEUE;
            goto check_again;
        }
        break;
    case DM_MAPIO_REQUEUE:
        /* The target wants to requeue the I/O */
        break;
    case DM_MAPIO_DELAY_REQUEUE:
        /* The target wants to requeue the I/O after a delay */
        dm_requeue_original_request(tio, true);
        break;

---
</pre>
</font>
</p>

<h3><a name="path_select">path select</a></h3>
<p>
<font size="2">
<pre>
multipath_clone_and_map
---
    /* Do we need to select a new pgpath? */
    pgpath = READ_ONCE(m->current_pgpath);
    if (!pgpath || !test_bit(MPATHF_QUEUE_IO, &m->flags))
        pgpath = choose_pgpath(m, nr_bytes);
---
We will choose path in following cases:
<ul>
<li> m->current_pgpath is NULL
The typical case which will set current_pgpath to NULL is fail_path.
When io error occurs on the path.
dm_softirq_done
  -> dm_done
    -> multipath_end_io //<font color="red">blk_path_error</font>
      -> fail_path
When user mode message indicates that (multipathd)
multipath_message
<li> MPATHF_QUEUE_IO is <font color="red">not</font> set
It is set by __switch_pg when <a href="#device_handler">device handler</a> is set
---
    if (m->hw_handler_name) {
        set_bit(MPATHF_PG_INIT_REQUIRED, &m->flags);
        set_bit(MPATHF_QUEUE_IO, &m->flags);
    }
---
The PG initialization process is started by pg_init_all_paths.
When it is completed, pg_init_done is invoked and MPATHF_QUEUE_IO is cleared.
after pg_init_done clears MPATHF_QUEUE_IO, it will also kick the requeue list.
as we know, when MPATHF_QUEUE_IO is set, the reqs are all requeued.
pg_init_done
  -> process_queued_io_list
    -> dm_mq_kick_requeue_list.
      -> __dm_mq_kick_requeue_list
        -> blk_mq_delay_kick_requeue_list
</ul>
<br/>
<strong>Therefore, we will select path every time!</strong><br/>
choose_pgpath
---
<font color="blue">
/* Don't change PG until it has no remaining paths */
</font>
check_current_pg:
    pg = READ_ONCE(m->current_pg);
    if (pg) {
        pgpath = choose_path_in_pg(m, pg, nr_bytes);
        if (!IS_ERR_OR_NULL(pgpath))
            return pgpath;
    }
---
static struct pgpath *choose_path_in_pg(struct multipath *m,
                    struct priority_group *pg,
                    size_t nr_bytes)
{
    unsigned long flags;
    struct dm_path *path;
    struct pgpath *pgpath;

    path = pg->ps.type->select_path(&pg->ps, nr_bytes);
    if (!path)
        return ERR_PTR(-ENXIO);

    pgpath = path_to_pgpath(path);

    if (unlikely(READ_ONCE(m->current_pg) != pg)) {
        /* Only update current_pgpath if pg changed */
        spin_lock_irqsave(&m->lock, flags);
        m->current_pgpath = pgpath;
<font color="red">
        __switch_pg(m, pg);
</font>
        spin_unlock_irqrestore(&m->lock, flags);
    }

    return pgpath;
}
Let's look at the round-robin selector:
static struct dm_path *rr_select_path(struct path_selector *ps, size_t nr_bytes)
{
    unsigned long flags;
    struct selector *s = ps->context;
    struct path_info *pi = NULL;

    spin_lock_irqsave(&s->lock, flags);
    if (!list_empty(&s->valid_paths)) {
<font color="blue">
    select and move it to the back of the list
</font>
        pi = list_entry(s->valid_paths.next, struct path_info, list);
        list_move_tail(&pi->list, &s->valid_paths);
    }
    spin_unlock_irqrestore(&s->lock, flags);

    return pi ? pi->path : NULL;
}

Except for round-robin selector, there are queue-length and service-time ones
which looks smarter.

</pre>
</font>
</p>

<h3><a name="path_group">path group</a></h3>
<p>
<font size="2">
<pre>
http://www.linux-admins.net/2010/10/multipath-usage-guide-for-san-in-rhel.html
Determines how the path group(s) are formed using the available paths. There are five different policies:
<ul>
<li> multibus: One path group is formed with all paths to a LUN. Suitable for devices that are in Active/Active mode.
<li> failover: Each path group will have only one path.
<li> group_by_serial: One path group per storage controller(serial). All paths that connect to the LUN through a controller are assigned to a <li> path group. Suitable for devices that are in Active/Passive mode.
<li> group_by_prio: Paths with same priority will be assigned to a path group.
<li> group_by_node_name: Paths with same target node name will be assigned to a path group.
</ul>
</pre>
</font>
</p>

<h3><a name="device_handler">device handler</a></h3>
<p>
<font size="2">
</font>
</p>



</body>
</html>
