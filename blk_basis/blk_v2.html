<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>BLK_V2</title>
</head>
<body>
<div>
    <h1>BLK_V2</h1> 
</div>
<p>
<font size="2">
<a href="#qos">QOS</a>
<ul>
<li> <a href="#qos_wbt">WBT</a>
<li> <a href="#qos_iocost">IOCost</a>
<ul>
<li> <a href="#qos_iocost_vtime">vtime</a>
</ul>
</ul>

<a href="#blk_cgroup">Cgroup</a>
<ul>
<li><a href="#blkcg_and_blkg">blkcg and blkg</a>
<li><a href="#lifecycle_of_blkg">lifecycle of blkg</a>
</ul>

<a href="#system">System</a>
<ul>
<li> <a href="#iowait">iowait</a>
</ul>

<a href="#bfq">BFQ</a>
<ul>
<li> <a href="#bfq_data_structure">Data Structure</a>
</ul>

</font>
</p>

<h2><a name="qos">QOS</a></h2>
<hr style="height:3px;border:none;border-top:3px solid black;" />
<p>
<font size="2">
<pre>
blk_mq_make_request
       |
blk_mq_sched_bio_merge  rq_qos_merge
       |
       [X]     rq_qos_throttle    <font color="blue">//The point to throttle the input of IO</font>
       |        - wbt, iolatency, iocost
       | 
__blk_mq_alloc_request
       |
blk_mq_start_request rq_qos_issue [S] // The point where the IO is issued to lldd
       |              - wbt
       .
       .
blk_mq_end_request()
       |
    bio_endio  rq_qos_done_bio [S]   <font color="blue"> //The point where the bio is ended</font>
       |        - iocost, iolatency
       |                            
__blk_mq_end_request rq_qos_done [S] <font color="blue"> //The point where the req is ended</font>
                - wbt, iocost

[X]  block task issueing IO
[S]  time and data stastics
</pre>
</font>
</p>

<h3><a name="qos_iocost">IOCost</a></h3>
<hr style="height:3px;border:none;border-top:1px solid black;" />
<h4><a name="qos_iocost_vtime">vtime</a></h4>
<p>
<font size="2">
<ul>
<li> absolute cost of a bio
<pre>
Let's look at what factors are involved,
calc_vtime_cost_builtin()
---
<font color="blue">
    // <B>[1]</B> size of the bio
</font>
    u64 pages = max_t(u64, bio_sectors(bio) >> IOC_SECT_TO_PAGE_SHIFT, 1);

<font color="blue">
    // <B>[2]</B> read and write, does the ssd and hdd need different values ?
</font>
    switch (bio_op(bio)) {
    case REQ_OP_READ:
        coef_seqio    = ioc->params.lcoefs[LCOEF_RSEQIO];
        coef_randio    = ioc->params.lcoefs[LCOEF_RRANDIO];
        coef_page    = ioc->params.lcoefs[LCOEF_RPAGE];
        break;
    case REQ_OP_WRITE:
        coef_seqio    = ioc->params.lcoefs[LCOEF_WSEQIO];
        coef_randio    = ioc->params.lcoefs[LCOEF_WRANDIO];
        coef_page    = ioc->params.lcoefs[LCOEF_WPAGE];
        break;
    default:
        goto out;
    }

    if (iocg->cursor) {
        seek_pages = abs(bio->bi_iter.bi_sector - iocg->cursor);
        seek_pages >>= IOC_SECT_TO_PAGE_SHIFT;
    }
<font color="blue">
    // <B>[3]</B> seek distance, it will decide the io is sequential or random
</font>
    if (!is_merge) {
        if (seek_pages > LCOEF_RANDIO_PAGES) {
            cost += coef_randio;
        } else {
            cost += coef_seqio;
        }
    }

    cost += pages * coef_page;
---
In theory, to get the cost of a bio, we need to consider following factors,
(1) io size, no matter sdd or hdd, iosize is the main factor to influence the cost
(2) read or write, this is more sensitive on ssd due to the wear leveling
(3) seek distance, this is more sensitive on hdd and even it could dominate the
    cost of an IO

Look into to the calc_vtime_cost_builtin(), the seek distance is only used to
decide random and sequential. This is not so accurate on both ssd and hdd.

This io cost will be charged to the blkg when issue


</pre>
<li> actual cost of a bio
<pre>
adjust_inuse_and_calc_cost()
---
    current_hweight(iocg, NULL, &hwi);
    cost = DIV64_U64_ROUND_UP(abs_cost * WEIGHT_ONE, hwi);
<font color="red" size="3">
    <U>//Bigger hweight Slower vtime</U>
</font>
---

ioc_rqos_throttle()
---
    if (!waitqueue_active(&iocg->waitq) && !iocg->abs_vdebt &&
        time_before_eq64(vtime + cost, now.vnow)) {
        iocg_commit_bio(iocg, bio, abs_cost, cost);
        ---
            bio->bi_iocost_cost = cost;
<font color="blue">
            //commit to the vtime per cgroup
</font>
            <U>atomic64_add(cost, &iocg->vtime);</U>
        ---
        return;
    }
---
</pre>
<li> device vtime
<pre>
device vtime is based on wallclock.
<font color="red">
1s worth of vtime is 2^37, namely, 1 nanosecond is about 2^7 vtime
</font>
ioc_now()
---
    now->now_ns = ktime_get();
    now->now = ktime_to_us(now->now_ns);
<font color="blue">
    // use vtime_rate to adjust the speed of device time.
    // for example, a faster device time would slow down the IO
</font>
    now->vrate = atomic64_read(&ioc->vtime_rate);
    do {
        seq = read_seqcount_begin(&ioc->period_seqcount);
        now->vnow = ioc->period_at_vtime +
            (now->now - ioc->period_at) * now->vrate;
    } while (read_seqcount_retry(&ioc->period_seqcount, seq));
---
</pre>
</ul>
</font>
</p>

<h3><a name="qos_wbt">WBT</a></h3>
<hr style="height:3px;border:none;border-top:1px solid black;" />
<p>
<font size="2">
In IO stack, read is always priority against write. 
<ul>
<li> Who to be throttled
<pre>
wbt_wait()
---
    flags = bio_to_wbt_flags(rwb, bio);
<font color="red">
    if (!(flags & WBT_TRACKED)) {
</font>
        ...
        return;
    }
---
The one who doesn't have WBT_TRACKED is throttled.
bio_to_wbt_flags()
---
    if (bio_op(bio) == REQ_OP_READ) {
        flags = WBT_READ;<font color="red">[1]</font>
    } else if (wbt_should_throttle(rwb, bio)) {
        ...
        flags |= WBT_TRACKED;
    }
---

wbt_should_throttle()
---
    switch (bio_op(bio)) {
    case REQ_OP_WRITE:
<font color="blue">
        /*
         * Don't throttle WRITE_ODIRECT
         * Actually, not only WRITE_ODIRECT, REQ_SYNC | REQ_IDLE has been a flag
         * to tell wbt that "don't throttle me" <font color="red">[2]</font>
         */
</font>>
        if ((bio->bi_opf & (REQ_SYNC | REQ_IDLE)) == (REQ_SYNC | REQ_IDLE))
            return false;
        fallthrough;
    case REQ_OP_DISCARD: 
        return true; <font color="red">[3]</font>
    default:
        return false;
    }
---

[1] read is not throttled
[2] dio_bio_write_op(), do_blockdev_direct_IO(),  iomap_dio_bio_actor()...
[3] buffer write and discard is always throttled
</pre>
<li> How to throttle
<pre>
This happens before allocate tag in blk_mq_make_request()
The task to be throttled will <U>fall into sleep.</U>

rq_qos_throttle()
  -> wbt_wait()
    -> __wbt_wait()
      -> wbt_inflight_cb()
        -> get_limit()
        ---
        if ((rw & REQ_OP_MASK) == REQ_OP_DISCARD)
            return rwb->wb_background;

        if ((rw & REQ_HIPRIO) || wb_recent_wait(rwb) || current_is_kswapd())
            limit = rwb->rq_depth.max_depth;
        else if ((rw & REQ_BACKGROUND) || close_io(rwb)) {
            limit = rwb->wb_background;
        } else 
            limit = rwb->wb_normal;

        return limit;
        ---

<U>Different write IO has different limit, this is the core of wbt</U>
[1] REQ_HIPRIO  (REQ_SYNC | REQ_META | REQ_PRIO)
[2] wb_recent_wait()
    ---
<font color="blue">
        /*
          * If a task was rate throttled in balance_dirty_pages() within the last
          * second or so, use that to indicate a higher cleaning rate.
          */

         // wb->dirty_sleep is set in balance_dirty_pages() when a task fall into
         // sleep due to dirty page limit. This tell us that we should work
         // harder to flush the dirty pages out.
</font>
        struct bdi_writeback *wb = &rwb->rqos.q->backing_dev_info->wb;

        return time_before(jiffies, wb->dirty_sleep + HZ);
    ---
[3] current_is_kswapd(), kswapd is trying to flush dirty pages out to relcaim memory.

[4] REQ_BACKGROUND
    wbc_to_write_flags()
    ---
    if (wbc->sync_mode == WB_SYNC_ALL)
        flags |= REQ_SYNC;
    else if (wbc->for_kupdate || wbc->for_background)
<font color="red">        flags |= REQ_BACKGROUND;</font>

    return flags
    ---

We have seen that there are 3 different limit, max_depth, wb_background and wb_normal.
And the policy adapts different IO type to different limit.

<U>ITOW, there are 3 counters to record the number of inflight IOs,</U>
get_rq_wait()
---
    if (wb_acct & WBT_KSWAPD)
        return &rwb->rq_wait[WBT_RWQ_KSWAPD];
    else if (wb_acct & WBT_DISCARD)
        return &rwb->rq_wait[WBT_RWQ_DISCARD];

    return &rwb->rq_wait[WBT_RWQ_BG];
---

[1] kswapd and discard have their own counters.
[2] all the ther kind of IO share the same counters.
    this means that, <U>the IO types that has higher limit could starve the one with lower limits.</U>

<font size="2"><U>The counters is increased</U> in</font>
__wbt_wait()
  -> rq_qos_wait()
      ---
        has_sleeper = wq_has_sleeper(&rqw->wait);
        if (!has_sleeper && acquire_inflight_cb(rqw, private_data))
<font color="blue">
        // Two points here,
        // [1] if there has been a sleeper here, don't try any more
        // [2] acquire_inflight_cb is a callback it is wbt_inflight_cb here
</font>
            return;

        prepare_to_wait_exclusive(&rqw->wait, &data.wq, TASK_UNINTERRUPTIBLE);
        has_sleeper = !wq_has_single_sleeper(&rqw->wait);
        do {
            /* The memory barrier in set_task_state saves us here. */
            if (data.got_token)
                break;
            io_schedule();
            has_sleeper = true;
            set_current_state(TASK_UNINTERRUPTIBLE);
        } while (1);
        finish_wait(&rqw->wait, &data.wq);
      ---
<font size="2">and <U>decreased in</U></font>
wbt_done()
  -> __wbt_done() //<font color="blue">wbt_is_tracked(), set in rq_qos_track()</font>
    -> get_rq_wait() //<font color="blue">get the rq_wait(the counter) instance associated with the kind of IO</font>
    -> wbt_rqw_done()
    ---
<font color="red">
    inflight = atomic_dec_return(&rqw->inflight);
</font>
    if (wb_acct & WBT_DISCARD)
        limit = rwb->wb_background;
    else if (rwb->wc && !wb_recent_wait(rwb))
        limit = 0;
    else
        limit = rwb->wb_normal;
<font color="blue">
    /*
     * Don't wake anyone up if we are above the normal limit.
     */
</font>
    if (inflight && inflight >= limit)
        return;

    if (wq_has_sleeper(&rqw->wait)) {
        int diff = limit - inflight;

        if (!inflight || diff >= rwb->wb_background / 2)
            wake_up_all(&rqw->wait);
    }

    ---
</pre>
<li> The policy to adapt the limit
<pre>
The wb_normal and wb_background is calculated in calc_wb_limits()
calc_wb_limits()
---
    if (rwb->min_lat_nsec == 0) {
        rwb->wb_normal = rwb->wb_background = 0;
    } else if (rwb->rq_depth.max_depth <= 2) {
        rwb->wb_normal = rwb->rq_depth.max_depth;
        rwb->wb_background = 1;
    } else {
        rwb->wb_normal = (rwb->rq_depth.max_depth + 1) / 2;
        rwb->wb_background = (rwb->rq_depth.max_depth + 3) / 4;
    }
---
[1] when min_lat_nsec is set to zero, rwb->wb_normal is set to zero and wbt is
    disabled. Refer to <font color="red">rwb_enabled()</font>
[2] wb_normal = max_depth / 2
    wb_background = max_depth /4
    Minimum of all is 1

Let's look at how to calculate the max_depth.
rq_depth_calc_max_depth()
---
        depth = min_t(unsigned int, rqd->default_depth,
                  rqd->queue_depth);
        if (rqd->scale_step > 0)
            depth = 1 + ((depth - 1) >> min(31, rqd->scale_step));
        else if (rqd->scale_step < 0) {
            unsigned int maxd = 3 * rqd->queue_depth / 4;

            depth = 1 + ((depth - 1) << -rqd->scale_step);
            if (depth > maxd) {
                depth = maxd;
                ret = true;
            }
        }

        rqd->max_depth = depth;
---
[1] scale_step == 0 is our default state.
[2] If we have suffered latency spikes, step will be > 0,
    and we shrink the allowed write depths.
[3] If step is < 0, we're only doing writes, and we allow a   '
    temporarily higher depth to increase performance.
</pre>
<li> How to adapt scale_step
<pre>
There are two action to adapt scale_step,
[1] scale_down to <B>increase</B> it, limit the write
    latency_exceeded()
    ---
<font color="blue">
    /*
     * If the 'min' latency exceeds our target, step down.
     */
</font>
    if (stat[READ].min > rwb->min_lat_nsec) {
        return LAT_EXCEEDED;
    }
    ---

[2] scale up to <B>decrease</B> it, relax the write
    latency_exceeded()
    ---
<font color="blue">
    // wbt only works when there is mixed read/write workload
</font>
    if (!stat_sample_valid(stat)) {
<font color="blue">
        /*
         * If we had writes in this stat window and the window is
         * current, we're only doing writes. If a task recently
         * waited or still has writes in flights, consider us doing
         * just writes as well.
         */
</font>
        if (stat[WRITE].nr_samples || wb_recent_wait(rwb) ||
            wbt_inflight(rwb))
            return LAT_UNKNOWN_WRITES;
        return LAT_UNKNOWN;
    }
    ---

   Or if the read latency is OK when read/write mixed, we also scale up.

<font color="red">What if there is only read ?</font>
LAT_UNKNOWN will be returned by latency_exceeded() and then
wb_timer_fn()
---
    case LAT_UNKNOWN:
        if (++rwb->unknown_cnt < RWB_UNKNOWN_BUMP)
            break;
        /*
         * We get here when previously scaled reduced depth, and we
         * currently don't have a valid read/write sample. For that'
         * case, slowly return to center state (step == 0).
         */
        if (rqd->scale_step > 0)
            scale_up(rwb);
        else if (rqd->scale_step < 0)
            scale_down(rwb, false);
        break;
---
</pre>
</ul>
</font>
</p>

<h2><a name="blk_cgroup">Block Cgroup</a></h2>
<hr style="height:3px;border:none;border-top:3px solid black;" />
<h3><a name="blkcg_and_blkg">blkcg and blkg</a></h3>
<p>
<font size="2">
There are two basic structures for block cgroup.
<ul>
<li> blkcg, namely the css
<pre>
per-blkcg data which is independent from any request_queue associated to it.
css is embedded in blkcg

io_cgrp_subsys.css_alloc
blkcg_css_alloc()
---
    for (i = 0; i < BLKCG_MAX_POLS ; i++) {
        struct blkcg_policy *pol = blkcg_policy[i];

        if (!pol || !pol->cpd_alloc_fn)
            continue;

        cpd = pol->cpd_alloc_fn(GFP_KERNEL);
        blkcg->cpd[i] = cpd;
        cpd->blkcg = blkcg;
        cpd->plid = i;
        if (pol->cpd_init_fn)
            pol->cpd_init_fn(cpd);
    }
    ...
    return &blkcg->css;
---

And the cpd is blkcg_policy_data. cpd can also be allocated when a blkcg policy
is installed (blkcg_policy_register).
</pre>
<li> blkcg_gq (blkg)
<pre>
A blkcg_gq (blkg) is association between a block cgroup (blkcg) and a
request_queue (q).  This is used by blkcg policies which need to track
information per blkcg - q pair.

__swap_writepage() -> bio_associate_blkg_from_page()
bio_set_dev() -> bio_associate_blkg()
wbc_init_bio()
  -> bio_associate_blkg_from_css()
    -> blkg_tryget_closest()
      -> blkg_lookup_create()
        -> blkg_create()

blkcg_activate_policy()
</pre>
</ul>
In summary
<pre>

      io_cgrp_subsys
                   
      blkcg css -> cpd of iocost               request_queue
          |        cpd of bfq                       |
       blkg_list                                 blkg_list
               \______________ ________________/
                              v
                            blkg -> pd of blk-throttle
                                    pd of bfq
                                    pd of iolatency          
                                    pd of iocost
</pre>
</font>
</p>

<h3><a name="lifecycle_of_blkg">lifecycle of blkg</a></h3>
<p>
<font size="2">
<ul>
<li> initial one
<pre>
blkg_alloc()
---
    percpu_ref_init(&blkg->refcnt, blkg_release, 0, gfp_mask);
---
</pre>
<li> get
<pre>
(1) held by children, see blkg_create()
(2) held by every bio, including cloned ones, 
    see bio_associate_blkg_from_css() and bio_clone_blkg_association()
</pre>
<li> put
<pre>
(1) put when bio is ended,
    bio_uninit() <- bio_endio()
                 <- bio_free()
                 <- bio_reset() >
(2) put when children blkg is destroyed
</pre>
<li> final one
<pre>
Note !!!
blkg_put() only put one reference of percpu_refcount.
If we want blkg_release() to be invoked, we must kill the percpu_refcount.

blkg_destroy() <- blkg_destroy_all() < blkcg_exit_queue()
               <- blkcg_destroy_blkgs() <- blkcg_unpin_online() <- blkcg_css_online() >
</pre>
</ul>
</font>
</p>

<h2><a name="system">System</a></h2>
<hr style="height:3px;border:none;border-top:3px solid black;" />

<h2><a name="iowait">iowait</a></h2>
<p>
<font size="2">

What is I/O wait?
<pre>
I/O wait applies to Unix and all Unix-based systems, including macOS,
FreeBSD, Solaris, and yes Linux.

I/O wait (iowait) is the percentage of time that the CPU (or CPUs) were idle
during which the system had pending disk I/O requests. (Source: man sar)
The top man page gives this simple explanation: “I/O wait = time waiting for I/O completion.”
In other words, the presence of I/O wait tells us that the system is idle at a time
when it could be processing outstanding requests.

“iowait shows the percentage of time that the CPU or CPUs were idle
during which the system had an outstanding disk I/O request.” –  iostat man page

When using Linux top and other tools, you’ll notice that a CPU (and its cores) operate
in the following states: us (user), sy (system), id (idle), ni (nice), si (software interrupts),
hi (hardware interrupts), st (steal) and wa (wait). Of these, the user, system, idle, and wait
values should add up to 100%. Note that “idle” and “wait” are not the same. “Idle” CPU means
there is no workload present while, on the other hand, “wait” (iowait) indicates when the CPU
is waiting in an idle state for outstanding requests.

If the CPU is idle, the kernel will ascertain if there are any pending I/O requests (i.e., SSD or NFS)
originating from the CPU. If there are, then the ‘iowait’ counter is incremented. If there’s nothing
pending, then the ‘idle’ counter is incremented instead.
</pre>
There are two key points of iowait time.
<ul>
<li> idle
<pre>
There is no running tasks on the cpu, but only the idle task
</pre>
<li> tasks sleeping for outstanding IOs
<pre>
There are tasks in TASK_UNINTERRUPTIBLE state to wait outstanding IO's completion.
</pre>
</ul>
How to account the iowait time in kernel ?
<ul>
<li> nr_iowait
<pre>
It will be increased by

io_schedule()
  -> io_schedule_prepare()
    -> current->in_iowait = 1
  -> __schedule()
  ---
              if (prev->in_iowait) {
<font color="red">                atomic_inc(&rq->nr_iowait);</font>
                delayacct_blkio_start();
            }
  ---

And decreased by
try_to_wake_up()
---
    if (p->in_iowait) {
        delayacct_blkio_end(p);
        atomic_dec(&task_rq(p)->nr_iowait);
    }
---
</pre>
<li> NOHZ is disabled
<pre>
update_process_times()
  -> account_process_tick()
  ---
    if (user_tick)
        account_user_time(p, cputime);
    else if (<font color="red">(p != this_rq()->idle)</font> || (irq_count() != HARDIRQ_OFFSET))
        account_system_time(p, HARDIRQ_OFFSET, cputime);
    else
        account_idle_time(cputime);
        ---
        if (atomic_read(&rq->nr_iowait) > 0)
            cpustat[CPUTIME_IOWAIT] += cputime;
        else
            cpustat[CPUTIME_IDLE] += cputime;
        ---
  ---
<font color="red">  -> scheduler_tick()</font>
</pre>
<li> NOHZ is enabled
<pre>
tick_nohz_idle_exit()
  -> tick_nohz_stop_idle()
    -> update_ts_time_stats()
    ---
    if (ts->idle_active) {
        delta = ktime_sub(now, ts->idle_entrytime);
<font color="red">        if (nr_iowait_cpu(cpu) > 0)</font>
            ts->iowait_sleeptime = ktime_add(ts->iowait_sleeptime, delta);
        else
            ts->idle_sleeptime = ktime_add(ts->idle_sleeptime, delta);
        ts->idle_entrytime = now;
    }
    ---
The question is that,
if there is only one task on this cpu, and the try_to_wake_up has decreased the
nr_iowait to zero before this. The time would not be account as iowait
</pre>
<li> get_iowait_time()
<pre>
    u64 iowait, iowait_usecs = -1ULL;

    if (cpu_online(cpu))
        iowait_usecs = get_cpu_iowait_time_us(cpu, NULL);

    if (iowait_usecs == -1ULL)
<font color="red">
        /* !NO_HZ or cpu offline so we can rely on cpustat.iowait */
</font>
        iowait = kcs->cpustat[CPUTIME_IOWAIT];
    else
        iowait = iowait_usecs * NSEC_PER_USEC;

    return iowait;
</pre>
</ul>
</font>
</p>

<h2><a name="bfq">BFQ</a></h2>
<hr style="height:3px;border:none;border-top:3px solid black;" />

<h3><a name="bfq_data_structure">Data Structure</a></h2>
<p>
<font size="2">
<pre>

bfq_group -> bfq_service_tree[RT, BE, IDLE] -> rb tree [      bfqq-async      ]
                                                              /        \
                                                            bfqq-1     bfq-3
                                                            /  \         \
                                                        bfqq-0 bfqq-2    bfq-4

Note: All of the async req shares the same bfqqs in its bfq_group->async_bfqq.
      The others would create a bfqq of theirselves. Except for REQ_OP_READ and
      REQ_SYNC, REQ_FUA and REQ_PREFLUSH are also deemed as sync. This means
      commit kthread of jbd2 of ext4 or cil push work of xfs would have their
      own's bfqq.
    

</pre>
</font>
</p>

<h3><a name="bfq_vtime">virtual time</a></h2>
<p>
<font size="2">
The bfq wf2q algorithm is similar with the task scheduling algorithm cfs.<br/>
And there is also a virtual time here. <br/>
However, it is not a real time but someting converted from the IO sectors.
<pre>
static u64 bfq_delta(unsigned long service, unsigned long weight)
{
    return div64_ul((u64)service << WFQ_SERVICE_SHIFT, weight);
}
</pre>
The service is the IO sectors dispatched from a bfq and its bfqg.
<pre>
__bfq_dispatch_request()
  -> bfq_dispatch_rq_from_bfqq()
  ---
    struct request *rq = bfqq->next_rq;
    unsigned long service_to_charge;

    service_to_charge = bfq_serv_to_charge(rq, bfqq);

    bfq_bfqq_served(bfqq, service_to_charge);
  ---

bfq_serv_to_charge()
---
<font color="blue">
    // bfq_async_charge_factor is 3, so async IO would get a bigger value here
    // 
</font>
    if (bfq_bfqq_sync(bfqq) || bfqq->wr_coeff > 1 ||
        bfq_asymmetric_scenario(bfqq->bfqd, bfqq))
        return blk_rq_sectors(rq);

    return blk_rq_sectors(rq) * bfq_async_charge_factor;
---

bfq_bfqq_served()
---
    for_each_entity(entity) {
        st = bfq_entity_service_tree(entity);
<font color="blue">
        // Charge the service with sectors to be dispatched
</font>
        entity->service += served;
<font color="blue">
        // Base virtual time of the service tree
</font>
        st->vtime += bfq_delta(served, st->wsum);
    }
---




</pre>
</font>
</p>

</body>
</html>


