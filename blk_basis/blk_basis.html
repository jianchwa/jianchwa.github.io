<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Block Basis</title>
</head>
<body>
<div>
    <h1>Block Basis</h1> 
</div>
<p>
<font size="2">
<a href="#Block_legacy">Block legacy</a>
<ul>
<a href="#Tag">Tag</a>
</ul>
<a href="#BIO">BIO</a>
<ul>
<li><a href="#Setup_and_complete_a_bio">Setup and complete a bio</a>
<li><a href="#Bio_operations">Bio operations</a>
</ul>
<a href="#Merge">Merge</a><br/>
<a href="#FLUSH_and_FUA">FLUSH and FUA</a><br/>
<a href="#Queue_state_flags">Queue state flags</a><br/>
<a href="#WBT">WBT</a><br/>
<a href="#DISK_and_BLKDEV">DISK and BLKDEV</a><br/>
<a href="#blk_integrity">blk_integrity</a><br/>
<a href="#blk-loop">blk loop</a>
<ul>
<li><a href="#How_to_create">How to create</a>
<li><a href="#Kthread_or_workqueue">Kthread or Workqueue ?</a>
<li><a href="#DIO_AIO_on_backing_file">DIO & AIO on backing file</a>

</ul>
<a href="#blk-stats">blk-stats</a>
</font>
</p>

<h2><a name="Block_legacy">Block legacy</a></h2>
<hr style="height:5px;border:none;border-top:3px solid black;" />
<h3><a name="Tag">Tag</a></h3>
<p>
<font size="2">
There is also a tag mechanism in block legacy. Quote comment from blk-mq about tagging.
<pre>
Device command tagging was first introduced with hardware supporting native command queuing. A tag is an integer value that uniquely identifies the position of the block IO in the driver submission queue, so when completed the tag is passed back from the device indicating which IO has been completed. This eliminates the need to perform a linear search of the in-flight window to determine which IO has completed.
</pre>
We don't look into how to implement it but just how to employ it in block legacy and do some comparing with tagging in blk-mq.<br/>
How to use it in driver level ?
<pre>
static inline struct scsi_cmnd *scsi_host_find_tag(struct Scsi_Host *shost,
        int tag)
{
    struct request *req = NULL;

    if (tag == SCSI_NO_TAG)
        return NULL;

    if (shost_use_blk_mq(shost)) {
        u16 hwq = blk_mq_unique_tag_o_hwq(tag);

        if (hwq < shost->tag_set.nr_hw_queues) {
            req = blk_mq_tag_to_rq(shost->tag_set.tags[hwq],
                blk_mq_unique_tag_to_tag(tag));
        }
    } else {
        req = <font color="red">blk_map_queue_find_tag(shost->bqt, tag)</font>;
    }

    if (!req)
        return NULL;
    return blk_mq_rq_to_pdu(req);
}
</pre>
A reverse mapping tag -> req -> driver pdu <br/>

How to assign tag to a req ?
<pre>
scsi_request_fn()
>>>>
        /*
         * Remove the request from the request list.
         */
        if (!(blk_queue_tagged(q) && !blk_queue_start_tag(q, req)))
            blk_start_request(req);
        <font color="blue">/*
         blk_queue_tagged() will check QUEUE_FLAG_QUEUED in the q->flags, means the hardware support native command queuing.
         blk_queue_start_tag() will try to assign tag for this rq, if tags has been used up, return 1.
         otherwise,
         bqt->next_tag = (tag + 1) % bqt->max_depth;
         rq->rq_flags |= RQF_QUEUED; //indicates tag has been assigned
         rq->tag = tag;
         bqt->tag_index[tag] = rq;
         blk_start_request(rq);
         list_add(&rq->queuelist, &q->tag_busy_list);
         */</font>
>>>>
        /*
         * We hit this when the driver is using a host wide
         * tag map. <font color="red">For device level tag maps the queue_depth check
         * in the device ready fn would prevent us from trying
         * to allocate a tag</font>. Since the map is a shared host resource
         * we add the dev to the starved list so it eventually gets
         * a run when a tag is freed.
         */
        if (blk_queue_tagged(q) && !(req->rq_flags & RQF_QUEUED)) {
            spin_lock_irq(shost->host_lock);
            if (list_empty(&sdev->starved_entry))
                list_add_tail(&sdev->starved_entry,
                          &shost->starved_list);
            spin_unlock_irq(shost->host_lock);
            goto not_ready;
        }
>>>>
 not_ready:
    <font color="blue">/*
     * The tag here looks like the driver tag in blk-mq.
     * In block legacy, the req is requeued and inserted to the head of q->queue_head directly.
     * In blk-mq, the action is similar, refer to blk_mq_dispatch_rq_list. (but __blk_mq_try_issue_directly looks like not assigned with this.)
     */</font>
    spin_lock_irq(q->queue_lock);
    blk_requeue_request(q, req);
    atomic_dec(&sdev->device_busy);
>>>>
</pre>
</font>
</p>

<h2><a name="BIO">BIO</a></h2>
<hr style="height:5px;border:none;border-top:3px solid black;" />
<p>
<font size="2">
Let's look into the _basic unit_ in block layer, the bio.<br/>
We could deem there is a bio layer between the fs and block layer.
<pre>
                         
                FS LAYER
     ------------------------------------------------
                          | submit_bio 
                          |
                          V generic_make_request <-------+
     ------------------------------------------------    |
                             blk-throttl                 |
                BIO LAYER    bio remap +--> partition    |
                                       |                 |
                                       +--> bio based device mapper (stackable)
    -------------------------------------------------    |
                          |                              |
                          V  blk_queue_bio/blk_mq_make_request

                BLOCK LAGACY/BLK-MQ
</pre>
<strong>The basic architecture of a bio.</strong>
<pre>
request->bio __                    
               \                  
                \     bio        
                 \   ________    
                  ->| bi_next        next bio in one request, the blocks in these bios should be contigous on disk
                    |
                    | bi_disk        gendisk->request_queue 
                    |
                    | bi_partno      partition NO.
                    |
                    | bi_opf         bio_op, req_flag_bits, same with req->cmd_flags
                    |
                    | bi_phys_segments  Number of segments in this BIO after physical address coalescing is performed.
                    |
                    | bi_end_io　　　blk_update_request->req_bio_endio->bio_endio
                    |
                    | bi_vcnt        how many bio_vec's
                    | bi_max_vecs    max bio_vecs can hold
                    | bi_io_vec      pointer to bio_io_vec list    
                    |         \     　________    
                    |          --->  | bv_page       
                    |                | bv_len        
                    |                | bv_offset     
                    |                 ________       
                    |                | bv_page       
                    |                | bv_len        
                    |                | bv_offset    These two pages could be non physical contigously
                    |                               But the corresponding blocks on storage disk should be contigous.
                    | bi_pool        as its name
                    | 
                    | bi_iter        the current iterating status in bio_vec list
                                      ___________
                                     | bi_sector    device address in 512 byte sectors
                                     | bi_size      residual I/O count
                                     | bi_idx       current index into bvl_vec
                                     | bi_done      number of bytes completed
                                     | bi_bvec_done number of bytes completed in current bvec


(Some members associated with cgroup,blk-throttle,merge-assistant are ignored here.)
</pre>
</font>
</p>

<h3><a name="Setup_and_complete_a_bio">Setup and complete a bio</a></h3>
<p>
<font size="2">
Let's take the submit_bh_wbc() as example to show how to setup a bio
<pre>
static int submit_bh_wbc(int op, int op_flags, struct buffer_head *bh,
			 enum rw_hint write_hint, struct writeback_control *wbc)
{
	struct bio *bio;
	>>>>
	bio = bio_alloc(GFP_NOIO, 1); // the second parameter is the count of bvec

	if (wbc) {
		wbc_init_bio(wbc, bio);
		wbc_account_io(wbc, bh->b_page, bh->b_size);
	}

	bio->bi_iter.bi_sector = bh->b_blocknr * (bh->b_size >> 9);
	bio_set_dev(bio, bh->b_bdev);
	//(bio)->bi_disk = (bdev)->bd_disk;
	//(bio)->bi_partno = (bdev)->bd_partno;
	bio->bi_write_hint = write_hint;

	bio_add_page(bio, bh->b_page, bh->b_size, bh_offset(bh));
	>>>>//Fs with blocksize smaller than pagesize, could reach here.
		if (bio->bi_vcnt > 0) {
			bv = &bio->bi_io_vec[bio->bi_vcnt - 1];

			if (page == bv->bv_page &&
		   	 offset == bv->bv_offset + bv->bv_len) {
				bv->bv_len += len;
				goto done;
			} 
		} //merged with previous one 

		if (bio->bi_vcnt >= bio->bi_max_vecs)
			return 0;

		bv		= &bio->bi_io_vec[bio->bi_vcnt];
		bv->bv_page	= page;
		bv->bv_len	= len;
		bv->bv_offset	= offset;

		bio->bi_vcnt++;
	done:
		bio->bi_iter.bi_size += len;
	>>>>
	BUG_ON(bio->bi_iter.bi_size != bh->b_size);

	bio->bi_end_io = end_bio_bh_io_sync;
	bio->bi_private = bh; //reverse mapping to the bh

	/* Take care of bh's that straddle the end of the device */
	guard_bio_eod(op, bio);

	if (buffer_meta(bh))
		op_flags |= REQ_META;
	if (buffer_prio(bh))
		op_flags |= REQ_PRIO;
	bio_set_op_attrs(bio, op, op_flags);
	
	submit_bio(bio);
	return 0;
}
</pre>
Most of the information to construct a bio is from the bh. If we want to dig deeper, we have to look into how to setup a bh.<br/>
<pre>
static int
grow_dev_page(struct block_device *bdev, sector_t block,
	      pgoff_t index, int size, int sizebits, gfp_t gfp)
{
	>>>>
	page = find_or_create_page(inode->i_mapping, index, gfp_mask);
		-> pagecache_get_page()
			-> __page_cache_alloc() //no_page case
				-> __alloc_pages_node(n, gfp, 0);
	/*
	 <strong>The pages of page cache are allocated one by one</strong>. It's more flexible to
	 map and unmap, page in and swap out. And in the past, the memory is limited, there is not
	 enougth contiguous pages to take advantage of.
	 */
	BUG_ON(!PageLocked(page));
	>>>>`
	/*
	 * Allocate some buffers for this page
	 */
	bh = alloc_page_buffers(page, size, true);

	/*
	 * Link the page to the buffers and initialise them.  Take the
	 * lock to be atomic wrt __find_get_block(), which does not
	 * run under the page lock.
	 */
	spin_lock(&inode->i_mapping->private_lock);
	link_dev_buffers(page, bh);
	end_block = init_page_buffers(page, bdev, (sector_t)index << sizebits,
			size);
	>>>>
	do {
		if (!buffer_mapped(bh)) {
			init_buffer(bh, NULL, NULL);
			bh->b_bdev = bdev;
			bh->b_blocknr = block;
			if (uptodate)
				set_buffer_uptodate(bh);
			if (block < end_block)
				set_buffer_mapped(bh);
		}
		block++;
		bh = bh->b_this_page;
	} while (bh != head);
	>>>>
	spin_unlock(&inode->i_mapping->private_lock);
done:
	ret = (block < end_block) ? 1 : -ENXIO;
failed:
	unlock_page(page);
	put_page(page);
	return ret;
}
</pre>

One page from pagecache could be broken up into several bh's based on the blocksize of the associated filesystem (sb->s_blocksize). <strong>One bh corresponds to one block in disk</strong>. Then echo bh will be used to constructed a bio and submitted to block layer. At the moment, the bio only contain one bio_vec pointing to page of the bh. This is the classical path to setup a bio. Nowadays, some filesystems would like to create bios itself, during the procedure, the bio containing multiple bio_vec maybe created. For example:
<pre>
static int io_submit_add_bh(struct ext4_io_submit *io,
			    struct inode *inode,
			    struct page *page,
			    struct buffer_head *bh)
{
	int ret;

	if (io->io_bio && bh->b_blocknr != io->io_next_block) {
submit_and_retry:
		ext4_io_submit(io);
	}
	if (io->io_bio == NULL) {
		ret = io_submit_init_bio(io, bh);
		if (ret)
			return ret;
		io->io_bio->bi_write_hint = inode->i_write_hint;
	}
	ret = bio_add_page(io->io_bio, page, bh->b_size, bh_offset(bh));
	if (ret != bh->b_size)
		goto submit_and_retry;
	wbc_account_io(io->io_wbc, page, bh->b_size);
	io->io_next_block++;
	return 0;
}
</pre>
We could see that: one bio_vec would correspond to part or the whole page. 	

</font>
</p>

<h3><a name="Bio_operations">Bio operations</a></h3>
<p>
<font size="2">
<strong>bio advance</strong>
<pre>
static inline void bio_advance_iter(struct bio *bio, struct bvec_iter *iter,
                    unsigned bytes)
{
    iter->bi_sector += bytes >> 9;
    /* So this is why the bi_sector is located in bio->bi_iter, it could be
     * put forward */
    if (bio_no_advance_iter(bio))
    {/REQ_OP_DISCARD/SECTOR_ERASE/WRITE_SAME/WRITE_ZERO
        iter->bi_size -= bytes;
        iter->bi_done += bytes;
    } else {
        bvec_iter_advance(bio->bi_io_vec, iter, bytes);
        /* TODO: It is reasonable to complete bio with error here. */
    }
}

static inline bool bvec_iter_advance(const struct bio_vec *bv,
        struct bvec_iter *iter, unsigned bytes)
{
    >>>>
    while (bytes) {
        unsigned iter_len = bvec_iter_len(bv, *iter);
        unsigned len = min(bytes, iter_len);

        bytes -= len;
        iter->bi_size -= len; // remaining length
        iter->bi_bvec_done += len; //completed length of current bvec
        iter->bi_done += len; //completed length of this bio

        if (iter->bi_bvec_done == __bvec_iter_bvec(bv, *iter)->bv_len) {
            iter->bi_bvec_done = 0;
            iter->bi_idx++; //push forward the bvec table here
        }
    }
    return true;
}
</pre>
After invoke this function, we could confirm one bio has been finished througth
(bio->bi_iter.bi_size == 0). For example, in blk_update_request()
<pre>
blk_mq_end_request()
    -> blk_update_request()
        -> req_bio_endio()
>>>>
    bio_advance(bio, nbytes);

    /* don't actually finish bio if it's part of flush sequence */
    // when RQF_FLUSH_SEQ is set, the req->end_io would be invoked instead of
    // bio_end.
    if (bio->bi_iter.bi_size == 0 && !(rq->rq_flags & RQF_FLUSH_SEQ))
        bio_endio(bio);
>>>>
</pre>
<strong>bio clone</strong><br/>

in the device mapper stack, the bio will be cloned. Let's look at how to do that.

clone_bio(), clone a new bio contain the sector ~ (sector+len) of original one.
<pre>
static int clone_bio(struct dm_target_io *tio, struct bio *bio,
             sector_t sector, unsigned len)
{
    struct bio *clone = &tio->clone;

    __bio_clone_fast(clone, bio);
    >>>>
        bio->bi_disk = bio_src->bi_disk;
        bio->bi_partno = bio_src->bi_partno;
        bio_set_flag(bio, BIO_CLONED); // a cloned bio
        bio->bi_opf = bio_src->bi_opf;
        bio->bi_write_hint = bio_src->bi_write_hint;
        bio->bi_iter = bio_src->bi_iter;
        bio->bi_io_vec = bio_src->bi_io_vec;
        //The cloned bio will shared a same bvec table with previous one.
        bio_clone_blkcg_association(bio, bio_src);
    >>>>
    if (bio_op(bio) != REQ_OP_ZONE_REPORT)
        bio_advance(clone, to_bytes(sector - clone->bi_iter.bi_sector));
    clone->bi_iter.bi_size = to_bytes(len);
    //cut out the sector ~ (sector+len) part of original one here
    if (unlikely(bio_integrity(bio) != NULL))
        bio_integrity_trim(clone);

    return 0;
}
</pre>

</font>
</p>




<h2><a name="Merge">Merge</a></h2>
<p>
<font size= "2">
The main merging point.
<pre>
<font color="red">blk_mq_sched_try_merge</font>
This is used to merge bio with req.
It is usually in <font color="red">bio submitting</font> path.
elv_merge choose a rq which could merge with a new bio
and return how to merge.
(bio) (req) indicates the new one

if ELEVATOR_BACK_MERGE
    req -> bio -> (bio)
    then try to merge this req with latter one.
    (req) -?-> req

if ELEVATOR_FRONT_MERGE
    req -> (bio) -> bio
    then try to merge this req with former one.
    req -?-> (req)

<font color="red">elv_attempt_insert_merge</font>
This is used to merge req with req.
It is usually in <font color="red">req inserting</font> path.

Both elv_merge and elv_attempt_insert_merge employ <font color="red">q->last_merge
and request_queue elv rqhash </font>to find out contiguous reqs.

</pre>

<strong>Note: req is just a package. The real things are bios in them.</strong>
<br/>
<br/>

attempt_merge is used to merge two reqs (req, next).<br/>
The mainly checking it does:
<ul>
<li> !rq_mergeable(req) || !rq_mergeable(next) ?
<li> req_op(req) != req_op(next) ?
<li> blk_rq_pos(req) + blk_rq_sectors(req) != blk_rq_pos(next) ?
<li> rq_data_dir(req) != rq_data_dir(next) ?
<li> req->rq_disk != next->rq_disk ?
<li> req->write_hint != next->write_hint ?
<li> ll_merge_requests_fn (<font color="red">'ll' here means low level ?</font>)
<ul>
<li> (blk_rq_sectors(req) + blk_rq_sectors(next)) > blk_rq_get_max_sectors(req, blk_rq_pos(req))
<li> total_phys_segments > queue_max_segments(q)
</ul>
</ul>
<br/>
If two requests could be merged with echo other:
<pre>
    req->biotail->bi_next = next->bio;
    req->biotail = next->biotail;

    req->__data_len += blk_rq_bytes(next);

    elv_merge_requests(q, req, next);

    /*
     * 'next' is going away, so update stats accordingly
     */
    blk_account_io_merge(next);

    req->ioprio = ioprio_best(req->ioprio, next->ioprio);
    if (blk_rq_cpu_valid(next))
        req->cpu = next->cpu;

    /*
     * ownership of bio passed from next to req, return 'next' for
     * the caller to free
     */
    next->bio = NULL;
</pre>

Then next one will be freed though __blk_put_request().<br/>

</font>
</p>


<h2><a name="FLUSH_and_FUA">FLUSH and FUA</a></h2>
<hr style="height:5px;border:none;border-top:3px solid black;" />
<p>
<font size="2">
First, we need to know the volatile write cache. <br/>
Quote from Documentation/block/writeback_cache_control.txt 
<pre>
Many storage devices, especially in the consumer market, come with volatile
write back caches.  That means the devices signal I/O completion to the
operating system before data actually has hit the non-volatile storage.  This
behavior obviously speeds up various workloads, but it means the operating
system needs to force data out to the non-volatile storage when it performs
a data integrity operation like fsync, sync or an unmount. >
</pre>

There are two flag set in bio or req to indicate which operation on vwc will be
carried out.
<ul>
<li> REQ_FLUSH, REQ_FLUSH flag indicates a explicit cache flushes. 
<pre>
The REQ_FLUSH flag can be OR ed into the r/w flags of a bio submitted from
the filesystem and will make sure the volatile cache of the storage device
has been flushed before the actual I/O operation is started.  This explicitly
guarantees that previously completed write requests are on non-volatile
storage before the flagged bio starts. In addition the REQ_FLUSH flag can be
set on an otherwise empty bio structure, which causes only an explicit cache
flush without any dependent I/O. 
</pre>
<li> REQ_FUA, REQ_FUA means Force Unit Access.
<pre>
The REQ_FUA flag can be OR ed into the r/w flags of a bio submitted from the
filesystem and will make sure that I/O completion for this request is only
signaled after the data has been committed to non-volatile storage.
</pre>
</ul>

The block device driver need to notify the queue that whether it supports
REQ_FLUSH and REQ_FUA through blk_queue_write_cache(). And the flags will 
be set into queue->queue_flags.
<pre>
void blk_queue_write_cache(struct request_queue *q, bool wc, bool fua)
{
    spin_lock_irq(q->queue_lock);
    if (wc)
        queue_flag_set(QUEUE_FLAG_WC, q);
    else
        queue_flag_clear(QUEUE_FLAG_WC, q);
    if (fua)
        queue_flag_set(QUEUE_FLAG_FUA, q);
    else
        queue_flag_clear(QUEUE_FLAG_FUA, q);
    spin_unlock_irq(q->queue_lock);

    wbt_set_write_cache(q->rq_wb, test_bit(QUEUE_FLAG_WC, &q->queue_flags));
}
</pre>

<strong>How to implement the flush operation</strong><br/>
There are 4 flush sequence flag:
<ul>
<li> REQ_FSEQ_PREFLUSH
<li> REQ_FSEQ_DATA
<li> REQ_FSEQ_POSTFLUSH
<li> REQ_FSEQ_DONE
</ul>

These flush operation life cycle could include any ones of them. blk core will
execute them in sequence. blk_flush_policy() is used to construct this sequence.
Let's see it.
<pre>
static unsigned int blk_flush_policy(unsigned long fflags, struct request *rq)
{
    unsigned int policy = 0;

    if (blk_rq_sectors(rq))
        policy |= REQ_FSEQ_DATA;

    if (fflags & (1UL << QUEUE_FLAG_WC)) {
        if (rq->cmd_flags & REQ_PREFLUSH)
            policy |= REQ_FSEQ_PREFLUSH;
        if (!(fflags & (1UL << QUEUE_FLAG_FUA)) &&
            (rq->cmd_flags & REQ_FUA))
            policy |= REQ_FSEQ_POSTFLUSH;
    }
    return policy;
}
</pre>

Two things need to be emphasized here.
<ul>
<li>REQ_FSEQ_PREFLUSH/POSTFLUSH are only executed when the block device support vwc.
<li>if the device not support fua, blk-core use data+flush pair to simulate it.
</ul>


If blk_flush_policy() just return REQ_FSEQ_DATA, the request can be processed
directly without going through flush machinery. For blk-mq, it will be inserted
into the tail of hctx->dispatch.<br/>

Otherwise, a flush sequence will be started.<br/>
The flush sequence is carried out based on blk_flush_queue->flush_queue[2].
In addition, there are two idx to indicates the current state of the flush_queue.
<ul>
<li>flush_pending_idx
<li>flush_running_idx
</ul>
Both of them only have two values 0/1. At initial state, pending == running.
After kick a flush sequence, the pending_idx is toggled, then the pending_idx become
different from running_idx which means flush is in flight. During the process
while flush is in flight, the new flushes will be queued on pending_idx which is
different from the running_idx. After the flush is completed, the running_idx
is toggled then the running_idx is same with pending_idx again.
<br/>
a preallocated request - flush_rq will do the actual flush work on behalf of the
FLUSH requests. when completed, all the FLUSH request on the running queuee would
be pushed forward to next step.

<pre>
                         (seq = PREFLUSH)
                   new flush rq --
                                  \
                                  |
pending_flush_queue <- rqa0    <--+
                        (seq = PREFLUSH)         <-----+   kick the next flush sequence      
                                                        \
                        (seq = PREFLUSH)                 \
running_flush_queue <- rqb0 <--                           \  
                               \    (seq = POSTFLUSH)     |
                                +-- rqb1              <-+ |
                                                         \|  
    proxy flush_rq                                        | push forward the running 
        (w/ tag borrowd from bqb0, RQF_FLUSH_SEQ)         | FLUSH requests to next step
              \                                          /
               \ requeue -> bypass_insert               /
                \                                      /
                 +--> hctx->dispatch      completion --+

<strong>Note: all the rqs in flush queue are flagged RQF_FLUSH_SEQ</strong>

</pre>

we ignore the REQ_FSEQ_DATA in above diagram. One thing need to be noted here:
<pre>
A sequenced PREFLUSH/FUA request with DATA is completed twice.
Once while executing DATA and again after the whole sequence is
complete.  The first completion updates the contained bio but doesn't
finish it so that the bio submitter is notified only after the whole
sequence is complete.  This is implemented by testing RQF_FLUSH_SEQ in
req_bio_endio().
</pre>

<strong>Talking about the borrowed tag</strong><br/>
Why does the flush_rq borrow tags from the FLUSH request ?

flush_rq is allocated separately, so it is not in the tag_set of blk-mq.

For the non-scheduler case, the FLUSH request has occupied a driver tag and it
depends on the completion of flush_rq. Assume the scenario, all the driver tags
are held by FLUSH requests, consequentially, the flush_rq cannot get driver tag
any more and cannot make the flush sequence forward. A IO hang comes up. To
avoid this, flush_rq could borrow driver tag from the FLUSH requests.

Recently, a commit 923218f (blk-mq: don't allocate driver tag upfront for flush rq
) was introduced, it change the way how to handle the tag borrowing in blk-mq. Let's
look into the patch next. That will be helpful to get deeper into the blk-mq and blk-flush.
<br/><br/>
Before this patch, when with io scheduler, the blk-mq will allocate driver tag ahead
of deliver it to blk-flush. Then blk-flush may borrow this driver tag to the proxy
flush_rq. Then this flush_rq will be queued to hctx->dispatch.
<pre>
blk_mq_make_request()
>>>>
    if (unlikely(is_flush_fua)) {
        blk_mq_put_ctx(data.ctx);
        blk_mq_bio_to_request(rq, bio);
        if (q->elevator) {
            blk_mq_sched_insert_request(rq, false, true, true,
                    true);
        } 
>>>>

blk_mq_sched_insert_request()
>>>>
    if (rq->tag == -1 && op_is_flush(rq->cmd_flags)) {
        blk_mq_sched_insert_flush(hctx, rq, can_block);
        return;
    }
>>>>
static void blk_mq_sched_insert_flush(struct blk_mq_hw_ctx *hctx,
                      struct request *rq, bool can_block)
{
    if (blk_mq_get_driver_tag(rq, &hctx, can_block)) {
        blk_insert_flush(rq);
        blk_mq_run_hw_queue(hctx, true);
    } else
        blk_mq_add_to_requeue_list(rq, false, true);
}
</pre>

And this will cause a issue. Look at the comment of reorder_tags_to_front()
<pre>
If we fail getting a driver tag because all the driver tags are already
assigned and on the dispatch list, BUT the first entry does not have a
tag, then we could deadlock. For that case, move entries with assigned
driver tags to the front, leaving the set of tagged requests in the
same order, and the untagged set in the same order.
</pre>

The patch changes the way to handle this case, just allocate driver tag upfront.
And let flush_rq get a driver tag just before .queue_rq() in
blk_mq_dispatch_rq_list(). This will not cause IO hang described above, because
the FLUSH requests just occupy sched tags. But the flush_rq still need to borrow
the sched tag to cheat the blk-mq.
<pre>
blk_kick_flush()
>>>>
    if (q->mq_ops) {
        struct blk_mq_hw_ctx *hctx;

        flush_rq->mq_ctx = first_rq->mq_ctx;

        if (!q->elevator) {
            fq->orig_rq = first_rq;
            flush_rq->tag = first_rq->tag;
            hctx = blk_mq_map_queue(q, first_rq->mq_ctx->cpu);
            blk_mq_tag_set_rq(hctx, first_rq->tag, flush_rq);
        } else {
            flush_rq->internal_tag = first_rq->internal_tag;
>>>>
</pre>
</font>
</p>


<h2><a name="Queue_state_flags">Queue state flags</a></h2>
<hr style="height:5px;border:none;border-top:3px solid black;" />
<p>
<font size="2">
Let's look at the similar 3 flags of request_queue.
<ul>
<li> QUEUE_FLAG_STOPPED<br/>
It is only used in block legacy, looks like the BLK_MQ_S_STOPPED.
The quiescing mechanism has big advantage on it, so BLK_MQ_S_STOPPED is rarely
used now.
<br/>
<li> <strong>QUEUE_FLAG_DYING</strong><br/>
First, let's look at blk_set_queue_dying()
<pre>
void blk_set_queue_dying(struct request_queue *q)
{
    spin_lock_irq(q->queue_lock);
    queue_flag_set(QUEUE_FLAG_DYING, q);
    spin_unlock_irq(q->queue_lock);

    /*
     * <font color="red">When queue DYING flag is set, we need to block new req
     * entering queue, </font>so we call blk_freeze_queue_start() to
     * prevent I/O from crossing blk_queue_enter().
     */
    blk_freeze_queue_start(q);  // just kill the percpu ref and will not wait.
    <font color="blue">
    /*
       blk_queue_enter()

       percpu_ref_tryget_live() will fail when the     percpu ref is killed (__PERCPU_REF_DEAD)
       >>>>
       ret = wait_event_interruptible(q->mq_freeze_wq,
                (atomic_read(&q->mq_freeze_depth) == 0 &&
                 (preempt || !blk_queue_preempt_only(q))) ||
                blk_queue_dying(q));
        if (blk_queue_dying(q))
            return -ENODEV;

     */
    </font>

    if (q->mq_ops)
        blk_mq_wake_waiters(q); //<font color="red">wake the ones waiting on driver tag</font>
    else {
        struct request_list *rl;                                                                                      
                                                             /
        spin_lock_irq(q->queue_lock);                       |  block legacy
        blk_queue_for_each_rl(rl, q) {                      |  
            if (rl->rq_pool) {                              |  get_request()    <------------------ +
                wake_up_all(&rl->wait[BLK_RW_SYNC]);        |      -> __get_request()               |
                wake_up_all(&rl->wait[BLK_RW_ASYNC]); ----> |             -> if blk_queue_dying()   |
            }                                               |               return                  |
        }                                                   |      -> if blk_queue_dying            |
        spin_unlock_irq(q->queue_lock);                     |            return                     | wakeup retry
    }                                                       |      -> waiting on rl->wait[is_sync]  |
                                                            |              |                        |
                                                            |              |                        |
                                                             \             +------------------------+


    /* Make blk_queue_enter() reexamine the DYING flag. */
    wake_up_all(&q->mq_freeze_wq);   //after this, no one could cross blk_queue_enter() in generic_make_request()
}
</pre>
QUEUE_FLAG_DYING mainly gates the blk_queue_enter() and generic_make_request(),
the difference from blk queue freezing is blk_queue_enter() <font color="red">will fail and return
-ENODEV</font> instead of block there. <br/>

There are some differences between block legacy and blk-mq. <br/>
<ul>
<li> there is not any QUEUE_FLAG_DYING checking in blk_mq_get_tag(), it means
that even if someone waiting tag is waked up by blk_set_queue_dying(), but it
will do anything for it.
<li> no checking in blk_execute_rq_nowait() as the block legacy and only a
comment as follow:
<pre>
     don't check dying flag for MQ because the request won't
     be reused after dying flag is set
</pre>
</ul>
These seems to mean that if someone crosses the blk_queue_enter() in blk_mq, it
will be able to submit request on a dying queue. Is it right or safe ?
Maybe yes, at least, in blk_cleanup_queue(), it will wait to drain the queue
first. <br/>

<br/>
<li> <strong>QUEUE_FLAG_QUIESCED</strong><br/>
Checked through blk_queue_quiesced() in the following paths.
<pre>
__blk_mq_run_hw_queue()
    -> blk_mq_sched_dispatch_requests() // under <font color="red"> rcu or src lock</font>
        -> if blk_queue_quiesced()
            return  // will not dequeue from io scheduler or ctx queue
blk_mq_try_issue_directly()
    -> __blk_mq_try_issue_directly() // under <font color="red"> rcu or src lock</font>
        -> if blk_queue_quiesced
            blk_mq_sched_insert_request() // to io scheduler or ctx queue
</pre>
When the queue is quiesced, the <font color="red">reqs will not enter into lldd but only stay in
blk-mq layer queues. In the other words, bios still could be submitted and will
not be issued.</font></br>
In blk_mq_quiesce_queue, synchronize_srcu/rcu ensure the QUEUE_FLAG_QUIESCED
will be visible when it returns.<br/>

</ul>
</font>
</p>


<h2><a name="WBT">WBT</a></h2>
<hr style="height:5px;border:none;border-top:3px solid black;" />
<p>
<font size="2">
WBT = Write Buff Throttle <br/>
<strong>Why we need wbt ?</strong><br/>
Let's quote some comment from the developer of this feature Jens.
<pre>
When we do background buffered writeback, it should have little impact
on foreground activity. That's the definition of background activity...
But for as long as I can remember, heavy buffered writers have not
behaved like that. For instance, if I do something like this:

$ dd if=/dev/zero of=foo bs=1M count=10k

on my laptop, and then try and start chrome, it basically won't start
before the buffered writeback is done. Or, for server oriented
workloads, where installation of a big RPM (or similar) adversely
impacts database reads or sync writes. When that happens, I get people
yelling at me.
</pre>
In conclusion, the <font color="red">foreground IOs</font> should be priorized
over the <font color="red">background</font> ones.
<br/>
<strong>Who will be throttled</strong><br/>
wbt_should_throttle() gives the answer.
<pre>
static inline bool wbt_should_throttle(struct rq_wb *rwb, struct bio *bio)
{
    const int op = bio_op(bio);

    /*
     * If not a WRITE, do nothing
     */
    if (op != REQ_OP_WRITE)
        return false;

    /*
     * Don't throttle WRITE_ODIRECT
     */
    if ((bio->bi_opf & (REQ_SYNC | REQ_IDLE)) == (REQ_SYNC | REQ_IDLE))
        return false;

    return true;
}
</pre>
The suspect is what's about the synchronous write ?<br/>
For example, the updating of the metadata of filesystem ?<br/>

<strong>How to implement it</strong><br/>
Let's first look at the hooks across the blk-mq layer. <br/>
<pre>
          blk_mq_make_request()
                wbt_wait()
                    if !may_queue()
                        sleep

                wbt_track()
                    save track info 
                    on rq->issue_stat

          blk_mq_start_request()                        wb_timer_fn()
                wbt_issue()                                 account the latency of sync IO
                    sync issue time                         and adjust the limits of different IO type

          blk_mq_free_request()/__blk_mq_end_request()
                wbt_done()
                    dec inflight
                    wake up

          __blk_mq_requeue_request()
                wbt_requeue()
                    clear sync issue time
</pre>
Yeah, it looks like the kyber IO scheduler.<br/>
But there is a big difference regarding to the action when limit is reached. <br>
<ul>
<li>For wbt, submitting path will sleep before blk_mq_get_request in blk_mq_make_request(), 
On the one hand, wbt can limit the usage of requests/tags, on the other hand the
submiting path cannot insert request any more.
<li>For kyber, the path who sleep to wait token is dispatching path, which is the
kworker context for write back. At the moment, the requests still could be
inserted, even merged.
</ul>
</font>
</p>


<h2><a name="DISK_and_BLKDEV">DISK and BLKDEV</a></h2>
<p>
<font size="2">
Except for <strong>QUEUE</strong> in block layer, there is another important
concept <strong>DISK</strong>. QUEUE is an equipment of DISK to process the IOs.
blk-mq or blk-legacy is regarding to QUEUE, but DISK is still the DISK. BLKDEV
is a concept of fs layer. "Everything is file" in linux, BLKDEV is the "file" of
DISK.<br/>


There is a special filesystem <font color="red">"bdev"</font> defined in
fs/block_dev.c. We could get the block_device from it through blkdev number <br/>

<pre>
struct block_device *bdget(dev_t dev)
{
    struct block_device *bdev;
    struct inode *inode;

    /* if not exist, a new one will be created. */
    inode = iget5_locked(blockdev_superblock, hash(dev),
            bdev_test, bdev_set, &dev);

    if (!inode)
        return NULL;

    bdev = &BDEV_I(inode)->bdev;

    if (inode->i_state & I_NEW) {
    ...
    }
    return bdev;
}
</pre>

When we access the block device directly, for example /dev/sda1, we will not pass 
througth bdev fs first. /dev/ is devtmpfs, not bdev fs. We could refer to init_special_inode
to know this. <br/>

Look at the diagram of BLKDEV, DISK and QUEUE.
<pre>
        sda1    sda2    sda3    sda4              <font color="red">BLKDEV</font>

        __disk_get_part(blkdev->bd_disk,        \
                blkdev->bd_partno)              |
                                                | 
       hd1      hd2     hd3     hd4              > <font color="red">DISK</font>
                                                |
       bio->bi_iter.bi_sector += hd->start_sect |
       bio->bi_partno = 0;                      /

           request_queue->make_request_fn      
           blk_queue_bio/blk_mq_make_request       <font color="red">QUEUE</font>
                     ...

blkdev - block_device
disk   - gendisk
hd     - hd_struct
</pre>

In a realy workload, the stream is as following:
<pre>
mount_bdev
  sget
    set_bdev_super       xxx_get_block
      set sb->s_bdev       map_bh
                             bh->bdev = sb->s_bdev
                             |
                             V
                         submit_bh_wbc
                           bio_set_dev(bio, bh->b_bdev)
                             bio->bi_disk = bdev->bd_disk 
                             bio->bi_partno = bdev->bd_partno
                             |
                             V
                         generic_make_request
                           generic_make_request_checks
                             blk_partition_remap
                               bio->bi_iter.bi_sector += hd->start_sect |
                                  bio->bi_partno = 0;
                           queue->make_request_fn

</pre>

</font>
</p>


<h2><a name="blk_integrity">blk_integrity</a></h2>
<p>
<font size="2">
<strong>What is blk_integrity for ?</strong><br/>
<pre>

       [ system memory ]
               |   
               | D  
               | M   path1
               | A
               V        sas/fc/iscsi
         [ HBA memory]- - - - - - - - ->[ storage volume ]
                             path2
</pre>
The data integrity on path2 could be ensured by the transport protocol, for example:
e.g
<ul>
<li> SCSI family protocols (SBC Data Integrity Field, SCC protection proposal)
<li> SATA/T13 (External Path Protection) t
<li> <font color="red">iscsi head/data digest ?</font>
</ul>
The path1 is protected by blk_integrity what we will talk next.<br/>
<br/>
<br/>
<strong>How is blk_integrity implemented ?</strong><br/>
Quote from Documentation/block/data-integrity.txt
<pre>
Because the format of the protection data is tied to the physical
disk, each block device has been extended with a block integrity
profile (struct blk_integrity).  This optional profile is registered
with the block layer using blk_integrity_register().

The profile contains callback functions for generating and verifying
the protection data, as well as getting and setting application tags.
The profile also contains a few constants to aid in completing,
merging and splitting the integrity metadata.
</pre>
Let's look at how does the scsi sd implement this.<br/>
<pre>
sd_probe_async
  -> sd_dif_config_host
--
    /* Enable DMA of protection information */
    if (scsi_host_get_guard(sdkp->device->host) & SHOST_DIX_GUARD_IP) {
        if (type == T10_PI_TYPE3_PROTECTION)
            bi.profile = &t10_pi_type3_ip;
        else
            bi.profile = &t10_pi_type1_ip;

        bi.flags |= BLK_INTEGRITY_IP_CHECKSUM;
    } else
        if (type == T10_PI_TYPE3_PROTECTION)
            bi.profile = &t10_pi_type3_crc;
        else
            bi.profile = &t10_pi_type1_crc;

    bi.tuple_size = sizeof(struct t10_pi_tuple);
    sd_printk(KERN_NOTICE, sdkp,
          "Enabling DIX %s protection\n", bi.profile->name);

    if (dif && type) {
        bi.flags |= BLK_INTEGRITY_DEVICE_CAPABLE;

        if (!sdkp->ATO)
            goto out;

        if (type == T10_PI_TYPE3_PROTECTION)
            bi.tag_size = sizeof(u16) + sizeof(u32);
        else
            bi.tag_size = sizeof(u16);

        sd_printk(KERN_NOTICE, sdkp, "DIF application tag size %u\n",
              bi.tag_size);
    }

out:
    blk_integrity_register(disk, &bi);
--
</pre>
<br/>
<br/>
The process of blk_integrity<br/>
<pre>
blk_mq_make_request
  -> bio_integrity_prep
    -> bio_integrity_add_page  //bio->bi_integrity
    -> bio_integrity_process(bio, &bio->bi_iter, bi->profile->generate_fn); //<font color="red">bio_data_dir(bio) == WRITE)</font>

bio_endio
  -> bio_integrity_endio
    -> __bio_integrity_endio
--
    if (bio_op(bio) == REQ_OP_READ && !bio->bi_status &&
        (bip->bip_flags & BIP_BLOCK_INTEGRITY) && bi->profile->verify_fn) {
        INIT_WORK(&bip->bip_work, bio_integrity_verify_fn);
        queue_work(kintegrityd_wq, &bip->bip_work);
        return false;
    }
--

static void bio_integrity_verify_fn(struct work_struct *work)
{
    struct bio_integrity_payload *bip =
        container_of(work, struct bio_integrity_payload, bip_work);
    struct bio *bio = bip->bip_bio;
    struct blk_integrity *bi = blk_get_integrity(bio->bi_disk);
    struct bvec_iter iter = bio->bi_iter;

    /*
     * At the moment verify is called bio's iterator was advanced
     * during split and completion, we need to rewind iterator to
     * it's original position.
     */
    if (bio_rewind_iter(bio, &iter, iter.bi_done)) {
        bio->bi_status = bio_integrity_process(bio, &iter,
                               bi->profile->verify_fn);
    } else {
        bio->bi_status = BLK_STS_IOERR;
    }

    bio_integrity_free(bio);
    bio_endio(bio);
}

</pre>
<br/>
<br/>
<strong>blk_integrity and fs</strong><br/>
After the request is issued to HBA, the data will be transported to HBA internal
buffer through DMA and then verify it based on protection meta data. During the
DMA transporting, the data in the sglist (<font color="red">page caches</font>) cannot be be
modified. This is guaranteed by fs.<br/>
<pre>
Steps of writing data to a file:
1. writing into the page cache
aops.write_begin
  -> lock page
  -> wait_for_stable_page
    -> if bdi_cap_stable_pages_required //<font color="red">BDI_CAP_STABLE_WRITES</font>
         wait_on_page_writeback
copy from user buffer to page cache
aops.write_end

2. writeback the pagecache to disk
lock page
set page writeback
submit_bio
unlock page

3. io completion
end bio
  -> end_page_writeback
    -> test_clear_page_writeback
    -> wake_up_page(page, PG_writeback)
</pre>
BDI_CAP_STABLE_WRITES is set in <strong>blk_integrity_register</strong>.

</font>
</p>


<h2><a name="blk-loop">blk loop</a></h2>
<p>
<font size="2">
What's blk-loop for ?
<pre>

    /dev/loopX     /home/ubuntu-16.04.4-desktop-amd64.iso
         |         ^         |              |
         v         |         v              v
    +-------------C-------------------+  +-------+
    |     vfs cache|                  |  |  DIO  |
    +-------------C-------------------+  +-------+
         |         |         |              |
         v         |         v              v
    +-------------C------------------------------+
    |  block layer |                             |
    +-------------C------------------------------+
         |         |         |
         v         |         v
        blk-loop driver    SCSI layer
</pre> 
The backend of a block device could be a HDD, SSD, or storage subsystem linked
by fc or iscsi, and also could be a local file.<br/>
<br/>
There is another concept: direct IO.
<pre>
The data from applications will go directly to block layer, bypassing the system
file cache.
</pre>

</font>
</p>

<h3><a name="How_to_create">How to create</a></h3>
<p>
<font size="2">
<strong>Step 1</strong>
<pre>
/dev/loop-control 
loop_ctl_fops
  -> loop_control_ioctl //LOOP_CTL_ADD
    -> loop_add
There are a lot of interesting things in loop_add, let's look at it.
static int loop_add(struct loop_device **l, int i)
{
    struct loop_device *lo;
    struct gendisk *disk;
    int err;

    err = -ENOMEM;
    lo = kzalloc(sizeof(*lo), GFP_KERNEL);
    if (!lo)
        goto out;

    lo->lo_state = Lo_unbound; //<font color="green">This means no file is bound on this device</font>

    /* allocate id, if @id >= 0, we're requesting that specific id */
    if (i >= 0) {
        err = idr_alloc(&loop_index_idr, lo, i, i + 1, GFP_KERNEL);
        if (err == -ENOSPC)
            err = -EEXIST;
    } else {
        err = idr_alloc(&loop_index_idr, lo, 0, 0, GFP_KERNEL);
    }
    if (err < 0)
        goto out_free_dev;
    i = err;

    err = -ENOMEM;
    lo->tag_set.ops = &loop_mq_ops;
    lo->tag_set.nr_hw_queues = 1;
    <font color="green">/*
    It should be an interesting theme to find out how many hw_queues to be
    required to get better performance.
    The real work is done in loop kthread, what .queue_rq does is just to insert
    a work or wakeup the kthread.
     */</font>
    lo->tag_set.queue_depth = 128;
    lo->tag_set.numa_node = NUMA_NO_NODE;
    lo->tag_set.cmd_size = sizeof(struct loop_cmd);
    lo->tag_set.flags = BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_SG_MERGE;
    lo->tag_set.driver_data = lo;

    err = blk_mq_alloc_tag_set(&lo->tag_set);
    if (err)
        goto out_free_idr;

    lo->lo_queue = blk_mq_init_queue(&lo->tag_set);
    if (IS_ERR_OR_NULL(lo->lo_queue)) {
        err = PTR_ERR(lo->lo_queue);
        goto out_cleanup_tags;
    }
    lo->lo_queue->queuedata = lo;

    blk_queue_max_hw_sectors(lo->lo_queue, BLK_DEF_MAX_SECTORS);

    <font color="green">/*
     * By default, we do buffer IO, so it doesn't make sense to enable
     * merge because the I/O submitted to backing file is handled page by
     * page. For directio mode, merge does help to dispatch bigger request
     * to underlayer disk. We will enable merge once directio is enabled.
     */</font>
    queue_flag_set_unlocked(QUEUE_FLAG_NOMERGES, lo->lo_queue);

    err = -ENOMEM;
    disk = lo->lo_disk = alloc_disk(1 << part_shift);
    ...
    disk->fops        = &lo_fops; //<font color="red">this the fops for /dev/loopX</font>
    disk->private_data    = lo;
    disk->queue        = lo->lo_queue;
    sprintf(disk->disk_name, "loop%d", i);
    add_disk(disk);
    *l = lo;
    return lo->lo_number;
    ...
}

</pre>

<strong>Step 2</strong>

<pre>
/dev/loopX
lo_fops
  -> lo_ioctl //LOOP_SET_FD
    -> loop_set_fd
static int loop_set_fd(struct loop_device *lo, fmode_t mode,
               struct block_device *bdev, unsigned int arg)
{
    ...
    file = fget(arg);
    if (!file)
        goto out;
    ...
    mapping = file->f_mapping;
    inode = mapping->host;
    <font color="green">//regular file or block file</font>
    if (!S_ISREG(inode->i_mode) && !S_ISBLK(inode->i_mode))
        goto out_putf;

    if (!(file->f_mode & FMODE_WRITE) || !(mode & FMODE_WRITE) ||
        !file->f_op->write_iter)
        lo_flags |= LO_FLAGS_READ_ONLY;

    error = -EFBIG;
    size = get_loop_size(lo, file);
    if ((loff_t)(sector_t)size != size)
        goto out_putf;
    error = loop_prepare_queue(lo);
    <font color="blue">
            kthread_init_worker(&lo->worker);
            lo->worker_task = kthread_run(loop_kthread_worker_fn,
                    &lo->worker, "loop%d", lo->lo_number);
            if (IS_ERR(lo->worker_task))
            return -ENOMEM;
            set_user_nice(lo->worker_task, MIN_NICE);
    </font>

    set_device_ro(bdev, (lo_flags & LO_FLAGS_READ_ONLY) != 0);

    lo->use_dio = false;
    lo->lo_device = bdev;
    lo->lo_flags = lo_flags;
    lo->lo_backing_file = file;
    lo->transfer = NULL;
    lo->ioctl = NULL;
    lo->lo_sizelimit = 0;
    lo->old_gfp_mask = mapping_gfp_mask(mapping);
    mapping_set_gfp_mask(mapping, lo->old_gfp_mask & ~(__GFP_IO|__GFP_FS));

    if (!(lo_flags & LO_FLAGS_READ_ONLY) && file->f_op->fsync)
        blk_queue_write_cache(lo->lo_queue, true, false);

    loop_update_dio(lo);
    set_capacity(lo->lo_disk, size);
    bd_set_size(bdev, size << 9);
    loop_sysfs_init(lo);
    /* let user-space know about the new size */
    kobject_uevent(&disk_to_dev(bdev->bd_disk)->kobj, KOBJ_CHANGE);

    set_blocksize(bdev, S_ISBLK(inode->i_mode) ?
              block_size(inode->i_bdev) : PAGE_SIZE);

    lo->lo_state = Lo_bound;
    ...
}
</pre>

</font>
</p>



<h3><a name="Kthread_or_workqueue">Kthread or Workqueue ?</a></h3>
<p>
<font size="2">
When request enters into .queue_rq, how to handle it next ? <br/>
It need to be handled in another context, because we have owned a deep stack
from vfs_read/write to driver .queue_rq. This context  could be kworker or
standalone kthread. But which one shoud we use ? <br/>
commit e03a3d7 ( block: loop: use kthread_work ) change the block loop from work
to kthread context. Let's look at what block loop does before and after this patch.
<pre>
Work based.

           Concurrently                   Sequentially                         
    Read   Read   Read   Read      Write<->Write<->Write<->Write
    +---+  +---+  +---+  +---+     +---+
    | W |  | W |  | W |  | W |     | W |
    +---+  +---+  +---+  +---+     +---+
      |      |      |      |         |
   + -v- - - v - - -v- - - v - - - - v - - +
   |          Unbound worker pool          |
   + - - - - - - - - - - - - - - - - - - - +

+---+
| W |  work instance
+---+

</pre>
For the read, block loop issues them concurrently as far as possible.
This is due to read operastions often need to wait for the page caches to be
filled, it is usually a sychronous one. Issuing Read concurrently is good for
random read, but it is not so efficient for sequential read which often could
hit the page cache.<br/>
For the write, block loop issue them sequentially, because writes usually
reaches on page cache, it is usually fast enough.
<br/>
<pre>
             Write<->Write<->Read<->Read<->Write ....
          +- - - - -+
          | kthread |
          +- - - - -+
</pre>
When DIO/AIO is introduced, the read/write on backing file is not blocking
operations.
</font>
</p>


<h3><a name="DIO_AIO_on_backing_file">DIO & AIO on backing file</a></h3>
<p>
<font size="2">
In linux, read operastions are almost synchronous except for the required data
has been already in the page cache, otherwise, it has to wait for the page cache
to be filled by the block device through block layer and blk driver. Even if we
have readahead mechanism, but the page cache cannot be often hit with random
read.<br/>
Consequently, the loop driver execute context (kworker or standalone kthread)
has to wait and this will delay the other requests which may has associated page
cache already. <br/>
On ther other hand, there are two layer page cache would be involved, one for
file over loop device, one for the backing file. This is unnecessary and wastes
memory.<br/>
<br/>
Leiming introduced backing file DIO and AIO supporting in block loop.
<pre>
commit bc07c10a3603a5ab3ef01ba42b3d41f9ac63d1b6
Author: Ming Lei <ming.lei@canonical.com>
Date:   Mon Aug 17 10:31:51 2015 +0800

    block: loop: support DIO & AIO
    
    There are at least 3 advantages to use direct I/O and AIO on
    read/write loop's backing file:
    
    1) double cache can be avoided, then memory usage gets
    decreased a lot
    
    2) not like user space direct I/O, there isn't cost of
    pinning pages
    
    3) avoid context switch for obtaining good throughput
    - in buffered file read, random I/O top throughput is often obtained
    only if they are submitted concurrently from lots of tasks; but for
    sequential I/O, most of times they can be hit from page cache, so
    concurrent submissions often introduce unnecessary context switch
    and can't improve throughput much. There was such discussion[1]
    to use non-blocking I/O to improve the problem for application.
    - with direct I/O and AIO, concurrent submissions can be
    avoided and random read throughput can't be affected meantime
    
    xfstests(-g auto, ext4) is basically passed when running with
    direct I/O(aio), one exception is generic/232, but it failed in
    loop buffered I/O(4.2-rc6-next-20150814) too.
    
    Follows the fio test result for performance purpose:
        4 jobs fio test inside ext4 file system over loop block
    
    1) How to run
        - KVM: 4 VCPUs, 2G RAM
        - linux kernel: 4.2-rc6-next-20150814(base) with the patchset
        - the loop block is over one image on SSD.
        - linux psync, 4 jobs, size 1500M, ext4 over loop block
        - test result: IOPS from fio output
    
    2) Throughput(IOPS) becomes a bit better with direct I/O(aio)
            -------------------------------------------------------------
            test cases          |randread   |read   |randwrite  |write  |
            -------------------------------------------------------------
            base                |8015       |113811 |67442      |106978
            -------------------------------------------------------------
            base+loop aio       |8136       |125040 |67811      |111376
            -------------------------------------------------------------
    
    - somehow, it should be caused by more page cache avaiable for
    application or one extra page copy is avoided in case of direct I/O
    
    3) context switch
            - context switch decreased by ~50% with loop direct I/O(aio)
        compared with loop buffered I/O(4.2-rc6-next-20150814)
    
    4) memory usage from /proc/meminfo
            -------------------------------------------------------------
                                       | Buffers       | Cached
            -------------------------------------------------------------
            base                       | > 760MB       | ~950MB
            -------------------------------------------------------------
            base+loop direct I/O(aio)  | < 5MB         | ~1.6GB
            -------------------------------------------------------------
    
    - so there are much more page caches available for application with
    direct I/O
    
    [1] https://lwn.net/Articles/612483/
    
    Signed-off-by: Ming Lei <ming.lei@canonical.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>
</pre>
After that, we get following diagram.
<pre>
    /dev/loopX            > /home/ubuntu-16.04.4-desktop-amd64.iso
         |               /         |
         v              /          v
    +-------------+    /       +-------+
    | vfs cache|  |   /        |  DIO  |
    +-------------+  /         +-------+
         |          /              |
         v         /               v
    +-------------C-----------------------------+
    | block layer  |                            |
    +-------------C-----------------------------+
         |         |               |
         v         |               v
        blk-loop driver        SCSI layer
</pre>

</font>
</p>




<h3><a name="blk-stats">blk-stats</a></h3>
<p>
<font size="2">
Before look into the implementations of blk-stat in kernel, let's first look at
how to utilize the information provided by blk-stats, iostat.<br/>
<pre>
#iostat -c -d -x /dev/sda2 2 100
Linux 4.16.0-rc3+ (will-ThinkPad-L470)     03/20/2018     _x86_64_    (4 CPU)

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
          12.61    0.03    2.23    0.82    0.00   84.31

Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda2              0.20     5.86    2.46    4.04    23.54    56.83    24.72     0.14   20.56    6.17   29.31   5.67   3.69


rrqm/s      The number of read requests merged per second queued to the device.
wrqm/s      The number of write requests merged per second queued to the device.
r/s         The number of read requests issued to the device per second.
w/s         The number of write requests issued to the device per second.
avgrq-sz    The average size (in sectors) of the requests issued to the device.
avgqu-sz    The average queue length of the requests issued to the device.
await       The average time (milliseconds) for I/O requests issued to the device to be served.
            This includes the time spent by the requests in queue and the time spent servicing them.
r_await     The average time (in milliseconds) for read requests issued to the device to be served.
            This includes the time spent by the requests in queue and the time spent servicing them.
w_await     The average time (in milliseconds) for write requests issued to the device to be served.
            This includes the time spent by the requests in queue and the time spent servicing them.
svctm       The average service time (in milliseconds) for I/O requests issued to the device.
            Warning! Do not trust this field; it will be removed in a future version of sysstat.
%util       Percentage of CPU time during which I/O requests were issued to the device (bandwidth utilization for the device).
            Device saturation occurs when this values is close to 100%.
</pre>
How to calculate them ?


Based on <a href="#write_ext_stat">write_ext_stat</a>
<pre>
ioj, ioi     two samples, j = i + 1
itv          interval of two samples

rrqm/s    (ioj->rd_merges, ioi->rd_merges)/itv   
wrqm/s    (ioj->wr_merges, ioi->wr_merges)/itv   
r/s       (ioj->rd_ios, ioi->rd_ios)/itv
w/s       (ioj->wr_ios, ioi->wr_ios)/itv
avgrq-sz  ((ioj->rd_sect - ioi->rd_sect) + (ioj->wr_sect - ioi->wr_sect))/
          (ioj->nr_ios - ioi->nr_ios)
avgqu-sz  (ioj->rq_ticks, ioi->rq_ticks)/itv
await     ((ioj->rd_ticks - ioi->rd_ticks) + (ioj->wr_ticks + ioj->wr_ticks))/
          (ioj->nr_ios - ioi->nr_ios)
         
r_await    similar with await
w_await    similar with await
         
%util      (ioj->tot_ticks - ioi->tot_ticks)/itv
</pre>
We could refer <a href="#read_diskstats_stat">read_diskstats_stat</a> to know where does these 
data come from.<br/>
<br/>
Next, let's find out how to generate this statistics data in kernel.<br/>
Based on <a href="#diskstats_show">diskstats_show</a>
There are following members in hd_struct.dkstats ( a percpu variable)
<ul>
<li> ios       r/w
<pre>
__blk_mq_end_request
  -> blk_account_io_done
    -> part_stat_inc(cpu, part, ios[rw]);
</pre>
<li> merges    r/w
<pre>
bio_attempt_back/font/disacard_merge
  -> blk_account_io_start // new_io == false
    -> part_stat_inc(cpu, part, merges[rw]);
</pre>
<li> sectors   r/w
<pre>
blk_mq_end_request
  -> blk_update_request
    -> blk_account_io_completion
      ->  part_stat_add(cpu, part, sectors[rw], bytes >> 9);
</pre>
<li> ticks     r/w
<pre>
__blk_mq_end_request
  -> blk_account_io_done
    -> part_stat_add(cpu, part, ticks[rw], <font color="red">(jiffies - req->start_time)</font>);
rq->start_time is set in blk_mq_rq_ctx_init and will inherit the smaller start_time of the merged rqs
<font color="red">
What if the duration here is smaller than 1 jiffies ?
This could be possible on a machine that has a high-speed storage device and low HZ
</font>
</pre>
<li> io_ticks
<li> time_in_queue
<pre>
blk_account_io_start/merge/done diskstats_show/part_stat_show
  -> part_round_stats
    -> part_in_flight // f299b7c (blk-mq: provide internal in-flight variant)
      -> blk_mq_in_flight
    -> part_round_stats_single

static void part_round_stats_single(struct request_queue *q, int cpu,
                    struct hd_struct *part, unsigned long now,
                    unsigned int inflight)
{
    if (inflight) {
        __part_stat_add(cpu, part, time_in_queue,
                inflight * (now - part->stamp));
        __part_stat_add(cpu, part, io_ticks, (now - part->stamp));
    }
    part->stamp = now;
}
io_ticks here means the time when there is in-flight IO in request queue.
</pre>
</ul>



<strong>Reference</strong><br/>
<a name="read_diskstats_stat">read_diskstats_stat</a>
<pre>
void read_diskstats_stat(int curr)
{
    ...
    if ((fp = fopen(DISKSTATS, "r")) == NULL) <font color="green">//  proc/diskstats</font>
        return;

    while (fgets(line, 256, fp) != NULL) {

        <font color="green">/* major minor name rio rmerge rsect ruse wio wmerge wsect wuse running use aveq */</font>
        i = sscanf(line, "%u %u %s %lu %lu %lu %lu %lu %lu %lu %u %u %u %u",
               &major, &minor, dev_name,
               &rd_ios, &rd_merges_or_rd_sec, &rd_sec_or_wr_ios, &rd_ticks_or_wr_sec,
               &wr_ios, &wr_merges, &wr_sec, &wr_ticks, &ios_pgr, &tot_ticks, &rq_ticks);

        if (i == 14) {
            /* Device or partition */
            if (!dlist_idx && !DISPLAY_PARTITIONS(flags) &&
                !is_device(dev_name, ACCEPT_VIRTUAL_DEVICES))
                continue;
            sdev.rd_ios     = rd_ios;
            sdev.rd_merges  = rd_merges_or_rd_sec;
            sdev.rd_sectors = rd_sec_or_wr_ios;
            sdev.rd_ticks   = (unsigned int) rd_ticks_or_wr_sec;
            sdev.wr_ios     = wr_ios;
            sdev.wr_merges  = wr_merges;
            sdev.wr_sectors = wr_sec;
            sdev.wr_ticks   = wr_ticks;
            sdev.ios_pgr    = ios_pgr;
            sdev.tot_ticks  = tot_ticks;
            sdev.rq_ticks   = rq_ticks;
        }
        ...
        save_stats(dev_name, curr, &sdev, iodev_nr, st_hdr_iodev);
    }
    ...
}
</pre>

<a name="diskstats_show">diskstats_show</a>
<pre>
static int diskstats_show(struct seq_file *seqf, void *v)
{
    struct gendisk *gp = v;
    struct disk_part_iter piter;
    struct hd_struct *hd;
    char buf[BDEVNAME_SIZE];
    unsigned int inflight[2];
    int cpu;

    /*
    if (&disk_to_dev(gp)->kobj.entry == block_class.devices.next)
        seq_puts(seqf,    "major minor name"
                "     rio rmerge rsect ruse wio wmerge "
                "wsect wuse running use aveq"
                "\n\n");
    */

    disk_part_iter_init(&piter, gp, DISK_PITER_INCL_EMPTY_PART0);
    while ((hd = disk_part_iter_next(&piter))) {
        cpu = part_stat_lock();
        part_round_stats(gp->queue, cpu, hd);
        part_stat_unlock();
        part_in_flight(gp->queue, hd, inflight);
        seq_printf(seqf, "%4d %7d %s %lu %lu %lu "
               "%u %lu %lu %lu %u %u %u %u\n",
               MAJOR(part_devt(hd)), MINOR(part_devt(hd)),
               disk_name(gp, hd->partno, buf),
               part_stat_read(hd, ios[READ]),
               part_stat_read(hd, merges[READ]),
               part_stat_read(hd, sectors[READ]),
               jiffies_to_msecs(part_stat_read(hd, ticks[READ])),
               part_stat_read(hd, ios[WRITE]),
               part_stat_read(hd, merges[WRITE]),
               part_stat_read(hd, sectors[WRITE]),
               jiffies_to_msecs(part_stat_read(hd, ticks[WRITE])),
               inflight[0],
               jiffies_to_msecs(part_stat_read(hd, io_ticks)),
               jiffies_to_msecs(part_stat_read(hd, time_in_queue))
            );
    }
    disk_part_iter_exit(&piter);

    return 0;
}
</pre>

<a name="write_ext_stat">write_ext_stat</a>
<pre>
void write_ext_stat(int curr, unsigned long long itv, int fctr,
            struct io_hdr_stats *shi, struct io_stats *ioi,
            struct io_stats *ioj)
{
    char *devname = NULL;
    struct stats_disk sdc, sdp;
    struct ext_disk_stats xds;
    double r_await, w_await;
    
    /*
     * Counters overflows are possible, but don't need to be handled in
     * a special way: The difference is still properly calculated if the
     * result is of the same type as the two values.
     * Exception is field rq_ticks which is incremented by the number of
     * I/O in progress times the number of milliseconds spent doing I/O.
     * But the number of I/O in progress (field ios_pgr) happens to be
     * sometimes negative...
     */
    sdc.nr_ios    = ioi->rd_ios + ioi->wr_ios;
    sdp.nr_ios    = ioj->rd_ios + ioj->wr_ios;

    sdc.tot_ticks = ioi->tot_ticks;
    sdp.tot_ticks = ioj->tot_ticks;

    sdc.rd_ticks  = ioi->rd_ticks;
    sdp.rd_ticks  = ioj->rd_ticks;
    sdc.wr_ticks  = ioi->wr_ticks;
    sdp.wr_ticks  = ioj->wr_ticks;

    sdc.rd_sect   = ioi->rd_sectors;
    sdp.rd_sect   = ioj->rd_sectors;
    sdc.wr_sect   = ioi->wr_sectors;
    sdp.wr_sect   = ioj->wr_sectors;
    
    compute_ext_disk_stats(&sdc, &sdp, itv, &xds);
    
    r_await = (ioi->rd_ios - ioj->rd_ios) ?
          (ioi->rd_ticks - ioj->rd_ticks) /
          ((double) (ioi->rd_ios - ioj->rd_ios)) : 0.0;
    w_await = (ioi->wr_ios - ioj->wr_ios) ?
          (ioi->wr_ticks - ioj->wr_ticks) /
          ((double) (ioi->wr_ios - ioj->wr_ios)) : 0.0;

    /* Print device name */
    if (DISPLAY_PERSIST_NAME_I(flags)) {
        devname = get_persistent_name_from_pretty(shi->name);
    }
    if (!devname) {
        devname = shi->name;
    }
    if (DISPLAY_HUMAN_READ(flags)) {
        printf("%s\n%13s", devname, "");
    }
    else {
        printf("%-13s", devname);
    }

    /*       rrq/s wrq/s   r/s   w/s  rsec  wsec  rqsz  qusz await r_await w_await svctm %util */
    printf(" %8.2f %8.2f %7.2f %7.2f %8.2f %8.2f %8.2f %8.2f %7.2f %7.2f %7.2f %6.2f %6.2f\n",
           S_VALUE(ioj->rd_merges, ioi->rd_merges, itv),
           S_VALUE(ioj->wr_merges, ioi->wr_merges, itv),
           S_VALUE(ioj->rd_ios, ioi->rd_ios, itv),
           S_VALUE(ioj->wr_ios, ioi->wr_ios, itv),
           ll_s_value(ioj->rd_sectors, ioi->rd_sectors, itv) / fctr,
           ll_s_value(ioj->wr_sectors, ioi->wr_sectors, itv) / fctr,
           xds.arqsz,
           S_VALUE(ioj->rq_ticks, ioi->rq_ticks, itv) / 1000.0,
           xds.await,
           r_await,
           w_await,
           /* The ticks output is biased to output 1000 ticks per second */
           xds.svctm,
           /*
            * Again: Ticks in milliseconds.
        * In the case of a device group (option -g), shi->used is the number of
        * devices in the group. Else shi->used equals 1.
        */
           shi->used ? xds.util / 10.0 / (double) shi->used
                     : xds.util / 10.0);    /* shi->used should never be null here */
}

</pre>
</font>
</p>


</body>
</html>
