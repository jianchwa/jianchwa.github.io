<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>XFS_V2</title>
</head>
<body>
<div>
    <h1>XFS_V2</h1> 
</div>
<p>
<font size="2">
<a href="#concepts">Concepts</a>
<ul>
<li> <a href="#link_count">link count</a>
<li> <a href="#posix_unlink">posix unlink</a>
<li> <a href="#asynchronous_journal">asynchronous journal</a>
</ul>

<a href="#infrastructure">infrastructure</a>
<ul>
<li> <a href="#iov_iter">iov_iter</a>
<li> <a href="#inode_lock_and_fault">inode lock and page fault</a>
<li> <a href="#sendfile">sendfile</a>
<li> <a href="#open_and_close">open and close</a>
<li> <a href="#mount">mount</a>
<li> <a href="#writenotify">writenotify</a>
</ul>

<a href="#inode">Inode</a>
<ul>
<li> <a href="#reference_of_inode">reference of inode</a>
<li> <a href="#lazytime_mode">lazytime mode</a>
<li> <a href="#inode_cache">inode cache</a>
</ul>

<a href="#dcache">Dcache</a>
<ul>
<li> <a href="#shrink_of_dcache">shrink of dcache</a>
<li> <a href="#lookup">lookup</a>
<li> <a href="#dcache_and_metadata">dcache and metadata</a>
<li> <a href="#add_to_dcache_hash">add to dcache hash</a>
</ul>

<a href="#tmpfs">Tmpfs</a><br/>
<a href="#xfs_log">Xfs Log</a>
<ul>
<li> <a href="#xlog_framework">xlog framework</a>
<li> <a href="#relog">Relog</a>
<li> <a href="#deferred_operations">Deferred operations</a>
<li> <a href="#log_space">log space</a>
</ul>
<a href="#xfs_inode">Xfs Inode</a>
<ul>
<li> <a href="#xfs_inode_mgnt">inode management</a>
</ul>
<a href="#xfs_buf">xfs buf</a>
</font>
</p>



<h2><a name="concepts">Concepts</a></h2>
<hr style="height:3px;border:none;border-top:3px solid black;" />

<h3><a name="link_count">link count</a></h3>
<p>
<font size="2">
<pre>
The link count of a file tells the total number of links a file has
which is nothing but the number of hard-links a file has. This count,
however, does not include the soft-link count.

Note: The soft-link is not part of the link count since the soft-link's
inode number is different from the original file. 

When a directory is created, except for the link count for the dentry in
parent directory, two extra link count is increased,
(1) one on the parent directory for ".." dentry in child directory
(2) one on the child directory for "." dentry ion child directory
</pre>
</font>
</p>

<h3><a name="posix_unlink">unlink</a></h3>
<p>
<font size="2">
Quote from https://pubs.opengroup.org/onlinepubs/9699919799/functions/unlink.html#tag_16_635
<pre>
When the file's link count becomes 0 and no process has the file open, the space occupied by
the file shall be freed and the file shall no longer be accessible. If one or more processes
have the file open when the last link is removed, the link shall be removed before unlink()
                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
returns, but the removal of the file contents shall be postponed until all references to the
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
file are closed.
^^^^^^^^^^^^^^
</pre>
How does xfs implement this ?

<ul>
<li> When nlink of inode reaches zero
<pre>
xfs_droplink()
---
    xfs_trans_ichgtime(tp, ip, XFS_ICHGTIME_CHG);
<font color="blue">
    // drop the link in the indoe on disk
</font>
    drop_nlink(VFS_I(ip));
    xfs_trans_log_inode(tp, ip, XFS_ILOG_CORE);

    if (VFS_I(ip)->i_nlink)
        return 0;

    return xfs_iunlink(tp, ip);
---
<font color="blue">
// link this inode on the list on agi->agi_unlinked[]
</font>
xfs_iunlink()
  -> xfs_iunlink_update_bucket()
  ---
    agi->agi_unlinked[bucket_index] = cpu_to_be32(new_agino);
    offset = offsetof(struct xfs_agi, agi_unlinked) +
            (sizeof(xfs_agino_t) * bucket_index);
<font color="red">    xfs_trans_log_buf(tp, agibp, offset, offset + sizeof(xfs_agino_t) - 1);</font>
---
</pre>
<li> when reference of inode reaches zero
<pre>
xfs_fs_destroy_inode()
  -> xfs_inactive()
  ---
    if (VFS_I(ip)->i_nlink != 0) {
        ...
        return;
    }

    if (S_ISREG(VFS_I(ip)->i_mode) &&
        (ip->i_d.di_size != 0 || XFS_ISIZE(ip) != 0 ||
         ip->i_df.if_nextents > 0 || ip->i_delayed_blks > 0))
        truncate = 1;
    ...
    if (S_ISLNK(VFS_I(ip)->i_mode))
        error = xfs_inactive_symlink(ip);
    else if (truncate)
<font color="red">        error = xfs_inactive_truncate(ip);</font>
    if (error)
        return;
    ...
    /*
     * Free the inode.
     */
    error = xfs_inactive_ifree(ip);
      -> xfs_ifree()
<font color="red">        -> xfs_iunlink_remove()</font>
  ---
</pre>
<li> Recover AGI unlinked lists
<pre>
This is called during recovery to <U>process any inodes which we unlinked but
not freed when the system crashed.</U>  These inodes will be on the lists in the
AGI blocks. What we do here is scan all the AGIs and fully truncate and free
any inodes found on the lists. Each inode is removed from the lists when it
has been fully truncated and is freed. The freeing of the inode and its
removal from the list must be atomic.

xlog_recover_process_iunlinks()
  -> xlog_recover_process_one_iunlink()
  ---
    ino = XFS_AGINO_TO_INO(mp, agno, agino);
    error = xfs_iget(mp, NULL, ino, 0, 0, &ip);
    ...
    error = xfs_imap_to_bp(mp, NULL, &ip->i_imap, &dip, &ibp, 0);
    ...
    <font color="red">agino = be32_to_cpu(dip->di_next_unlinked);</font>
    xfs_buf_relse(ibp);

    xfs_irele(ip);
      -> iput(VFS_I(ip));
<font color="blue">
      //The reference would be zero and triger xfs_fs_destroy_inode()
</font>

  ---
</pre>
</ul>
</font>
</p>


<h3><a name="asynchronous_journal">asynchronous journal</h3>
<p>
<font size="2">
Actually, both xfs and ext4-jbd2 employ asynchronous journal which<br/>
could batch IOs to journal to promote performance. Asynchronous journal<br/>
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
may lead to lose data (inode could also be lost) but still keep metadata consistent.<br/>
Let's look at how does xfs and ext4-jbd2 implement the asynchronous journal
<ul>
<li> XFS
<pre>
__xfs_trans_commit()
  -> xfs_log_commit_cil()
    -> xlog_cil_push_background()
---
<font color="blue">
    // min_t(int, (log)->l_logsize >> 3, BBTOB(XLOG_TOTAL_REC_SHIFT(log)) << 4)
    //                                         ^^^^^^^^^^^^^^^^^^^^^^^^
    //                                         Max of the xlog buffer
</font>
    if (cil->xc_ctx->space_used < XLOG_CIL_SPACE_LIMIT(log)) {
        up_read(&cil->xc_ctx_lock);
        return;
    }

    spin_lock(&cil->xc_push_lock);
    if (cil->xc_push_seq < cil->xc_current_sequence) {
        cil->xc_push_seq = cil->xc_current_sequence;
        queue_work(log->l_mp->m_cil_workqueue, &cil->xc_push_work);
    }

    up_read(&cil->xc_ctx_lock);

<font color="blue">
    /*
     * If we are well over the space limit, throttle the work that is being
     * done until the push work on this context has begun.
     */
</font>
    if (cil->xc_ctx->space_used >= XLOG_CIL_BLOCKING_SPACE_LIMIT(log)) {
        xlog_wait(&cil->xc_push_wait, &cil->xc_push_lock);
        return;
    }
    spin_unlock(&cil->xc_push_lock);
---
</pre>
<li> EXT4-JBD2
<pre>
When to start commit jdb2?
(1) log space is not enough
start_this_handle()
  -> add_transaction_credits()
  ---
    needed = atomic_add_return(total, &t->t_outstanding_credits);
    if (needed > journal->j_max_transaction_buffers) {
<font color="blue">
        /*
         * If the current transaction is already too large,
         * then start to commit it: we can then go back and
         * attach this handle to a new transaction.
         */
</font>
        atomic_sub(total, &t->t_outstanding_credits);
        ...
        wait_transaction_locked(journal);
        return 1;
    }
  ---
(2) transaction is too old
jbd2_journal_stop()
---
<font color="blue">
    /*
     * If the handle is marked SYNC, we need to set another commit
     * going!  We also want to force a commit if the transaction is too
     * old now.
     */
</font>
    if (handle->h_sync ||
        time_after_eq(jiffies, transaction->t_expires)) {
        /* This is non-blocking */
        jbd2_log_start_commit(journal, tid);
    }
---
(3) commit timeouts
jbd2_get_transaction()
---
    /* Set up the commit timer for the new transaction. */
    journal->j_commit_timer.expires = round_jiffies_up(transaction->t_expires);
    add_timer(&journal->j_commit_timer);
---

commit_timeout()
---
    journal_t *journal = from_timer(journal, t, j_commit_timer);

    wake_up_process(journal->j_task);
---
</pre>
</ul>
In constrast with asynchronous journal, synchronous journal could guarantee<br/>
the metadata has been on disk after the syscall returns, but this could hurts<br/>
the performance. Batch synchronous journal IO is to promote this which has been<br/>
supported by jbd2.
<pre>
jbd2_journal_stop()
---
<font color="blue">
    /*
     * Implement synchronous transaction batching.<font color="red">If the handle
     * was synchronous, don't force a commit immediately.  Let's
     * yield and let another thread piggyback onto this
     * transaction.</font>  Keep doing that while new threads continue to
     * arrive.  It doesn't cost much - we're about to run a committed
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
     * and sleep on IO anyway.  Speeds up many-threaded, many-dir
       ^^^^^^^^^^^^^^^^^^^^^^
                The io here means the journal IO if we commit immediately.
     * operations by 30x or more...
     *
     * We try and optimize the sleep time against what the
     * underlying disk can do, instead of having a static sleep
     * time.  This is useful for the case where our storage is so
     * fast that it is more optimal to go ahead and force a flush
     * and wait for the transaction to be committed than it is to
     * wait for an arbitrary amount of time for new writers to
     * join the transaction.  We achieve this by measuring how
     * long it takes to commit a transaction, and compare it with
     * how long this transaction has been running, and if run time
     * < commit time then we sleep for the delta and commit.  This
     * greatly helps super fast disks that would see slowdowns as
     * more threads started doing fsyncs.
     *
     * But don't do this if this process was the most recent one '
     * to perform a synchronous write.  We do this to detect the
     * case where a single process is doing a stream of sync
     * writes.  No point in waiting for joiners in that case.
     *
     * Setting max_batch_time to 0 disables this completely.
     */
</font>
    pid = current->pid;
    if (handle->h_sync && journal->j_last_sync_writer != pid &&
        journal->j_max_batch_time) {
        u64 commit_time, trans_time;

        journal->j_last_sync_writer = pid;

        read_lock(&journal->j_state_lock);
        commit_time = journal->j_average_commit_time;
        read_unlock(&journal->j_state_lock);

        trans_time = ktime_to_ns(ktime_sub(ktime_get(),
                           transaction->t_start_time));

        commit_time = max_t(u64, commit_time,
                    1000*journal->j_min_batch_time);
        commit_time = min_t(u64, commit_time,
                    1000*journal->j_max_batch_time);

        if (trans_time < commit_time) {
            ktime_t expires = ktime_add_ns(ktime_get(),
                               commit_time);
            set_current_state(TASK_UNINTERRUPTIBLE);
            schedule_hrtimeout(&expires, HRTIMER_MODE_ABS);
        }
    }

    if (handle->h_sync)
        transaction->t_synchronous_commit = 1;

    if (handle->h_sync ||
        time_after_eq(jiffies, transaction->t_expires)) {
        jbd2_log_start_commit(journal, tid);
        /*
         * Special case: JBD2_SYNC synchronous updates require us
         * to wait for the commit to complete.
         */
        if (handle->h_sync && !(current->flags & PF_MEMALLOC))
            wait_for_commit = 1;
    }

    stop_this_handle(handle);

    if (wait_for_commit)
        err = jbd2_log_wait_commit(journal, tid);
---
</pre>
</font>
</p>


<h2><a name="infrastructure">infrastructure</a></h2>
<hr style="height:3px;border:none;border-top:3px solid black;" />

<h3><a name="iov_iter">iov_iter</a></h3>
<p>
<font size="2">
What's iov_iter ?<br/>
<pre>
struct iov_iter {
    unsigned int type;   <font color="blue">Type, IOVEC/KVEC/BVEC</font>
    size_t iov_offset;   <font color="blue">current offset, like bio.bi_iter.bi_sector</font>
    size_t count;        <font color="blue">residual count like bio.bi_iter.bi_size</font>
    union {
        const struct iovec *iov;   
        const struct kvec *kvec;
        const struct bio_vec *bvec;
        ...
    };
<font color="blue">
    See the bio_vec, basically, iov and kvec have similar functions.
    The bio.bi_io_vec always points the head of the bvec array.
    iov/kvec/bvec always points the current vector, like bio.bi_io_vec[bio->bi_iter.bi_idx]
</font>
    ...
};

Just refer to iterate_and_advance() to know how does the iov_iter work

Even though we try to compare the bio with iov_iter, but they are not different.
bio has a map between the block device and buffer in memory,
however, iov_iter only describes the buffer, which is more similar with sglist.
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
</pre>
Next, we will figure out what does the IOVEC/KVEC/BVEC mean and who dose use them.
<ul>
<li> IOVEC, address from userland
<pre>

struct iovec
{
    void __user *iov_base;
    __kernel_size_t iov_len;
};

There are two cases here,
(1) construct iov_iter in kernel, and address from userland
vfs_read()
  -> new_sync_read() <font color="blue">//read_iter</font>
  ---
    struct iovec iov = { .iov_base = buf, .iov_len = len };
    struct kiocb kiocb;
    struct iov_iter iter;
    ssize_t ret;

    init_sync_kiocb(&kiocb, filp);
<font color="blue">
    // We have said that iov_iter is just buffer, the io is described 
    // by kiocb.ki_filp and kiocb.ki_pos
</font>
    kiocb.ki_pos = (ppos ? *ppos : 0);
    iov_iter_init(&iter, READ, &iov, 1, len);

    ret = call_read_iter(filp, &kiocb, &iter);
  ---
(2) iov array is from userland

static ssize_t vfs_readv(
    struct file *file,
<font color="red">    const struct iovec __user *vec,</font>
    unsigned long vlen,
    loff_t *pos,
    rwf_t flags)
{
    struct iovec iovstack[UIO_FASTIOV];
    struct iovec *iov = iovstack;
    struct iov_iter iter;
    ssize_t ret;

    ret = import_iovec(READ, vec, vlen, ARRAY_SIZE(iovstack), &iov, &iter);
<font color="blue">
    // __import_iovec()->iovec_from_user() would copy the iovec array from userland
</font>
    if (ret >= 0) {
        ret = do_iter_read(file, &iter, pos, flags);
        kfree(iov);
    }

    return ret;
}

</pre>
<li> KVEC, address from kernel, BVEC, a page in it
<pre>
struct kvec {
    void *iov_base;
    size_t iov_len;
};

struct bio_vec {
    struct page    *bv_page;
    unsigned int    bv_len;
    unsigned int    bv_offset;
};

nbd_send_cmd() use both of them,

nbd_send_cmd()
---
    struct nbd_request request = {.magic = htonl(NBD_REQUEST_MAGIC)};
    struct kvec iov = {.iov_base = &request, .iov_len = sizeof(request)};
    struct iov_iter from;
<font color="blue">
    //1st kvec is used to send nbd command
</font>
    iov_iter_kvec(&from, WRITE, &iov, 1, sizeof(request));
    ...
    cmd->index = index;
    cmd->cookie = nsock->cookie;
    cmd->retries = 0;
    request.type = htonl(type | nbd_cmd_flags);
    if (type != NBD_CMD_FLUSH) {
        request.from = cpu_to_be64((u64)blk_rq_pos(req) << 9);
        request.len = htonl(size);
    }
    handle = nbd_cmd_handle(cmd);
    memcpy(request.handle, &handle, sizeof(handle));
<font color="blue">
    //the iov_iter 'from' carry the nbd command
</font>
    result = sock_xmit(nbd, index, 1, &from,
            (type == NBD_CMD_WRITE) ? MSG_MORE : 0, &sent);

send_pages:
    if (type != NBD_CMD_WRITE)
        goto out;
<font color="blue">
    //Then let's send out the request with bvec iov_iter_bvec
</font>
    bio = req->bio;
    while (bio) {
        struct bio *next = bio->bi_next;
        struct bvec_iter iter;
        struct bio_vec bvec;

        bio_for_each_segment(bvec, bio, iter) {
            bool is_last = !next && bio_iter_last(bvec, iter);
            int flags = is_last ? 0 : MSG_MORE;

<font color="blue">
            //Setup a iov_iter for every bvec
</font>
            iov_iter_bvec(&from, WRITE, &bvec, 1, bvec.bv_len);
            ...
            result = sock_xmit(nbd, index, 1, &from, flags, &sent);
            ...
            if (is_last)
                break;
        }
        bio = next;
    }
---
</pre>
</ul>
The other modules that receive iov_iter could use following interfaces to handle it
<pre>

size_t copy_page_to_iter(struct page *page, size_t offset, size_t bytes,
             struct iov_iter *i)
{
    if (unlikely(!page_copy_sane(page, offset, bytes)))
        return 0;
<font color="red">
    if (i->type & (ITER_BVEC|ITER_KVEC)) {
</font>
        void *kaddr = kmap_atomic(page);
        size_t wanted = copy_to_iter(kaddr + offset, bytes, i);
        kunmap_atomic(kaddr);
        return wanted;
    } else if (unlikely(iov_iter_is_discard(i)))
        return bytes;
    else if (likely(!iov_iter_is_pipe(i)))
        return copy_page_to_iter_iovec(page, offset, bytes, i);
    else
        return copy_page_to_iter_pipe(page, offset, bytes, i);
}


size_t _copy_to_iter(const void *addr, size_t bytes, struct iov_iter *i)
{
    const char *from = addr;
    if (unlikely(iov_iter_is_pipe(i)))
        return copy_pipe_to_iter(addr, bytes, i);
    if (iter_is_iovec(i))
        might_fault();
    iterate_and_advance(i, bytes, v,
<font color="blue">
        //IOVEC, userland buffers, the 'I' in iterate_and_advance
</font>
        copyout(v.iov_base, (from += v.iov_len) - v.iov_len, v.iov_len),
<font color="blue">
        //BVEC, pages, the 'B' in iterate_and_advance
</font>
        memcpy_to_page(v.bv_page, v.bv_offset,
                   (from += v.bv_len) - v.bv_len, v.bv_len),
<font color="blue">
        //KVEC, kernel buffers, the 'K' in iterate_and_advance
</font>
        memcpy(v.iov_base, (from += v.iov_len) - v.iov_len, v.iov_len)
    )

    return bytes;
}
</pre>
</font>
</p>

<h3><a name="inode_lock_and_fault">inode lock and page fault</font></h3>
<p>
<font size="2">
We cannot take the inode lock (inode.i_rwsem) in page fault path.<br/>
For example,
<pre>
ext4_dax_vm_ops.ext4_dax_huge_fault()
---
    if (write) {
        ...
    } else {
<font color="red">        down_read(&EXT4_I(inode)->i_mmap_sem);</font>
    }
    result = dax_iomap_fault(vmf, pe_size, &pfn, &error, &ext4_iomap_ops);
    if (write) {
        ...
    } else {
<font color="red">        up_read(&EXT4_I(inode)->i_mmap_sem);</font>
    }
---

xfs_file_vm_ops.xfs_filemap_fault()
---
<font color="blue">
    // XFS_MMAPLOCK_SHARED -> xfs_inode_t.i_mmaplock
</font>
    xfs_ilock(XFS_I(inode), <font color="red">XFS_MMAPLOCK_SHARED</font>);
    if (IS_DAX(inode)) {
        pfn_t pfn;

        ret = dax_iomap_fault(vmf, pe_size, &pfn, NULL,
                (write_fault && !vmf->cow_page) ?
                 &xfs_direct_write_iomap_ops :
                 &xfs_read_iomap_ops);
        if (ret & VM_FAULT_NEEDDSYNC)
            ret = dax_finish_sync_fault(vmf, pe_size, pfn);
    } else {
        if (write_fault)
            ret = iomap_page_mkwrite(vmf,
                    &xfs_buffered_write_iomap_ops);
        else
            ret = filemap_fault(vmf);
    }
    xfs_iunlock(XFS_I(inode), XFS_MMAPLOCK_SHARED);
---
</pre>
<B>Why ?</B>
Refer to link, https://lwn.net/Articles/548098 <br/>
<pre>
The problem has to do with the order in which locks are acquired.
For normal filesystem operations, the filesystem code will obtain any locks it requires;
the memory management subsystem will then grab mmap_sem should that be required â€” to bring a read or write buffer into RAM, for example. 
When a page fault happens, though, the lock ordering is reversed:
first mmap_sem is taken, then the filesystem code is invoked to bring the needed page into RAM.
</pre>
Let's look at the code that could show us the lock ordering,
<ul>
<li> fs lock -> mmap lock
<pre>
ext4_dax_read_iter()
  -> <font color="red">inode_lock_shared()</font>
  -> dax_iomap_rw()
    -> iomap_apply()
      -> dax_iomap_actor()
        -> dax_copy_to_iter()
          -> _copy_mc_to_iter()
          <font color="blue">
          // when we write to userland buffer, page fault would happen, and then
          // do_user_addr_fault() would come
          </font>
</pre>
<li> mmap lock -> fs lock
<pre>
do_user_addr_fault()
<font color="red">  -> mmap_read_lock(mm)</font>
  -> handle_mm_fault()
    -> __handle_mm_fault()
      -> handle_pte_fault()
        -> do_fault()
          -> do_read_fault()
            -> __do_fault()
              -> vma->vm_ops->fault(vmf);
                 ext4_dax_fault()
                 xfs_filemap_fault()
</pre>
</ul>
</font>
</p>

<h3><a name="sendfile">sendfile</font></h3>
<p>
<font size="2">
sendfile is one of the feature to implement zero-copy<br/>
<pre>

    read       write              sendfile
        |uuuuuu|        ---\
        ^      \        ---/
-------/--------\--------------------------------------
      /          v
  |ssssss|   |dddddd|           |ssssss|  -> |dddddd|

do_sendfile()
  -> splice_direct_to_actor()
    -> do_splice_to() //<font color="blue">read from src into pipe</font>
      -> f_op->splice_read()
    -> do_splice_from() //<font color="blue">write from pipe into dest</font>
      -> f_op->splice_write()
</pre>
Let's how does the pipe dance here ?
<ul>
<li> splice_read
<pre>

generic_file_splice_read()
  -> ext4_file_read_iter()
    -> generic_file_read_iter()
      -> generic_file_read_iter()
        -> copy_page_to_iter()
          -> copy_page_to_iter_pipe()
---
    off = i->iov_offset;
<font color="red">    buf = &pipe->bufs[i_head & p_mask];</font>
    if (off) {
        if (offset == off && buf->page == page) {
            /* merge with the last one */
            buf->len += bytes;
            i->iov_offset += bytes;
            goto out;
        }
        i_head++;
        buf = &pipe->bufs[i_head & p_mask];
    }
    if (pipe_full(i_head, p_tail, pipe->max_usage))
        return 0;

    buf->ops = &page_cache_pipe_buf_ops;
    get_page(page);
<font color="red">
    buf->page = page;
    buf->offset = offset;
    buf->len = bytes;
</font>
    pipe->head = i_head + 1;
    i->iov_offset = offset + bytes;
    i->head = i_head;
out:
    i->count -= bytes;
---

The pipe here looks like the another kind of bvec arrays.
</pre>
<li> splice_write
<pre>
iter_file_splice_write()
---
    pipe_lock(pipe);

    splice_from_pipe_begin(&sd);
    while (sd.total_len) {
        struct iov_iter from;
        unsigned int head, tail, mask;
        size_t left;
        int n;

        ret = splice_from_pipe_next(pipe, &sd);
        if (ret <= 0)
            break;

        ...
        head = pipe->head;
        tail = pipe->tail;
        mask = pipe->ring_size - 1;

        /* build the vector */
        left = sd.total_len;
        for (n = 0; !pipe_empty(head, tail) && left && n < nbufs; tail++, n++) {
            struct pipe_buffer *buf = &pipe->bufs[tail & mask];
            size_t this_len = buf->len;

            if (this_len > left)
                this_len = left;
            ...
            array[n].bv_page = buf->page;
            array[n].bv_len = this_len;
            array[n].bv_offset = buf->offset;
            left -= this_len;
        }
<font color="blue">
        // construct a bvec iov_iter from the pipe
</font>
        iov_iter_bvec(&from, WRITE, array, n, sd.total_len - left);
        ret = vfs_iter_write(out, &from, &sd.pos, 0);
        if (ret <= 0)
            break;

        sd.num_spliced += ret;
        sd.total_len -= ret;
        *ppos = sd.pos;
<font color="blue">
        /* dismiss the fully eaten buffers, adjust the partial one */
</font>
        tail = pipe->tail;
        while (ret) {
            struct pipe_buffer *buf = &pipe->bufs[tail & mask];
            if (ret >= buf->len) {
                ret -= buf->len;
                buf->len = 0;
                pipe_buf_release(pipe, buf);
                tail++;
                pipe->tail = tail;
                if (pipe->files)
                    sd.need_wakeup = true;
            } else {
                buf->offset += ret;
                buf->len -= ret;
                ret = 0;
            }
        }
    }
done:
    kfree(array);
    splice_from_pipe_end(pipe, &sd);

    pipe_unlock(pipe);

---
</pre>
</ul>
</font>
</p>

<h3><a name="open_and_close">open_and_close</font></h3>
<p>
<font size="2">
When you open or close a file, you could get or put many instances' reference.<br/>
Please refer to following comment from link https://lwn.net/Articles/494158/
<pre>
The management of file structure reference counts is done with calls to fget() and fput().
A file structure, which represents an open file, can depend on a lot of resources:
as long as a file is open, the kernel must maintain its underlying storage device,
filesystem, network protocol information, security-related information, user-space
notification requests, and more. An fget() call will ensure that all of those resources
stay around as long as they are needed. A call to fput(), instead, might result in the
destruction of any of those resources. For example, closing the last file on an unmounted
filesystem will cause that filesystem to truly go away.
</pre>
Next, let's try to figure out what they are.<br/>
We could get some hint in the __fput()
<pre>
__fput is deferred to the task work context which will be invoked before the
task returns to userland. Refer to the following comment to get the point

----
What all this means is that a call to fput() can do a lot of work, and that
work may require the acquisition of a number of locks. The problem is that
fput() can also be called from any number of contexts; there are a few hundred
fput() and fput_light() calls in the kernel. Each of those call sites has its
own locking environment and, usually, no knowledge of what code in other
subsystems may be called from fput(). So the potential for problems like
locking-order violations is real.
----

void fput_many(struct file *file, unsigned int refs)
{
    if (atomic_long_sub_and_test(refs, &file->f_count)) {
        struct task_struct *task = current;

        if (likely(!in_interrupt() && !(task->flags & PF_KTHREAD))) {
            init_task_work(&file->f_u.fu_rcuhead, ____fput);
            if (!task_work_add(task, &file->f_u.fu_rcuhead, TWA_RESUME))
                return;
        }

        if (llist_add(&file->f_u.fu_llist, &delayed_fput_list))
            schedule_delayed_work(&delayed_fput_work, 1);
    }
}

exit_to_user_mode_loop()
  -> tracehook_notify_resume()
    -> task_work_run()
</pre>
<br/>
Let's focus on the dput() and mntput() in __fput().
<ul>
<li> mntput()
<pre>
mntput()
  -> mntput_no_expire()
---
    ...
    lock_mount_hash();
    smp_mb();
    mnt_add_count(mnt, -1);
    count = mnt_get_count(mnt);
    if (count != 0) {
        WARN_ON(count < 0);
        rcu_read_unlock();
        unlock_mount_hash();
        return;
    }
    ...
    if (likely(!(mnt->mnt.mnt_flags & MNT_INTERNAL))) {
        struct task_struct *task = current;
        if (likely(!(task->flags & PF_KTHREAD))) {
            init_task_work(&mnt->mnt_rcu, __cleanup_mnt);
            if (!task_work_add(task, &mnt->mnt_rcu, TWA_RESUME))
                return;
        }
        if (llist_add(&mnt->mnt_llist, &delayed_mntput_list))
            schedule_delayed_work(&delayed_mntput_work, 1);
        return;
    }
    cleanup_mnt(mnt);
---

If the reference of the mnt is exhausted, cleanup_mnt() will be invoked.
cleanup_mnt()
---
    ...
<font color="blue">
    //put the dentry of the mount point
</font>
    dput(mnt->mnt.mnt_root);
<font color="blue">
    //This is the most important part, the real work of umount would be done
    //here
</font>
    deactivate_super(mnt->mnt.mnt_sb);
    mnt_free_id(mnt);
    call_rcu(&mnt->mnt_rcu, delayed_free_vfsmnt);
---

deactivate_super()
  -> deactivate_locked_super()
    -> fs->kill_sb()
       kill_block_super()
       ---
        generic_shutdown_super(sb);
        sync_blockdev(bdev);
        WARN_ON_ONCE(!(mode & FMODE_EXCL));
        blkdev_put(bdev, mode | FMODE_EXCL);
       ---

generic_shutdown_super()
---
<font color="blue">
    // shrink the dcache to release all of the reference to inode cache
</font>
    shrink_dcache_for_umount(sb);
    sync_filesystem(sb);
    sb->s_flags &= ~SB_ACTIVE;

<font color="blue">
    /* evict all inodes with zero refcount */
    // Can we ensure all of the inode to be evicted ?
    // It should be YES, because every opened file holds a reference of mount.
    // When we reach here, all of them should be closed.
</font>
    evict_inodes(sb);
    ...
    if (!list_empty(&sb->s_inodes)) {
        printk("VFS: Busy inodes after unmount of %s. "
           "Self-destruct in 5 seconds.  Have a nice day...\n",
           sb->s_id);
    }
---
</pre>
<li> dput()
<pre>
Every opened file holds a reference to the dentry and every dentry holds a
reference to inode.
</pre>
</ul>

The mnt and dentry associated with the file is assigned in vfs_open()
<pre>
path_openat()
  -> link_path_walk()
  -> open_last_lookups()
  -> do_open()
    -> vfs_open()
int vfs_open(const struct path *path, struct file *file)
{
<font color="red">
    file->f_path = *path;
</font>
    return do_dentry_open(file, d_backing_inode(path->dentry), NULL);
}

</pre>
</font>
</p>

<h3><a name="mount">mount</font></h3>
<p>
<font size="2">
Let's first look at what will be done during a mount
<ul>
<li> prepare filesystem context
<pre>
do_new_mount()
  -> fs_context_for_mount()
    -> alloc_fs_context()
      -> fc->fs_type->init_fs_context()
         legacy_init_fs_context()
<font color="red">
         fc->ops = &legacy_fs_context_ops
</font>
</pre>
<li> get_tree
<pre>
vfs_get_tree()
  -> fc->ops->get_tree()
     legacy_get_tree()
     -> fc->fs_type->mount()
        ext4_mount()
          -> mount_bdev() with <font color="red">ext4_fill_super()</font>
mount_bdev()
---
    bdev = blkdev_get_by_path(dev_name, mode, fs_type);
    ...
    mutex_lock(&bdev->bd_fsfreeze_mutex);
    ...
    s = sget(fs_type, test_bdev_super, set_bdev_super, flags | SB_NOSEC,
         bdev);
    mutex_unlock(&bdev->bd_fsfreeze_mutex);
    if (s->s_root) {
        ...
    } else {
        s->s_mode = mode;
        snprintf(s->s_id, sizeof(s->s_id), "%pg", bdev);
        sb_set_blocksize(s, block_size(bdev));
        error = fill_super(s, data, flags & SB_SILENT ? 1 : 0);
        ---
            root = ext4_iget(sb, EXT4_ROOT_INO, EXT4_IGET_SPECIAL);
            ...
<font color="red">
            sb->s_root = d_make_root(root);
</font>
        ---
        ...
        s->s_flags |= SB_ACTIVE;
        bdev->bd_super = s;
    }

    return dget(s->s_root);
---
</pre>
</ul>
</font>
</p>

<h3><a name="writenotify">writenotify</font></h3>
<p>
<font size="2">
When a file is mapped, how does the kernel know the file is written ? <br/>
The answer is writenotify<br/>
<ul>
<li> vma access permissions
<pre>
do_mmap()
  -> mmap_region()
    -> vma_set_page_prot()
    ---
    unsigned long vm_flags = vma->vm_flags;
    pgprot_t vm_page_prot;

    vm_page_prot = vm_pgprot_modify(vma->vm_page_prot, vm_flags);
    if (vma_wants_writenotify(vma, vm_page_prot)) {
<font color="red">
        vm_flags &= ~VM_SHARED;
</font>
        vm_page_prot = vm_pgprot_modify(vm_page_prot, vm_flags);
    }
<font color="blue">
    // Note, vma->vm_flags is not modified
</font>
    /* remove_protection_ptes reads vma->vm_page_prot without mmap_lock */
    WRITE_ONCE(vma->vm_page_prot, vm_page_prot);
    ---
vma_wants_writenotify()
---
    /* If it was private or non-writable, the write bit is already clear */
    if ((vm_flags & (VM_WRITE|VM_SHARED)) != ((VM_WRITE|VM_SHARED)))
        return 0;
<font color="blue">
    /* The backer wishes to know when pages are first written to? */
</font>
    if (vm_ops && (vm_ops->page_mkwrite || vm_ops->pfn_mkwrite))
                           ^^^^^^^^^^^^            ^^^^^^^^^^^
        return 1;
---
The vma->wm_page_prot will be used when vmf_insert_mixed_mkwrite()

Finally, the mapped page doesn't have write permission
</pre>
<li> page fault due to no write permission
<pre>
handle_pte_fault()
---
    if (vmf->flags & FAULT_FLAG_WRITE) {
<font color="red">
        if (!pte_write(entry))
</font>
            return do_wp_page(vmf);
        entry = pte_mkdirty(entry);
    }
<font color="blue">
    // Set the pte access flags
</font>
    entry = pte_mkyoung(entry);
    if (ptep_set_access_flags(vmf->vma, vmf->address, vmf->pte, entry,
                vmf->flags & FAULT_FLAG_WRITE)) {
        update_mmu_cache(vmf->vma, vmf->address, vmf->pte);
    }
---

do_wp_page()
  -> wp_page_shared()
    -> do_page_mkwrite()

The page will be marked dirty during this.
</pre>
<li> clear the write permission again
<pre>
clear_page_dirty_for_io() is invoked during page writeback
</pre>
</ul>
</font>
</p>

<h2><a name="inode">inode</a></h2>
<hr style="height:3px;border:none;border-top:3px solid black;" />
<h3><a name="reference_of_inode">reference of inode</a></h3>
<p>
<font size="2">
The put of reference of inode is done in iput(),
<pre>
void iput(struct inode *inode)
{
    if (!inode)
        return;
    BUG_ON(inode->i_state & I_CLEAR);
retry:
    if (atomic_dec_and_lock(&inode->i_count, &inode->i_lock)) {
        if (inode->i_nlink && (inode->i_state & I_DIRTY_TIME)) {
            atomic_inc(&inode->i_count);
            spin_unlock(&inode->i_lock);
            trace_writeback_lazytime_iput(inode);
            mark_inode_dirty_sync(inode);
            goto retry;
        }
<font color="red">        iput_final(inode);</font>
    }
}
Regarding to the lazytime, refer to this section <a href="#lazytime_mode">lazytime mode</a>
</pre>
When the last reference of an inode is dropped, we may have two choices,
<ul>
<li> evict the inode
<li> retain indoe in cache if fs is active
</ul>
Before talking deeply into how to determine evict or not, let's look into<br>
When is the reference of inode get ?
<pre>
<font color="blue">
/*
 * inode->i_lock must be held
 */
</font>
void __iget(struct inode *inode)
{
    atomic_inc(&inode->i_count);
}

__iget() <- iterate_bdevs()
</pre>

</font>
</p>


<h3><a name="lazytime_mode">lazytime mode</a></h3>
<p>
<font size="2">
The commit is
<pre>
commit 0ae45f63d4ef8d8eeec49c7d8b44a1775fff13e8
Author: Theodore Ts'o <tytso@mit.edu>
Date:   Mon Feb 2 00:37:00 2015 -0500

    vfs: add support for a lazytime mount option
    
    Add a new mount option which enables a new "lazytime" mode.  <U>This mode
    causes atime, mtime, and ctime updates to only be made to the
    in-memory version of the inode.</U>  The on-disk times will only get
    updated when (a) if the inode needs to be updated for some non-time
    related change, (b) if userspace calls fsync(), syncfs() or sync(), or
    (c) just before an undeleted inode is evicted from memory.
    
    This is OK according to POSIX because there are no guarantees after a
    crash unless userspace explicitly requests via a fsync(2) call.
    
    For workloads which feature a large number of random write to a
    preallocated file, the lazytime mount option significantly reduces
    writes to the inode table.  The repeated 4k writes to a single block
    will result in undesirable stress on flash devices and SMR disk
    drives.  Even on conventional HDD's, the repeated writes to the inode
    table block will trigger Adjacent Track Interference (ATI) remediation
    latencies, which very negatively impact long tail latencies --- which
    is a very big deal for web serving tiers (for example).
    
    Google-Bug-Id: 18297052
    
    Signed-off-by: Theodore Ts'o <tytso@mit.edu>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

</pre>
Firstly, let's look at the 3 time fields in POSIX,
<ul>
<li> ctime, create time
<li> atime, access time
<li> mtime, modify time
</ul>
And they will be modified in following code path,
<ul>
<li> atime
<pre>
file_accessed()
  -> touch_atime()
    -> update_time() //<font color="blue">S_ATIME</font>

file_accessed() could be invoked by
 - generic_file_read_iter() // in direct IO path
 - generic_file_buffered_read()
 - generic_file_mmap()
 - ext4_dax_read_iter()
 ...
</pre>
<li> ctime and mtime
<pre>
file_update_time()
  -> update_time() // S_MTIME or S_CTIME

filemap_page_mkwrite()
  -> file_update_time()

__generic_file_write_iter() //cover both buffer and direct IO path
  -> file_update_time()
</pre>
</ul>
The time of inode is updated in generic_update_time,
<pre>
generic_update_time()
  -> __mark_inode_dirty()
    -> sb->s_op->dirty_inode()
       ext4_dirty_inode()
     ---
<font color="blue">
        // If flags only contain I_DIRTY_TIME, just return and leave the
        // modified times fields in memory
</font>
        if (flags == I_DIRTY_TIME)
            return;
        handle = ext4_journal_start(inode, EXT4_HT_INODE, 2);
        ...
        ext4_mark_inode_dirty(handle, inode);
          -> __ext4_mark_inode_dirty()
            -> ext4_mark_iloc_dirty()
              -> ext4_do_update_inode() // <font color="blue">synchronize the inode in-memory to the one on-disk</font>

        ext4_journal_stop(handle);
     ---
</pre>
If we look into the __mark_inode_dirty,<br/>
we could find out that I_DIRTY_TIME is set on inode->i_state, and the inode is inserted into the wb->b_dirty_time<br/>
There is no modifications on the on-disk inode buffer in-memory, what does the wb flush ?<br/>
<ul>
<li> When will the inode on wb->b_dirty_time be flushed ?
<pre>
wb_writeback()
  -> queue_io()
  ---
    if (!work->for_sync)
        time_expire_jif = jiffies - dirtytime_expire_interval * HZ;
    moved += move_expired_inodes(&wb->b_dirty_time, &wb->b_io,
                     time_expire_jif);
  ---

unsigned int dirtytime_expire_interval = 12 * 60 * 60;
<font color="blue">
See it !?, <B>it is 12 hours</B>
</font>
</pre>
<li> How to write out the dirty inode with I_DIRTY_TIME
<pre>
__writeback_single_inode()
---
    ret = do_writepages(mapping, wbc);
<font color="blue">
    /*
     * Make sure to wait on the data before writing out the metadata.
     * This is important for filesystems that modify metadata on data
     * I/O completion. We don't do it for sync(2) writeback because it has a
     * separate, external IO completion path and ->sync_fs for guaranteeing
     * inode metadata is written back correctly.
     */
</font>
    if (wbc->sync_mode == WB_SYNC_ALL && !wbc->for_sync) {
        int err = filemap_fdatawait(mapping);
        if (ret == 0)
            ret = err;
    }

    spin_lock(&inode->i_lock);

    dirty = inode->i_state & I_DIRTY;
    if ((inode->i_state & I_DIRTY_TIME) &&
        ((dirty & I_DIRTY_INODE) ||
         wbc->sync_mode == WB_SYNC_ALL || wbc->for_sync ||
         time_after(jiffies, inode->dirtied_time_when +
            dirtytime_expire_interval * HZ))) {
        dirty |= I_DIRTY_TIME;
        trace_writeback_lazytime(inode);
    }
    inode->i_state &= ~dirty;
    ...
    smp_mb();

    if (mapping_tagged(mapping, PAGECACHE_TAG_DIRTY))
        inode->i_state |= I_DIRTY_PAGES;

    spin_unlock(&inode->i_lock);
<font color="blue">
<U>
    //See it ? I_DIRTY_SYNC will be set here and ext4_dirty_inode() won't do
    nothing anymore
</U>
</font>
    if (dirty & I_DIRTY_TIME)
        mark_inode_dirty_sync(inode);
    /* Don't write the inode if only I_DIRTY_PAGES was set */
    if (dirty & ~I_DIRTY_PAGES) {
        int err = write_inode(inode, wbc);
        if (ret == 0)
            ret = err;
    }
---
</pre>
</ul>
Well, a normal case that flush out the time fields of inode should be in iput()
<pre>
void iput(struct inode *inode)
{
    if (!inode)
        return;
    BUG_ON(inode->i_state & I_CLEAR);
retry:
    if (atomic_dec_and_lock(&inode->i_count, &inode->i_lock)) {
        if (inode->i_nlink && (inode->i_state & I_DIRTY_TIME)) {
            atomic_inc(&inode->i_count);
            spin_unlock(&inode->i_lock);
            trace_writeback_lazytime_iput(inode);
<font color="red">
            mark_inode_dirty_sync(inode);
</font>
            goto retry;
        }
        iput_final(inode);
    }
}
</pre>
</font>
</p>

<h3><a name="inode_cache">inode cache</a></h3>
<p>
<font size="2">
Where is the inode cache ?<br/>
<pre>
    inode_hashtable =
        alloc_large_system_hash("Inode-cache",
                    sizeof(struct hlist_head),
                    ihash_entries,
                    14,
                    HASH_ZERO,
                    &i_hash_shift,
                    &i_hash_mask,
                    0
</pre>
The inode would be inserted into this hash table to be looked up quickly.
<ul>
<li> INSERT
<pre>
insert_inode_locked() <- __ext4_new_inode()
<font color="blue">
// Look up in cache, create a new one if not exist(I_NEW).
</font>
iget_locked() <- __ext4_iget()

<font color="blue">
Search for the inode specified by @hashval and @data in the inode cache,
and if present it is return it with an increased reference count. This is
a generalized version of iget_locked() for file systems where the inode
number is not sufficient for unique identification of an inode.
</font>
inode_insert5 <- iget5_locked()
              <- insert_inode_locked4()

__insert_inode_hash() <- insert_inode_hash()
</pre>
<li> LOOKUP
<pre>
A very common code path should be,
ext4_lookup()
---
<font color="blue">
    //Try to lookup the file in directory entry
</font>
    bh = ext4_lookup_entry(dir, dentry, &de);
    if (bh) {
        __u32 ino = le32_to_cpu(de->inode);
        brelse(bh);
        ...
<font color="blue">
        // Get a valid inode id, try to get the inode for it
</font>
        inode = ext4_iget(dir->i_sb, ino, EXT4_IGET_NORMAL);
          -> __ext4_iget()
          ---
            inode = iget_locked(sb, ino);
            ...
<font color="blue">
            // It is cached in inode cache
</font>
            if (!(inode->i_state & I_NEW))
                return inode;

            ei = EXT4_I(inode);
            iloc.bh = NULL;
<font color="blue">
            // Get the inode on disk
</font>
            ret = __ext4_get_inode_loc_noinmem(inode, &iloc);
            ...
            raw_inode = ext4_raw_inode(&iloc);
          ---
---
</pre>
</ul>
Some filesystems, such as xfs, maintain a inode cache itself.
<pre>
xfs_lookup()
  -> xfs_dir_lookup()
  -> xfs_iget()
  ---
<font color="blue">
    // See it ? this is a fs private inode cache and it is more scalable
</font>
    pag = xfs_perag_get(mp, XFS_INO_TO_AGNO(mp, ino));
    agino = XFS_INO_TO_AGINO(mp, ino);

again:
    error = 0;
    rcu_read_lock();
    ip = radix_tree_lookup(&pag->pag_ici_root, agino);

    if (ip) {
        error = xfs_iget_cache_hit(pag, ip, ino, flags, lock_flags);
        ...
    } else {
        rcu_read_unlock();
        ...

        error = xfs_iget_cache_miss(mp, pag, tp, ino, &ip,
                            flags, lock_flags);
                            
        ...
        ---
<font color="blue">
        // Allocate xfs_inode where a vfs inode is embedded in
</font>
        ip = xfs_inode_alloc(mp, ino);
        ...
        } else {
<font color="blue">
            // get the xfs_buf for this inode
</font>
            error = xfs_imap_to_bp(mp, tp, &ip->i_imap, &dip, &bp, 0);
<font color="blue">
            // fill the inode with data on disk
</font>
            error = xfs_inode_from_disk(ip, dip);
            xfs_trans_brelse(tp, bp);
        }
        ...
<font color="red">
        iflags = XFS_INEW;
</font>
        if (flags & XFS_IGET_DONTCACHE)
            d_mark_dontcache(VFS_I(ip));
        xfs_iflags_set(ip, iflags);

        /* insert the new inode */
        spin_lock(&pag->pag_ici_lock);
        error = radix_tree_insert(&pag->pag_ici_root, agino, ip);
        spin_unlock(&pag->pag_ici_lock);
        ---
    }
    xfs_perag_put(pag);
  ---
</pre>
Another thing need to be talked is the way to reclaim indoe cache
<ul>
<li> Which one could be reclaimed
<pre>

<B>For xfs</B>

iput()
  -> iput_final()
  ---
    if (op->drop_inode)
        drop = op->drop_inode(inode);
    else
        drop = generic_drop_inode(inode);
<font color="blue">
    // In generic_drop_inode, there are 3 conditions,
    //  - !inode->i_nlink, means the file or hardlink have been cut off
    //  - inode_unhashed(), means this inode is not in inode hash table
          <font color="red">for xfs, this true,</font> because it use inode cache of its own
    //  - I_DONTCACHE
</font>
    if (!drop && (sb->s_flags & SB_ACTIVE)) {
        inode_add_lru(inode);
        spin_unlock(&inode->i_lock);
        return;
    }
    ...
    WRITE_ONCE(inode->i_state, state | I_FREEING);
    if (!list_empty(&inode->i_lru))
        inode_lru_list_del(inode);
    spin_unlock(&inode->i_lock);

    evict(inode);
  ---
evict()
  -> destroy_inode()
    -> sb ops->destroy_inode()
       xfs_fs_destroy_inode()
         -> xfs_inactive() //<font color="blue">Won't do more because i_nlink is not zero</font>>
         -> xfs_inode_set_reclaim_tag()
         --
             xfs_perag_set_reclaim_tag(pag);
<font color="red">            __xfs_iflags_set(ip, XFS_IRECLAIMABLE);</font>
         --

This XFS_IRECLAIMABLE will be handled by
xfs_iget_cache_hit()
---
<font color="blue">
    /*
     * If IRECLAIMABLE is set, we've torn down the VFS inode already.
     * Need to carefully get it back into useable state.
     */
</font>
    if (ip->i_flags & XFS_IRECLAIMABLE) {
        ...
    }
---
<B>For normal</B>
prune_icache_sb()
  -> inode_lru_isolate()
  ---
<font color="blue">
    /*
     * Referenced or dirty inodes are still in use. Give them another pass
     * through the LRU as we canot reclaim them now.
     */
</font>
    if (atomic_read(&inode->i_count) ||
        (inode->i_state & ~I_REFERENCED)) {
        list_lru_isolate(lru, &inode->i_lru);
        spin_unlock(&inode->i_lock);
        this_cpu_dec(nr_unused);
        return LRU_REMOVED;
    }
  ---
</pre>
</ul>
</font>
</p>

<a name="dcache">Dcache</a>
<hr style="height:3px;border:none;border-top:3px solid black;" />
<a name="shrink_of_dcache">shrink of dcache</a>
<p>
<font size="2">
<ul>
<li> shrinker
<pre>
prune_dcache_sb()
---
    freed = list_lru_shrink_walk(&sb->s_dentry_lru, sc,
                     <font color="red">dentry_lru_isolate</font>, &dispose);
<font color="red">    shrink_dentry_list(&dispose);</font>
---
dentry_lru_isolate()
---
    if (!spin_trylock(&dentry->d_lock))
        return LRU_SKIP;
<font color="blue">
    /*
     * Referenced dentries are still in use. If they have active
     * counts, just remove them from the LRU. Otherwise give them
     * another pass through the LRU.
     */
</font>
    if (dentry->d_lockref.count) {
        d_lru_isolate(lru, dentry);
        spin_unlock(&dentry->d_lock);
        return LRU_REMOVED;
    }

    if (dentry->d_flags & DCACHE_REFERENCED) {
        dentry->d_flags &= ~DCACHE_REFERENCED;
        spin_unlock(&dentry->d_lock);
        return LRU_ROTATE;
    }

    d_lru_shrink_move(lru, dentry, freeable);
    spin_unlock(&dentry->d_lock);

    return LRU_REMOVED;
---

shrink_dentry_list()
  -> __dentry_kill()
    -> __d_drop()
      -> ___d_drop() //<font color="blue">remove the dentry from the hashtable</font>
    -> dentry_free()
      -> call_rcu(&dentry->d_u.d_rcu, __d_free);<font color="blue">Nothing will be freed under rcu_read_lock</font>
</pre>
<li> get and put
<pre>
dget()
---
    if (dentry)
        lockref_get(&dentry->d_lockref);
    return dentry;
---

dput()
---
    rcu_read_lock();
        if (likely(fast_dput(dentry))) {
            rcu_read_unlock();
            return;
        }
<font color="blue">
        /* Slow case: now with the dentry lock held */
</font>
        rcu_read_unlock();

        if (likely(retain_dentry(dentry))) {
            spin_unlock(&dentry->d_lock);
            return;
        }

        dentry = dentry_kill(dentry);
---
retain_dentry()
---
<font color="blue">
    //vfs_unlink() -> __d_drop() could cause this.
</font>
    if (unlikely(d_unhashed(dentry)))
        return false;
    ...
<font color="blue">
    // Two rounds in lru
</font>
    dentry->d_lockref.count--;
    if (unlikely(!(dentry->d_flags & DCACHE_LRU_LIST)))
        d_lru_add(dentry);
    else if (unlikely(!(dentry->d_flags & DCACHE_REFERENCED)))
        dentry->d_flags |= DCACHE_REFERENCED;
    return true;
---

To prevent dentry from being reclaimed,
(1) hold a reference with dget() (d_lru_isolate() will remove it from lru list in dentry_lru_isolate)
(2) use it and then DCACHE_REFERENCED will always be set
</pre>
</ul>
</font>
</p>

<a name="lookup">lookup</a>
<p>
<font size="2">
<ul>
<li> overview
<pre>
do_filp_open()
---
<font color="blue">
    // Try to use LOOKUP_RCU first, if failed with ECHILD,
    // try again w/o LOOKUP_RCU
</font>
    filp = path_openat(&nd, op, flags | LOOKUP_RCU);
    if (unlikely(filp == ERR_PTR(-ECHILD)))
        filp = path_openat(&nd, op, flags)
---

path_openat()
---
        const char *s = path_init(nd, flags); //<font color="blue">rcu_read_lock is get here</font>
<font color="blue">
        /home/jianchwa/linux-stable/fs/dcache.c 
        \____________ _______________/ \__ __/
                     v                    v
              link_path_walk()     open_last_lookups()
</font>
        while (!(error = link_path_walk(s, nd)) &&
               (s = open_last_lookups(nd, file, op)) != NULL)
            ;
        if (!error)
            error = do_open(nd, file, op); <font color="blue">// do the finally open</font>
        terminate_walk(nd);

---
</pre>
<li> fast lookup under rcu_read_lock
<pre>
There are 3 points here regarding to fast lookup
(1) it is only to accelerate the dentry cache hit cases
    if not hit, lookup would fallback to non-rcu mode
(2) dentry won't be freed during this but maybe killed
(3) <U>sequence count is used to protect the lookup against rename</U>

Regarding to the point 3,
vfs_rename()
  -> d_move()
    -> __d_move()
    ---
    spin_lock_nested(&dentry->d_lock, 2);
    spin_lock_nested(&target->d_lock, 3);
    ...
    write_seqcount_begin(&dentry->d_seq);
    write_seqcount_begin_nested(&target->d_seq, DENTRY_D_LOCK_NESTED);

    /* unhash both */
    if (!d_unhashed(dentry))
        ___d_drop(dentry);
    if (!d_unhashed(target))
        ___d_drop(target);

    /* ... and switch them in the tree */
    dentry->d_parent = target->d_parent;
    if (!exchange) {
        copy_name(dentry, target);
        target->d_hash.pprev = NULL;
        dentry->d_parent->d_lockref.count++;
        if (dentry != old_parent) /* wasn't IS_ROOT */
            WARN_ON(!--old_parent->d_lockref.count);
    }
    ...
    list_move(&dentry->d_child, &dentry->d_parent->d_subdirs);
    __d_rehash(dentry);
    ...
    write_seqcount_end(&target->d_seq);
    write_seqcount_end(&dentry->d_seq);
    ---

__d_lookup_rcu()
---
    hlist_bl_for_each_entry_rcu(dentry, node, b, d_hash) {
        unsigned seq;

seqretry:
<font color="blue">
        /*
         * The dentry sequence count protects us from concurrent
         * renames, and thus protects parent and name fields.
         * Otherwise, we may get a wrong entry, for example,
         * during rename /home/will/aaaa to /home/will/bbbb,
         * we could get baaa, bbaa, bbba, and if these files
         * do exist, look up is screwed up.
         */
</font>
        seq = raw_seqcount_begin(&dentry->d_seq);
        if (dentry->d_parent != parent)
            continue;
        if (d_unhashed(dentry))
            continue;

        if (unlikely(parent->d_flags & DCACHE_OP_COMPARE)) {
            ...
        } else {
            if (dentry->d_name.hash_len != hashlen)
                continue;
            if (dentry_cmp(dentry, str, hashlen_len(hashlen)) != 0)
                continue;
        }
        *seqp = seq;
        return dentry;
    }
---

lookup_fast()
---
    if (nd->flags & LOOKUP_RCU) {
        unsigned seq;
        dentry = __d_lookup_rcu(parent, &nd->last, &seq);
        ...
        *inode = d_backing_inode(dentry);
        if (unlikely(read_seqcount_retry(&dentry->d_seq, seq)))
            return ERR_PTR(-ECHILD);
<font color="blue">
        /*
         * This sequence count validates that the parent had no
         * changes while we did the lookup of the dentry above.
         */

         Why do we need to care about the parent ?
         renaming of the parent won't influence the childrens, right ?
</font>
        if (unlikely(__read_seqcount_retry(&parent->d_seq, nd->seq)))
            return ERR_PTR(-ECHILD);

        *seqp = seq;
        status = d_revalidate(dentry, nd->flags);
        if (likely(status > 0))
            return dentry;
    }
---

Before the finally open, we would recheck the sequence count,
do_open()
  -> complete_walk()
    -> unlazy_walk()
      -> legitimize_path()
           ---
            if (unlikely(!lockref_get_not_dead(&path->dentry->d_lockref))) {
                path->dentry = NULL;
                return false;
            }
            return !read_seqcount_retry(&path->dentry->d_seq, seq);
          ---
</pre>
</ul>
</font>
</p>

<h3><a name="dcache_and_metadata">dcache and metadata</a></h3>
<p>
<font size="2">
The main points that dcache interacts with filesystem
<pre>
link_path_walk()
  -> walk_component()
    -> lookup_slow()
      -> inode_lock_shared()
      -> __lookup_slow()
        -> d_alloc_parallel() //<font color="blue">allocate a dentry structure</font>
<font color="red">        -> inode->i_op->lookup()</font>
<font color="blue">
          look up dentry in metadata,
          do d_add() if found which would add dentry into dcache hashtable             
</font>
open_last_lookups()
  -> lookup_open()
    -> d_lookup() //<font color="blue">do look up under rename raed seqlock</font>
    -> dir_inode->i_op->create() //<font color="blue">do create if needed</font>

</pre>
</font>
</p>

<h3><a name="add_to_dcache_hash">add to dcache hash</a></h3>
<p>
<font size="2">
When a dentry is in the dcache hash table, namely, dentry_hashtable<br/>
__d_lookup could find it and d_unhashed() returns false.<br/>
When is the dentry inserted to the dentry_hashtable ?<br/>
<ul>
<li> lookup_open
<pre>
When the file does not exist,
lookup_open()
<font color="blue">
  // allocate dentry structure with d_alloc()
</font>
  -> d_alloc_parallel()
  -> dir_inode->i_op->lookup()
     ext2_lookup()
       -> ext2_inode_by_name() <font color="blue">//not found at this moment</font>
       -> d_splice_alias() <font color="blue">//inode is NULL at this moment</font>
  -> dir_inode->i_op->create()
     ext2_create()
       -> ext2_add_nondir()
         -> d_instantiate_new()
           -> d_instantiate() //fill inode information for a dentry
</pre>
<li> lookup
<pre>
When the file exists,
lookup_slow()
  -> ext2_lookup()
    -> d_splice_alias()
      -> __d_add()
        -> __d_set_inode_and_type() // install the inode into the dentry
          -> __d_rehash()
</pre>
</ul>
</font>
</p>

<h2><a name="tmpfs">Tmpfs</a></h2>
<hr style="height:3px;border:none;border-top:3px solid black;" />
<p>
<font size="2">
<ul>
<li> Tmpfs
<pre>
Everything in tmpfs is temporary in the sense that no files will be
created on your hard drive. The files live in memory and swap
space. If you unmount a tmpfs instance, everything stored therein is
lost.
</pre>

<li> File
<pre>
An in-core inode with specific operations.
See shmem_get_inode()

An important thing is the dentry of this inode will be pined in memory
shmem_mknod()
---
    inode = shmem_get_inode(dir->i_sb, dir, mode, dev, VM_NORESERVE);
    if (inode) {
        ...
        dir->i_size += BOGO_DIRENT_SIZE;
        dir->i_ctime = dir->i_mtime = current_time(dir);
        d_instantiate(dentry, inode);
        dget(dentry); /* Extra count - pin the dentry in core */
    }
---
<font color="red">
The dentry for the inode will be perserved in dcache until it is deleted
</font>
</pre>
<li> hard link
<pre>
An in-core dentry points to the linked inode

shmem_link()
---
    dir->i_size += BOGO_DIRENT_SIZE;
    inode->i_ctime = dir->i_ctime = dir->i_mtime = current_time(inode);
    inc_nlink(inode);
    ihold(inode);    /* New dentry reference */
    dget(dentry);<font color="blue">        /* Extra pinning count for the created dentry */</font>
    d_instantiate(dentry, inode);
---
</pre>
</font>
</p>

<h2><a name="xfs_log">Xfs Log</a></h2>
<hr style="height:3px;border:none;border-top:3px solid black;" />
<p>
<font size="2">
Let's take some relatively simple examples,
<ul>
<li> inode update log
<pre>
<li><B>log format (record)</B>
xfs_inode_item_ops.xfs_inode_item_format()
  -> xfs_inode_item_format_core()
    -> xfs_inode_to_log_dinode_ts()
      -> copy the in-memory inode to <font color="red">xfs_log_dinode</font>

Note ! <font color="red">the modifications are not applied on the inode cluster xfs buffer.</font>
<li><B>log push (checkpoint)</B>
xfs_inode_item_push()
---
    if (xfs_ipincount(ip) > 0 || xfs_buf_ispinned(bp) ||
        (ip->i_flags & XFS_ISTALE))
        return XFS_ITEM_PINNED;

    if (xfs_iflags_test(ip, XFS_IFLUSHING))
        return XFS_ITEM_FLUSHING;
<font color="blue">
    // cluster buffer need to be locked during IO
</font>
    if (!xfs_buf_trylock(bp))
        return XFS_ITEM_LOCKED;

    spin_unlock(&lip->li_ailp->ail_lock);

    xfs_buf_hold(bp);
<font color="blue">
    // flush the dirty inodes into cluster buffer
</font>
    error = xfs_iflush_cluster(bp);
    if (!error) {
<font color="blue">
    // queue the cluster buffer to delayed write queue
</font>
        if (!xfs_buf_delwri_queue(bp, buffer_list))
            rval = XFS_ITEM_FLUSHING;
        xfs_buf_relse(bp);
    }

---
</pre>
<li> xfs buf log
<pre>
<li><B>log format (record)</B>
xfs_item_ops.xfs_buf_item_format()
  -> xfs_buf_item_format_segment()
  ---
    for (;;) {
<font color="blue">
        //The bitmap here describes which 128 bytes chunks of the buffer have
        // been dirtied. This is done by
        // xfs_trans_log_buf()
        //   -> xfs_buf_item_log()
        //     -> xfs_buf_item_log_segment()_
</font>
        next_bit = xfs_next_bit(blfp->blf_data_map, blfp->blf_map_size,
                    (uint)last_bit + 1);
        if (next_bit == -1) {
            xfs_buf_item_copy_iovec(lv, vecp, bp, offset,
                        first_bit, nbits);
            blfp->blf_size++;
            break;
        } else if (next_bit != last_bit + 1 ||
                   xfs_buf_item_straddle(bp, offset, next_bit, last_bit)) {
            xfs_buf_item_copy_iovec(lv, vecp, bp, offset,
                        first_bit, nbits);
            blfp->blf_size++;
            first_bit = next_bit;
            last_bit = next_bit;
            nbits = 1;
        } else {
            last_bit++;
            nbits++;
        }
    }
<font color="red">
See it !? modifications are made into xfs buffer directly.
And we will see that the push just add the xfs buffer into the del write queue.
</font>
  ---
<li><B>log push(checkpoint)</B>
xfsaild_push()
  -> xfsaild_push_item()
    -> lip->li_ops->iop_push()
       xfs_buf_item_push()
       ---
        if (xfs_buf_ispinned(bp))
            return XFS_ITEM_PINNED;
        if (!xfs_buf_trylock(bp)) {
            if (xfs_buf_ispinned(bp))
                return XFS_ITEM_PINNED;
            return XFS_ITEM_LOCKED;
        }
<font color="red">
        if (!xfs_buf_delwri_queue(bp, buffer_list))
</font>
            rval = XFS_ITEM_FLUSHING;
        xfs_buf_unlock(bp);
       ---
  -> xfs_buf_delwri_submit_nowait()
    -> xfs_buf_delwri_submit_buffers()
    ---
    list_sort(NULL, buffer_list, xfs_buf_cmp);

    blk_start_plug(&plug);
    list_for_each_entry_safe(bp, n, buffer_list, b_list) {
<font color="blue">
    // The buf could be pinned and locked by new writing
    // During the whole IO, the xfs buf is locked.
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
</font>
        if (!wait_list) {
            if (xfs_buf_ispinned(bp)) {
                pinned++;
                continue;
            }
            if (!xfs_buf_trylock(bp))
                continue;
        }
        ...
        bp->b_flags &= ~_XBF_DELWRI_Q;
        bp->b_flags |= XBF_WRITE;
        if (wait_list) {
            ...
        } else {
            bp->b_flags |= XBF_ASYNC;
            list_del_init(&bp->b_list);
        }
        __xfs_buf_submit(bp, false);
<font color="blue">
        // bio->bi_end_io = xfs_buf_bio_end_io
</font>
    }
    blk_finish_plug(&plug);
    ---

xfs_buf_bio_end_io()
  -> xfs_buf_ioend_async()
    -> queue_work xfs_buf_ioend_work()
      -> xfs_buf_ioend()
        -> xfs_buf_item_done() //<font color="blue">log item is freed here under xfs buf lock</font>
        -> xfs_buf_relse()
          -> xfs_buf_unlock()
</pre>
</ul>
</font>
</p>

<h3><a name="xlog_framework">Framework</a></h3>
<p>
<font size="2">
iclog ring and state machine
<pre>
      .-->                                       
     /   ---
       /     \
      | iclog |
       \     /
         ---  /
          <--''                      -->
|------------------------------------------------|
           physical log space

The iclog has two parameters,
l_iclog_bufs      8
l_iclog_size      32K (max 256K)

iclog is allocated in xlog_alloc_log()
---
    for (i = 0; i < log->l_iclog_bufs; i++) {
        int align_mask = xfs_buftarg_dma_alignment(mp->m_logdev_targp);
        size_t bvec_size = howmany(log->l_iclog_size, PAGE_SIZE) *
                sizeof(struct bio_vec);

        iclog = kmem_zalloc(sizeof(*iclog) + bvec_size, KM_MAYFAIL);
        ...    
        iclog->ic_data = kmem_alloc_io(log->l_iclog_size, align_mask,
                        KM_MAYFAIL | KM_ZERO);
        ...
        iclog->ic_size = log->l_iclog_size - log->l_iclog_hsize;
        iclog->ic_state = XLOG_STATE_ACTIVE;
        ...
<font color="red">
        // link all of the iclog in a ring
</font>
        iclogp = &iclog->ic_next;
    }
---

The iclog in the ring will be employed one by one.
And every iclog has a state machine works as following,

XLOG_STATE_ACTIVE <font color="blue">Be able to receive log</font>
  - xlog_alloc_log()
  - xlog_state_do_callback()
      -> xlog_state_clean_iclog()
        -> xlog_state_activate_iclogs()
          -> xlog_state_activate_iclog()

XLOG_STATE_WANT_SYNC
  - xlog_state_switch_iclogs()
  
XLOG_STATE_SYNCING
  - xlog_state_release_iclog()
      -> __xlog_state_release_iclog()
      -> xlog_sync()

XLOG_STATE_DONE_SYNC
  - xlog_ioend_work()
    -> xlog_state_done_syncing()

XLOG_STATE_CALLBACK
  - xlog_state_done_syncing()
    -> xlog_state_do_callback()

</pre>


<ul>
<li> Transaction Format
<pre>
                 XLOG_REG_TYPE_TRANSHDR          XLOG_REG_TYPE_COMMIT
                  |                                |
< oph >< oph >< reg0 >< oph >< reg1 >< oph >...< regn >
   |                            |
  XLOG_START_TRANS             XLOG_REG_TYPE_ICORE

</pre>
<li> IO Format
<pre>
The transaction above is carried by iclog when sent out to disk.

Note !!! iclog is just used when issue IO. The log item is carried in anther place.
xlog_write()
---
            if (copy_len > 0) {
<font color="blue">
                // The reg here is a xfs_log_iovec
                // It will copy the data from xfs_log_iovec to iclog buffer
</font>
                memcpy(ptr, reg->i_addr + copy_off, copy_len);
                xlog_write_adv_cnt(&ptr, &len, &log_offset,
                           copy_len);
            }
---

The buffer carries log is allocated here,
Refer to xlog_cil_alloc_shadow_bufs() / xlog_cil_iovec_space()

The buffer for iclog contains header and payload, the header's layout is
xlog_rec_header_t.

Every iclog contains a crc num.
xlog_sync()
---
    /* calculcate the checksum */
    iclog->ic_header.h_crc = xlog_cksum(log, &iclog->ic_header,
                        iclog->ic_datap, size)
---
</pre>
<li> Transaction Durability
<pre>
In theory, when the commit record IO is completed, xlog guarantee that the whole
transaction has been on disk. How to implement this ?
There are two points here:
<ul>
<li> commit callback
The ctx is installed on the iclog->ic_callbacks where the commit record is written
xlog_cil_push_work()
---
    list_add_tail(&ctx->iclog_entry, &commit_iclog->ic_callbacks);
---
xlog_state_do_callback() would ensure the commit callback is invoked by the order of iclog.
<font color="red">
This indicates that even if the commit record is not on the same iclog with its
records (must be the one behind),  when the callback of commit record is invoked,
its records's IO have been completed.
</font>

xlog_state_do_callback()
---
        do {
<font color="blue">
                           ------------------->
                                                  Loop start here
                                                     ACTIVE
                                                  Current Head   ACTIVE    ACTIVE   ACTIVE
                                                        |
                                                        v
            // iclog0 -> iclog1 -> iclog2 -> iclog3 -> iclog4 -> iclog5 -> iclog6 -> iclog7
                  ^        ^         ^         ^
                  |        |         |         |
               SYNCING   SYNC_DONE  SYNC_DONE SYNCING
               [record]    [commit]
</font>
            if (xlog_state_iodone_process_iclog(log, iclog,
                            &ioerror))
                break;
            ...
            xlog_state_do_iclog_callbacks(log, iclog);
            if (XLOG_FORCED_SHUTDOWN(log))
                wake_up_all(&iclog->ic_force_wait);
            else
                xlog_state_clean_iclog(log, iclog);
            iclog = iclog->ic_next;
        } while (first_iclog != iclog);
---
<li> REQ_FUA
xlog_write_iclog()
---
    iclog->ic_bio.bi_opf = REQ_OP_WRITE | REQ_META | REQ_SYNC |
                REQ_IDLE | REQ_FUA;
---
</ul>


</pre>
</ul>
</font>
</p>

<h3><a name="relog">Relog</a></h3>
<p>
<font size="2">
Quote from https://www.infradead.org/~mchehab/kernel_docs/filesystems/xfs-delayed-logging-design.html
<pre>
XFS allows multiple separate modifications to a single object to be carried in the log at any given time.
<U>This allows the log to avoid needing to flush each change to disk before recording a new change to the object.</U>
XFS does this via a method called â€œre-loggingâ€. Conceptually, this is quite simple - all it requires is
that any new change to the object is recorded with a new copy of all the existing changes in the new
transaction that is written to the log.
</pre>
Regarding to the comment with underline, we could refer to the implementation of jbd2,
<pre>
jbd2 could be deemed as a WAL in blocks, namely, before flush the dirty blocks to real
position on disk, jbd2 would record them on journal first. In common case, jbd2 would
shadow the original buffer_head to do the journal IO.

jbd2_journal_write_metadata_buffer()
---
    spin_lock(&jh_in->b_state_lock);
repeat:
    if (jh_in->b_frozen_data) {
        ...
    } else {
        new_page = jh2bh(jh_in)->b_page;
        new_offset = offset_in_page(jh2bh(jh_in)->b_data);
    }
    ...
    set_bh_page(new_bh, new_page, new_offset);
    new_bh->b_size = bh_in->b_size;
    new_bh->b_bdev = journal->j_dev;
    new_bh->b_blocknr = blocknr;
    new_bh->b_private = bh_in;
    set_buffer_mapped(new_bh);
    set_buffer_dirty(new_bh);

    *bh_out = new_bh;

    spin_lock(&journal->j_list_lock);
    __jbd2_journal_file_buffer(jh_in, transaction, BJ_Shadow);
    spin_unlock(&journal->j_list_lock);
<font color="red">
    set_buffer_shadow(bh_in);
</font>
    spin_unlock(&jh_in->b_state_lock); //<font color="blue">Protect this journal buffer head</font>
---

do_get_write_access()
---
    spin_lock(&jh->b_state_lock);
    ...
    if (buffer_shadow(bh)) {
        spin_unlock(&jh->b_state_lock);
        wait_on_bit_io(&bh->b_state, BH_Shadow, TASK_UNINTERRUPTIBLE);
        goto repeat;
    }
    ...
---
<font color="red">
A very important thing need to be noted is that the modification has been made in
buffer_head which is the cache of the disk.
</font>
</pre>

</font>
</p>

<h3><a name="deferred_operations">Deferred operations</a></h3>
<p>
<font size="2">
Deferred Operations is a very bad name and could mislead the readers. <br/>
IMO, it should be called Big Transaction. The deferred operations,<br/>
cooperating with intent log, could split a complicated transaction into <br/>
multiple small transactions and still keep the atomicity of the original<br/>
transaction. Look at the following example,
<pre>
To complete T, we need following multiple operations,
T_A, T_B, T_C, T_D. And each of them need 3 sub-operations.

Deferred Operations would complete this work as following,

Intent log for T_A \
Intent log for T_B  \ t0
Intent log for T_C  /
Intent log for T_D /
-------------------
Done log for T_A  \
Real log for T_A0  \ t1
Real log for T_A1  /
Real log for T_A2 /
-------------------
Done log for T_B  \
Real log for T_B0  \ t2
Real log for T_B1  /
Real log for T_B2 /
-------------------
Done log for T_C  \
Real log for T_C0  \ t4
Real log for T_C1  /
Real log for T_C2 /
-------------------
Done log for T_D  \
Real log for T_D0  \ t5
Real log for T_D1  /
Real log for T_D2 /
-------------------

The Intent log could guarantee the whole big transaction's atomicity.
</pre>
xfs_defer_finish_noroll() is to carry out the work,
<pre>
xfs_defer_finish_noroll()
---
    /* Until we run out of pending work to finish... */
    while (!list_empty(&dop_pending) || !list_empty(&(*tp)->t_dfops)) {
<font color="blue">
        //Create intent log for every deferred_operations
</font>
        xfs_defer_create_intents(*tp);
        list_splice_init(&(*tp)->t_dfops, &dop_pending);
<font color="blue">
        //Roll the transaction
</font>
        error = xfs_defer_trans_roll(tp);
        ...
        dfp = list_first_entry(&dop_pending, struct xfs_defer_pending,
                       dfp_list);
<font color="blue">
        //Pick a defered operation and finish it
        // - create done
        // - finish_item <font color="red">do the real work</font>
</font>
        error = xfs_defer_finish_one(*tp, dfp);
    }
---
</pre>
</font>
</p>

<h3><a name="log_space">log space</a></h3>
<p>
<font size="2">
<pre>

      tail         head
   cycle=100     cycle=100 
        |           |
        v           v
|-------xxxxxxxxxxxx------------|



      head              tail
   cycle=101         cycle=100 
        |                |
        v                v
|xxxxxxx-----------------xxxxxxx|
</pre>
<ul>
<li> Why xfs need two log head ?
<pre>
It is related to XFS_TRANS_PERM_LOG_RES,
This kind of transaction could be rolled by multiple times.
Such as

#define    XFS_SYMLINK_LOG_COUNT        3
#define    XFS_REMOVE_LOG_COUNT        2
#define    XFS_LINK_LOG_COUNT        2
#define    XFS_RENAME_LOG_COUNT        2
#define    XFS_WRITE_LOG_COUNT        2
#define    XFS_WRITE_LOG_COUNT_REFLINK    8

A reflink transaction could be split into 8 sub-transactions.
If one sub-transaction need T bytes log space, we need
(1) reserve 8 * T log space on l_reserve_head,
(2) reserve T log space when transaction is rolled.

Refer to the code in xfs_trans_reserve()
---
<font color="blue">
    // rolled transaction
</font>
    if (tp->t_ticket != NULL) {
            ASSERT(resp->tr_logflags & XFS_TRANS_PERM_LOG_RES);
            error = xfs_log_regrant(mp, tp->t_ticket);
        } else {
            error = xfs_log_reserve(mp,
                        resp->tr_logres,
                        resp->tr_logcount,
                        &tp->t_ticket, XFS_TRANSACTION,
                        permanent);
        }

---
</pre>
The log space reserved here is always surplus.<br/>
There are two value in ticket, t_curr_res and t_unit_res.<br/>
<pre>
xlog_cil_alloc_shadow_bufs() will adapt the t_curr_res
based on the real situation and then give the left back in 
xfs_log_commit_cil()
  -> xfs_log_ticket_regrant/ungrant()


The log space will given back after AIL commit with push the lsn tail.
</pre>
<li> lsn of a transaction
<pre>
In software,
xfs_cil_ctx represents a transaction.
xfs_log_vec represents a log item and its space in memory. (li_lv and li_lv_shadow)
lsn         represents cycle << 32 | log block number //>>

There are two lsn for a Transaction
<B>(1) start lsn</B>
xlog_cil_push_work()
  -> xlog_write(log, &lvhdr, tic, <font color="red">&ctx->start_lsn,</font> NULL, 0, true)
  ---
        /* start_lsn is the first lsn written to. That's all we need. */
        if (!*start_lsn)
            *start_lsn = be64_to_cpu(iclog->ic_header.h_lsn);
  ---

<B>(2) commit lsn</B>
xlog_cil_push_work()
  -> xlog_commit_record(log, tic, &commit_iclog, <font color="red">&commit_lsn</font>);

The io completion callback of log block (iclog) is invoked <U>in the order of lsn</U>.
And the sign of successful committing of a transaction is the commit lsn (the log block) has been on disk
</pre>
<li> release of log space
<pre>
The releasing of log space is through pushing the <font color="red">tail lsn</font> forward.
The tail lsn is actually the minimum of <font color="red">start lsn</font> of non-applied log.

The tail lsn could be modified in two ways,
<B>(1) relog</B>
xfs_trans_committed()
<font color="blue">
    // See it ? ctx>start_lsn is used here !!!
</font>
  -> xfs_trans_committed_bulk(ctx->cil->xc_log->l_ailp, ctx->lv_chain, <font color="red">ctx->start_lsn</font>, abort)
    -> xfs_log_item_batch_insert()
      -> xfs_trans_ail_update_bulk()
      ---
    for (i = 0; i < nr_items; i++) {
        struct xfs_log_item *lip = log_items[i];
<font color="blue">
        // Has been on AIL list, relog case !!!
        // It also inicates that the previous log has been on disk !!!
</font>
        if (test_and_set_bit(XFS_LI_IN_AIL, &lip->li_flags)) {
            /* check if we really need to move the item */
            if (XFS_LSN_CMP(lsn, lip->li_lsn) <= 0)
                continue;

<font color="blue">
            // relog would release its previous log space !!!
</font>
            if (mlip == lip && !tail_lsn)
                tail_lsn = lip->li_lsn;

<font color="blue">
            // remove from the ail list and re-insert it later
</font>
            xfs_ail_delete(ailp, lip);
        } else {
            trace_xfs_ail_insert(lip, 0, lsn);
        }
        lip->li_lsn = lsn;
        list_add(&lip->li_ail, &tmp);
    }

<font color="blue">
    // all the log items uses the same lsn, namely, the start lsn of the transaction
</font>
    if (!list_empty(&tmp))
        xfs_ail_splice(ailp, cur, &tmp, lsn);

    xfs_ail_update_finish(ailp, tail_lsn);
      ---

<B>(2) checkpoint</B>
xfs_buf_ioend()
  -> xfs_buf_item_done()
    -> xfs_trans_ail_delete()
      -> xfs_ail_delete_one()
      ---
<font color="blue">
        // The minimum one is the head of the ailp->ail_head
</font>
        struct xfs_log_item    *mlip = xfs_ail_min(ailp);
        xfs_lsn_t        lsn = lip->li_lsn;

        xfs_ail_delete(ailp, lip);
        clear_bit(XFS_LI_IN_AIL, &lip->li_flags);
        lip->li_lsn = 0;

        if (mlip == lip)
            return lsn;
        return 0;
      ---
      -> xfs_ail_update_finish()

Why there is only one xfs_buf_ioend() here ?
Look at the definition of iop_push, only inode, dquot and buf define it.
This indicates that most of the transactions are made of xfs buf updating.

And inode updating would be applied on xfs buf finally.
Refer to
xfs_inode_item_push()
  -> xfs_iflush_cluster()
  -> xfs_buf_delwri_queue()
</pre>
</ul>
</font>
</p>

<h2><a name="xfs_inode">Xfs Inode</a></h2>
<hr style="height:3px;border:none;border-top:3px solid black;" />

<h3><a name="xfs_inode_mgnt">Inode management</a></h3>
<p>
<font size="2">
The inode number if xfs is composed with 3 parts,
<pre>
       AG number        Bn in AG       In in B
  |----------------|---------------|-------------|

Bn : block number in an AG
In : inode number in a block (a block could carry multiple inodes)
</pre>
This inode number has telled us the position of the inode on disk.<br/>
xfs's inodes are dynamically allocated instead of preallocating in static<br/>
position like ext4.<br/>
How to allocate in dynamical way ?<br/>
Allocate a block in that AG !<br/>
^^^^^^^^^^^^^^^^^^^^^^^^^^^
Then create a xfs_inobt_rec as following,
<pre>
struct xfs_inobt_rec {
    __be32   ir_startino;  //<font color="blue">start ino num of this chunk</font>
    __be32   ir_freecount; //<font color="blue">number of free inodes</font>
    __be64   ir_free; //<font color="blue">bitmap</font>

}
</pre>
and insert it into the AG inode b+tree
</font>
</p>


<h2><a name="xfs_buf">xfs buf</a></h2>
<hr style="height:3px;border:none;border-top:3px solid black;" />
<p>
<font size="2">
xfs uses xfs_buf to manage its metadata instead of using vfs pagecache.
<ul>
<li> allocate
<pre>
xfs_buf_get_map()
---
    error = xfs_buf_find(target, map, nmaps, flags, NULL, &bp);
    if (!error)
        goto found;
    if (error != -ENOENT)
        return error;

    error = _xfs_buf_alloc(target, map, nmaps, flags, &new_bp);
<font color="blue">
    // sema_init(&bp->b_sema, 0); /* held, no waiters */
</font>

    error = xfs_buf_allocate_memory(new_bp, flags);
    ---
<font color="blue">
    /*
     * for buffers that are contained within a single page, just allocate
     * the memory from the heap - there's no need for the complexity of
     * page arrays to keep allocation down to order 0.
     */
</font>
    size = BBTOB(bp->b_length);
    if (size < PAGE_SIZE) {
        int align_mask = xfs_buftarg_dma_alignment(bp->b_target);
        bp->b_addr = kmem_alloc_io(size, align_mask,
                       KM_NOFS | kmflag_mask);
        ...
        bp->b_offset = offset_in_page(bp->b_addr);
        bp->b_pages = bp->b_page_array;
        bp->b_pages[0] = kmem_to_page(bp->b_addr);
        bp->b_page_count = 1;
        bp->b_flags |= _XBF_KMEM;
        return 0;
    }

use_alloc_page:
    ...
    for (i = 0; i < bp->b_page_count; i++) {
        struct page    *page;
        uint        retries = 0;
retry:
        page = alloc_page(gfp_mask);
        ...
        nbytes = min_t(size_t, size, PAGE_SIZE - offset);
        size -= nbytes;
        bp->b_pages[i] = page;
        offset = 0;
    }

    ---
<font color="blue">
    // Do insert if new_bp is not NULL
</font>
    error = xfs_buf_find(target, map, nmaps, flags, new_bp, &bp);
    ...
found:
    if (!bp->b_addr) {
        error = _xfs_buf_map_pages(bp, flags);
        ...
    }
---
</pre>
<li> lookup
<pre>
xfs_buf_find()
---
    pag = xfs_perag_get(btp->bt_mount,
                xfs_daddr_to_agno(btp->bt_mount, cmap.bm_bn));

    spin_lock(&pag->pag_buf_lock);
    bp = rhashtable_lookup_fast(&pag->pag_buf_hash, &cmap,
                    xfs_buf_hash_params);
    if (bp) {
        atomic_inc(&bp->b_hold);
        goto found;
    }
    ...
found:
    spin_unlock(&pag->pag_buf_lock);
    xfs_perag_put(pag);
<font color="blue">
    // We need to return a locked xfs_buf here !!!
</font>
    if (!xfs_buf_trylock(bp)) {
        xfs_buf_lock(bp);
        XFS_STATS_INC(btp->bt_mount, xb_get_locked_waited);
    }

---
</pre>
<li> read from disk
<pre>
xfs_buf_read_map()
---
    error = xfs_buf_get_map(target, map, nmaps, flags, &bp);
    ...
    if (!(bp->b_flags & XBF_DONE)) {
<font color="blue">        /* Initiate the buffer read and wait. */</font>
        bp->b_ops = ops;
        error = _xfs_buf_read(bp, flags);
          -> xfs_buf_submit()
            -> __xfs_buf_submit() <font color="blue">// wait = !(bp->b_flags & XBF_ASYNC)</font>
            ---
<font color="blue">
            /*
              * Grab a reference so the buffer does not go away underneath us. For
              * async buffers, I/O completion drops the callers reference, which
              * could occur before submission returns.
              */
</font>
            xfs_buf_hold(bp);
            ...
            _xfs_buf_ioapply(bp);
            ...
            if (wait)
                error = xfs_buf_iowait(bp);

            xfs_buf_rele(bp);
            ---

        /* Readahead iodone already dropped the buffer, so exit. */
        if (flags & XBF_ASYNC)
            return 0;
    }
---

<font color="red" size = "2">
In non-readahead case, we have to wait the buffer to be read in.
</font>

When read completes, xfs would verify it.
xfs_buf_ioend()
---
    if (bp->b_flags & XBF_READ) {
        if (!bp->b_error && bp->b_ops)
<font color="red">            bp->b_ops->verify_read(bp);</font>
    }
---
</pre>
<li> log
<a href="#xfs_log">xfs log__xfs buf log</a>
<li> write to disk
<pre>
The xfs buf being written to disk must have been checkpointed.
During the IO, the xfs_buf is locked. Nobody can touch it.
xfs_buf_delwri_submit_buffers()
  -> xfs_buf_lock()
  -> __xfs_buf_submit() //<font color="blue"> wait is false</font>
     ---
<font color="blue">
    // buf has been locked, after unpin, nobody can pin it any more.
    // the modifications in buf should have been in log.
</font>
    if (bp->b_flags & XBF_WRITE)
        xfs_buf_wait_unpin(bp);
        ---
        if (atomic_read(&bp->b_pin_count) == 0)
            return;

        add_wait_queue(&bp->b_waiters, &wait);
        for (;;) {
            set_current_state(TASK_UNINTERRUPTIBLE);
            if (atomic_read(&bp->b_pin_count) == 0)
                break;
            io_schedule();
        }
        remove_wait_queue(&bp->b_waiters, &wait);
        set_current_state(TASK_RUNNING);
        ---
     ---

How to ensure the metadata to be on disk instead of disk caching, after release
the log space ?

xlog_sync()
---
<font color="blue">
    /*
     * Flush the data device before flushing the log to make sure all meta
     * data written back from the AIL actually made it to disk before
     * stamping the new log tail LSN into the log buffer.  For an external
     * log we need to issue the flush explicitly, and unfortunately
     * synchronously here; for an internal log we can simply use the block
     * layer state machine for preflushes.
     */
</font>
    if (log->l_targ != log->l_mp->m_ddev_targp || split) {
        xfs_blkdev_issue_flush(log->l_mp->m_ddev_targp);
        need_flush = false;
    }

    xlog_verify_iclog(log, iclog, count);
    xlog_write_iclog(log, iclog, bno, count, need_flush);
---
</pre>
<li> reclaim
<pre>
xfs_buf_rele()
---
    spin_lock(&bp->b_lock);
    release = atomic_dec_and_lock(&bp->b_hold, &pag->pag_buf_lock);
    if (!release) {
        if ((atomic_read(&bp->b_hold) == 1) && !list_empty(&bp->b_lru))
            __xfs_buf_ioacct_dec(bp);
        goto out_unlock;
    }
    ...
<font color="blue">
    // insert into the lru list
</font>
    if (!(bp->b_flags & XBF_STALE) && atomic_read(&bp->b_lru_ref)) {
        if (list_lru_add(&bp->b_target->bt_lru, &bp->b_lru)) {
            bp->b_state &= ~XFS_BSTATE_DISPOSE;
            atomic_inc(&bp->b_hold);
        }
        spin_unlock(&pag->pag_buf_lock);
    } 
---

xfs_buftarg_shrink_scan()
---
    freed = list_lru_shrink_walk(&btp->bt_lru, sc,
                     xfs_buftarg_isolate, &dispose);

    while (!list_empty(&dispose)) {
        struct xfs_buf *bp;
        bp = list_first_entry(&dispose, struct xfs_buf, b_lru);
        list_del_init(&bp->b_lru);
        xfs_buf_rele(bp);
    }
    freed = list_lru_shrink_walk(&btp->bt_lru, sc,
                     xfs_buftarg_isolate, &dispose);

    while (!list_empty(&dispose)) {
        struct xfs_buf *bp;
        bp = list_first_entry(&dispose, struct xfs_buf, b_lru);
        list_del_init(&bp->b_lru);
        xfs_buf_rele(bp);
    }
---
</pre>
<li> story of b_hold
<pre>
The story of b_hold
[0] initial value is 1
    _xfs_buf_alloc()
    ---
        atomic_set(&bp->b_hold, 1);
    ---
[1] xfs_buf_find() get and lock
    ---
    spin_lock(&pag->pag_buf_lock);
    bp = rhashtable_lookup_fast(&pag->pag_buf_hash, &cmap,
                    xfs_buf_hash_params);
    if (bp) {
        atomic_inc(&bp->b_hold);
        goto found;
    }
    ...
found:
    spin_unlock(&pag->pag_buf_lock);
    xfs_perag_put(pag);

    if (!xfs_buf_trylock(bp)) {
        if (flags & XBF_TRYLOCK) {
            xfs_buf_rele(bp);
            XFS_STATS_INC(btp->bt_mount, xb_busy_locked);
            return -EAGAIN;
        }
        xfs_buf_lock(bp);
        XFS_STATS_INC(btp->bt_mount, xb_get_locked_waited);
    }

    ---

There are mainly two ways to release this reference
a. xfs_log_commit_cil()
     -> xfs_trans_free_items()
       -> iop_unlock()
          xfs_buf_item_unlock()
            -> xfs_buf_relse()

b. xfs_trans_brelse()
     -> xfs_trans_del_item()
     -> xfs_buf_relse()

   release a buf if didn't dirty it

[2] buf log item

The buf log item holds a reference of the xfs_buf
xfs_buf_item_init()
  -> xfs_buf_hold()

This reference will be released in xfs_buf_iodone()
xfs_buf_iodone()
---
    xfs_buf_rele(bp);
    spin_lock(&ailp->ail_lock);
    xfs_trans_ail_delete(ailp, lip, SHUTDOWN_CORRUPT_INCORE);
    xfs_buf_item_free(BUF_ITEM(lip));
---

[3] delwri_queue
xfs_buf_delwri_queue()
---
    bp->b_flags |= _XBF_DELWRI_Q;
    if (list_empty(&bp->b_list)) {
        atomic_inc(&bp->b_hold);
        list_add_tail(&bp->b_list, list);
    }
---

get lock in xfs_buf_delwri_submit_buffers()
xfs_buf_iodone_callbacks()
  -> xfs_buf_ioend()
    -> xfs_buf_relse()
       unlock and release ref
</pre>
</ul>
</font>
</p>

</body>
</html>


