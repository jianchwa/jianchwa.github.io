<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>VFS</title>
</head>
<body>
<div>
    <h1>VFS</h1> 
</div>
<p>
<font size="2">
<a href="#BH">BH</a>
<ul>
<li><a href="#bh2bio">bh to bio</a>
<li><a href="#BH_state">BH state</a>
</ul>
<a href="#dcache">dcache</a>
<ul>
<li><a href="#path_walking">path_walking</a>
<li><a href="#locks_of_dentry">locks_of_dentry</a>
<li><a href="#lookup_in_parellel">lookup in parellel</a>
</ul>
<a href="#inode">inode</a><br/>
<ul>
<li><a href="#aops">address_space ops</a>
<li><a href="#iops">inode ops</a>
</ul>
<a href="#fs_misc">fs misc</a>
<ul>
<li><a href="#The_truth_of_page_lock">The truth of page lock</a>
</ul>
</font>
</p>


<h2><a name="BH">BH</a></h2>
<p>
<font size="2">
<pre>
Historically, a buffer_head was used to map a single block within a page, and of
course as the unit of I/O through the filesystem and block layers.
Nowadays the basic I/O unit is the bio, and buffer_heads are used for
<ul>
<li> extracting block mappings (via .get_block) 
<pre>
static inline void
map_bh(struct buffer_head *bh, struct super_block *sb, sector_t block)
{
    set_buffer_mapped(bh);
    bh->b_bdev = sb->s_bdev;
    bh->b_blocknr = block;
    bh->b_size = sb->s_blocksize;
}

A single block that a bh represents is the blocksize of filesystem, not the
block device.

In block layer, the logical block size is still 512 bytes which is the
traditional sector size, but the real block logical size of the storage hardware
is varaible. Look at the nvme driver code:
nvme_setup_rw
---
    cmnd->rw.opcode = (rq_data_dir(req) ? nvme_cmd_write : nvme_cmd_read);
    cmnd->rw.nsid = cpu_to_le32(ns->head->ns_id);
    cmnd->rw.slba = cpu_to_le64(nvme_block_nr(ns, blk_rq_pos(req)));
                                  -> <font color="blue">(sector >> (ns->lba_shift - 9))</font>
    cmnd->rw.length = cpu_to_le16((blk_rq_bytes(req) >> ns->lba_shift) - 1);

---
What if the logical block size of the device is larger than 4K ?

</pre>
<li> tracking state of block within a page cache
<pre>
There could be one or more bh in one page. We could see this in
alloc_page_buffers.
---
    head = NULL;
    offset = PAGE_SIZE;
    while ((offset -= size) >= 0) { <font color="blue">// size here is blocksize of fs</font>
        bh = alloc_buffer_head(gfp);
        if (!bh)
            goto no_grow;

        bh->b_this_page = head;
        bh->b_blocknr = -1;
        head = bh;

        bh->b_size = size;

        /* Link the buffer to its page */
        set_bh_page(bh, page, offset);
    }
    return head;
---
</pre>
<li> wrapping bio submission for backward compatibility reasons (e.g. submit_bh).
</ul>
</pre>
</font>
</p>

<h3><a name="bh2bio">bh to bio</a></h3>
<p>
<font size="2">
Look at the submit_bh_wbc to know the basic steps:
<pre>
    ---
    bio = bio_alloc(GFP_NOIO, 1);

    if (wbc) {
        wbc_init_bio(wbc, bio);
        wbc_account_io(wbc, bh->b_page, bh->b_size);
    }

    bio->bi_iter.bi_sector = bh->b_blocknr * (bh->b_size >> 9);
    bio_set_dev(bio, bh->b_bdev);
    bio->bi_write_hint = write_hint;

    bio_add_page(bio, bh->b_page, bh->b_size, bh_offset(bh));
    BUG_ON(bio->bi_iter.bi_size != bh->b_size);

    bio->bi_end_io = end_bio_bh_io_sync;
                      -> bh->b_end_io(bh, !bio->bi_status);
    bio->bi_private = bh;

    /* Take care of bh's that straddle the end of the device */
    guard_bio_eod(op, bio);

    if (buffer_meta(bh))
        op_flags |= REQ_META;
    if (buffer_prio(bh))
        op_flags |= REQ_PRIO;
    bio_set_op_attrs(bio, op, op_flags);

    submit_bio(bio);
    ---
</pre>
</font>
</p>


<h3><a name="BH_state">BH state</a></h3>
<p>
<font size="2">
enum bh_state_bits defines the state of a bh and is contained in bh->b_state<br/>
And there are 3 marcos to define the set, clear and test operations about this
state. They are defined in include/linux/buffer_head.h 
<ul>
<li> set_buffer_##name
<li> clear_buffer_##name
<li> buffer_##name
</ul>
<br/>
Let's look at how to use them in fs.
<ul>
<li> BH_Uptodate
<pre>
Means the bh contains valid data.
When read/write operation is completed successfully, BH_Uptodate will be set on
the bh, otherwise clear it.
</pre>
<li> BH_Lock
<pre>
When a bh is under io, we will lock it.
A very classical scenario is __bread_slow:
---
    lock_buffer(bh);
    if (buffer_uptodate(bh)) {
        unlock_buffer(bh);
        return bh;
    } else {
        get_bh(bh);
        bh->b_end_io = end_buffer_read_sync;   
        submit_bh(REQ_OP_READ, 0, bh);
        wait_on_buffer(bh);
        if (buffer_uptodate(bh))
            return bh;
    }
    brelse(bh);
    return NULL;
---
end_buffer_read_sync
  -> __end_buffer_read_notouch
  ---
    if (uptodate) {
        set_buffer_uptodate(bh);
    } else {
        /* This happens, due to failed read-ahead attempts. */
        clear_buffer_uptodate(bh);
    }
    unlock_buffer(bh)
  ---
</pre>
<li> BH_Dirty
<pre>
BH_Dirty is set in mark_buffer_dirty.
Except for setting BH_Dirty, it would also:
 - set page dirty
 - __mark_inode_dirty(inode, <font color="red">I_DIRTY_PAGES</font>)
   __mark_inode_dirty will hand over this inode to writeback, then the
   dirty data will be written to disk.

The BH_Dirty is usually cleared before IO.
For example:
---
    lock_buffer(bh);
    clear_buffer_dirty(bh);

    get_bh(bh); /* for end_buffer_write_sync() */
    bh->b_end_io = end_buffer_write_sync;
    submit_bh(REQ_OP_WRITE, 0, bh);

    wait_on_buffer(bh);
---
</pre>
<li> BH_Req
<li> BH_New
<li> BH_Uptodate_Lock
<li> BH_Mapped
<pre>
Has a disk mapping, in the other word, this bh corresponds to a block on disk.
It is usually set after get_block.
Look at the follow combinations between Mapped and Uptodate
Mapped Uptodate
  No    No        "unknown" - must do get_block()
  No    Yes       "hole" - zero-filled (no associated block on disk image)
  Yes   No        "allocated" - allocated on disk, not read in
  Yes   Yes       "valid" - allocated and up-to-date in memory.
</pre>
<li> BH_Async_Read/Write
<pre>
The bh->b_end_io is end_buffer_async_read/write.
</pre>
</ul>
</font>
</p>

<h2><a name="dcache">dcache</a></h2>
<hr style="height:5px;border:none;border-top:1px solid black;" />
<p>
<font size="2">
dcache, dentry cache, directory entry cache.<br/>
<pre>
A dentry's core job is to represent a directory or file in filesystem and cache
the mapping between the file/directory and the associated inode. This inode
contains the core operations of filesystem.
</pre>
The dentries encode the fs tree structure, the name of files.
The main part of a dentry is :
<ul>
<li> inode
<li> name (final part of the path)
<li> parent (the name of the containing directory)
</ul>
</font>
</p>

<h3><a name="path_walking">path_walking</a></h3>
<p>
<font size="2">
The path walking is mainly done in link_path_walk, let's look at the skeleton of
it.
<pre>
    for(;;) {
        ...
        hash_len = hash_name(<font color="red">nd->path.dentry, name</font>);
<font color="blue">
        hash_name will calculate the length and hash of the path component.
        hash_len = len << 32 | hash
        hash value is calculated based on pointer of parent dentry and entry name.
</font>
        ...
        nd->last.hash_len = hash_len;
        nd->last.name = name;
        nd->last_type = type;
<font color="blue">
        nd->last is the name component we walking currently.
</font>
        name += hashlen_len(hash_len);
        if (!*name)
            goto OK;
        /*
         * If it wasn't NUL, we know it was '/'. Skip that
         * slash, and continue until no more slashes.
         */
        do {
            name++;
        } while (unlikely(*name == '/'));
        if (unlikely(!*name)) {
            ...
        } else {
            /* not the last component */
            err = walk_component(nd, WALK_FOLLOW | WALK_MORE);
        }
        ...
    }
walk_component will mainly do 3 things:
1. try to get the dentry in the cache
lookup_fast
  -> __d_lookup(&nd->path.dentry, &nd->last)
    -> get hash list by d_hash(name->hash)
2. if not in cache, try to get it from fs
lookup_slow
  -> __lookup_slow
    -> d_alloc_parallel // allocate dentry
    -> inode->i_op->lookup
<font color="blue">
    // this will cause some io to get in the filesystem metadata of directory
    // and inode.
</font>
3. follow_managed
   mountpoint will be resolved here.

</pre>

</font>
</p>

<h3><a name="locks_of_dentry">locks_of_dentry</a></h3>
<p>
<font size="2">
Refer to Documentation/filesystems/path-lookup.txt<br/>
dcache is used to speed up the looking up of inode associated with a path name.
this look up could come from multiple cores concurrently and frequently, so the
lock mechanism is very important. Let's look into the lock of denty cache next
and find out the how it promote the performance.
<pre>
In Documentation/filesystems/path-lookup.txt, it always says <font
color="red">"would like to do path walking without taking locks or reference
counts of intermediate dentries along the path.</font>", why ?

Look into the path lookup process,
    [0]            [2]                    [4]
   +---+      +---------+            +-----------+
   |   v      |         v            |           v
/home/will/Desktop/wangjianchao/source_code/linux-stable/Makefile 
       |      ^         |            ^           |           ^
       +------+         +------------+           +-----------+
          [1]                 [3]                     [5]

[0]  dentry of "/", "home"
[1]  dentry of "home", "will"
[2]  dentry of "will", "Desktop"
[3]  dentry of "Desktop", "wangjianchao"
[4]  dentry of "wangjianchao", "source_code"
[5]  dentry of "source_code", "Makefile"

walk_component will be executed for [0] ~ [5], and lookup_fast will be invoked
every time. At the moment, the component dentry's d_lock has to be locked to
serialize the accessing to the dentry.
__d_lookup
---
        spin_lock(&dentry->d_lock);
        if (dentry->d_parent != parent)
            goto next;
        if (d_unhashed(dentry))
            goto next;

        if (!d_same_name(dentry, parent, name))
            goto next;

        dentry->d_lockref.count++;
        found = dentry;
        spin_unlock(&dentry->d_lock);
---

The contending on the lock of dentry of "home", "will" and "Desktop" should be
very high. On the system of a lot of cores, the dentry cache could become a
scalability problem with workload which perform lot of lookup.
</pre>
Currently, there are two path walking modes:
<ul>
<li> ref-walk
<pre>
ref-walk is the traditional way of performing dcache lookup using d_lock to
serialize the concurrent modifications to the dentry and take a reference count
on it.
</pre>
<li> rcu-walk
<pre>
rcu-walk uses seqcount based dentry lookups, and can perform lookup of intermediate
elements <font color="red">without any stores to shared data</font> in the dentry or inode.
</pre>
</ul>
The 'storing to shared data' means, in ref-walk, it need to:
<ul>
<li> 1. lock and unlock on spinlock d_lock
<pre>
    it is to serialize the modifications to dentry from rename or other
</pre>
<li> 2. reference counter increment of dentry
<pre>
    prevent the dentry to be released.

    in ref-walk, we need to take a reference count when get every intermediate
    component, and put it when step into next one.
    walk_component
      -> lookup_fast
        -> __d_lookup // get the reference
      -> step_into
        -> path_to_nameidata
          -> dput(nd->path.dentry) // if not LOOKUP_RCU
</pre>
</ul>

To kill them, rcu-walk does as following:
<ul>
<li> Take the RCU lock for the entire path walk.
<pre>
path_openat/path_lookupat...
  -> path_init
<font color="red">
    -> rcu_read_lock // if LOOKUP_RCU
</font>
  -> link_path_walk
    -> walk_component
      -> lookup_fast
        -> __d_lookup_rcu
  -> terminate_walk
<font color="red">
    -> rcu_read_unlock // if LOOKUP_RCU
</font>
So now dentry refcounts are not required for dentry persistence.
Because the dentry free employs call_rcu.
dput
  -> dentry_kill
    -> __dentry_kill
      -> dentry_free
        -> call_rcu(&dentry->d_u.d_rcu, __d_free);
</pre>
<li> Use seqlock to protect the dentry name, parent and inode.
<pre>
The snapshot of the dentry's name, parent and inode (for child lookup) will be
protected by the per-dentry seqlock. dentry lookups recheck the sequence after
the child is found in case anything changed in the parent in the path walk.
lookup_fast
---
    if (nd->flags & LOOKUP_RCU) {
        unsigned seq;
        bool negative;
        dentry = __d_lookup_rcu(parent, &nd->last, &seq);
        ---
        <pre>
<font color="blue">
        hlist_bl_for_each_entry_rcu(dentry, node, b, d_hash) {
            unsigned seq;

    seqretry:
        /*
         * The dentry sequence count protects us from concurrent
         * renames, and thus protects parent and name fields.
         *
         * The caller must perform a seqcount check in order
         * to do anything useful with the returned dentry.
         */
            seq = raw_seqcount_begin(&dentry->d_seq);
            if (dentry->d_parent != parent)
                continue;
            if (d_unhashed(dentry))
                continue;

            if (unlikely(parent->d_flags & DCACHE_OP_COMPARE)) {
                ...
            } else {
                if (dentry->d_name.hash_len != hashlen)
                    continue;
                if (dentry_cmp(dentry, str, hashlen_len(hashlen)) != 0)
                    continue;
            }
            *seqp = seq;
            return dentry;
        }
</font>
       </pre>
        ---
        ...
<font color="blude">
        //This sequence count validates that the inode matches
        //the dentry name information from lookup.
</font>
        *inode = d_backing_inode(dentry);
        negative = d_is_negative(dentry);
        if (unlikely(read_seqcount_retry(&dentry->d_seq, seq)))
            return -ECHILD;
        ...
<font color="blude">
        //This sequence count validates that the parent had no
        //changes while we did the lookup of the dentry above.
</font>
        if (unlikely(__read_seqcount_retry(&parent->d_seq, nd->seq)))
            return -ECHILD;

        *seqp = seq;
        ...
    }
---

The write_seqcount on dentry->d_seq occurs when modify the dentry, such as
__d_move.
</pre>
</ul>

There are two points that why seqlock is better than spinlock in almost-read
scenario.
<ul>
<li> No lock contending in read scenario
<li> No write operation in read scenario, this is good for cache.
</ul>
</font>
</p>


<h3><a name="lookup_in_parellel">lookup in parellel</a></h3>
<p>
<font size="2">

There are two parts of dentry look up, the fast path and slow path.<br/>
Let's look at the slow path here.<br/>

Quote from here
<pre>
https://lwn.net/Articles/685108/

All directory operations are done with the inode mutex (i_mutex) held, which prevents anything else 
from touching that directory. But the most common operation, lookup, is non-destructive, so there is
no real conceptual reason to stop it from happening in parallel.
</pre>

The typical scenario could be
<pre>

CPU0     CPU1      CPU2     CPU3     CPU4
T0       T1        T2       T3       T4
  \      \         |        /        /
    \      \       |      /        /
      \      \     |    /        /
               
              /var/log/ 
          T0  T1  T2  T3  T4

If all of the dentries of T0 ~ T1 happen to be not in memory, all of them have
to invoke lookup_slow.

If the lock here is a mutex, the performance will very bad.
</pre>

Then a rw_semaphore is introduced to replace the mutex.
<pre>
static struct dentry *lookup_slow(const struct qstr *name,
                  struct dentry *dir,
                  unsigned int flags)
{
    struct inode *inode = dir->d_inode;
    struct dentry *res;
    inode_lock_shared(inode);
<font color="blue">      -> down_read(&inode->i_rwsem); </font>
    res = __lookup_slow(name, dir, flags);
    inode_unlock_shared(inode);
    return res;
}
</pre>
<br/>
But there is a problem: the mutex currently protects the directory entry (dentry).
A lookup operation can cause dentries to be created, which can lead to races if two
dentries are created for the same name.<br/>
How to handle this ?
<pre>
Look at the d_alloc_parallel.
---
    struct hlist_bl_head *b = in_lookup_hash(parent, hash);
<font color="blue">
    // alloc a dentry structrue here.
</font>
    struct dentry *new = d_alloc(parent, name);
 
retry:
    rcu_read_lock();
    r_seq = read_seqbegin(&rename_lock);
<font color="blue">
    // look up the a dentry with (parent, name) in hash cache
    // there could be some one create a same one concurrently.
</font>
    dentry = __d_lookup_rcu(parent, name, &d_seq);
    if (unlikely(dentry)) {
        ...
<font color="blue">
    // anything changes on the dentry ?
</font>
        if (read_seqcount_retry(&dentry->d_seq, d_seq)) {
            rcu_read_unlock();
            dput(dentry);
            goto retry;
        }
        rcu_read_unlock();
        dput(new);
        return dentry;
    }
    if (unlikely(read_seqretry(&rename_lock, r_seq))) {
        rcu_read_unlock();
        goto retry;
    }

    hlist_bl_lock(b);
<font color="blue">
    // A spin lock here.
    // So there could be only one entering this critical section,
    // namely, only one of concurrent lookups with same parent and name pair
    // could add its dentry on the hash cache, the others have to wait. When
    // they come in this critical section, a dentry with same name and parent
    // pair has been there.
    // At the moment, there are 2 cases:
    //  - the dentry is in lookup, indicating inode->i_op->lookup is ongoing.
    //    we have to wait.
    //  - otherwise, the lookup has been completed, we could return this dentry
    //    directly.
</font>
    hlist_bl_for_each_entry(dentry, node, b, d_u.d_in_lookup_hash) {
        if (dentry->d_name.hash != hash)
            continue;
        if (dentry->d_parent != parent)
            continue;
        if (!d_same_name(dentry, parent, name))
            continue;
        hlist_bl_unlock(b);
        /* now we can try to grab a reference */
        if (!lockref_get_not_dead(&dentry->d_lockref)) {
            rcu_read_unlock();
            goto retry;
        }

        rcu_read_unlock();
        /*
         * somebody is likely to be still doing lookup for it;
         * wait for them to finish
         */
        spin_lock(&dentry->d_lock);
        d_wait_lookup(dentry);
        if (unlikely(dentry->d_name.hash != hash))
            goto mismatch;
        if (unlikely(dentry->d_parent != parent))
            goto mismatch;
        if (unlikely(d_unhashed(dentry)))
            goto mismatch;
        if (unlikely(!d_same_name(dentry, parent, name)))
            goto mismatch;
        /* OK, it *is* a hashed match; return it */
        spin_unlock(&dentry->d_lock);
        dput(new);
        return dentry;
    }
    rcu_read_unlock();
    /* we can't take ->d_lock here; it's OK, though. */
    new->d_flags |= DCACHE_PAR_LOOKUP; <font color="red">// dentry in-lookup is set here.</font>
    new->d_wait = wq;
    hlist_bl_add_head_rcu(&new->d_u.d_in_lookup_hash, b);
    hlist_bl_unlock(b);
    return new;

---
</pre>

</font>
</p>



<h2><a name="inode">inode</a></h2>
<h3><a name="aops">address_space ops</a></h3>
<font size="2">
<ul>
<li> readpage/readpages
<pre>
// f_ops.read_iter
generic_file_read_iter
  -> generic_file_buffered_read

//vm_ops.fault
filemap_fault
  -> page_cache_read // if no_cached_page
    -> __page_cache_alloc
    -> a_ops->readpage
or-> a_ops->readpage // if page_not_uptodate

__do_page_cache_readahead
  -> read_pages
    -> a_ops->readpages // if there is readpages
	// otherwise one by one
    -> a_ops->readpage
</pre>

<li> writepage/writepages
<pre>
// start writeback on mapping dirty pages in range
__filemap_fdatawrite_range
  -> do_writepages
    -> a_ops->writepages or
	   generic_writepages
	     -> pagevec_lookup_range_tag
		 -> __writepage
		   -> a_ops->writepage

// memory management, on-demand
shrink_page_list
  -> pageout
    -> a_ops->writepage

//wb, periodic and on-demand
wb_writeback
  -> writeback_sb_inodes
    -> __writeback_single_inode
	  -> do_writepages
</pre>
<li> direct_IO
<pre>
bypass the pagecache

generic_file_read_iter
if iocb->ki_flags & IOCB_DIRECT
  -> filemap_write_and_wait_range // we will bypass the pagecache, so have to
                                  // write and wait the data in pagecache to disk.
  -> a_ops->direct_IO

__generic_file_write_iter
  -> generic_file_direct_write
    -> file_write_and_wait_range
	-> invalidate_inode_pages2_range //invalidate page cache to ensure buffered
	                                 //reads to go to disk to get new data.
</pre>
</ul>
<li> write_begin/write_end
<pre>
__generic_file_write_iter
  -> generic_perform_write
---
		status = a_ops->write_begin(file, mapping, pos, bytes, flags,
						&page, &fsdata);
		if (unlikely(status < 0))
			break;

		if (mapping_writably_mapped(mapping))
			flush_dcache_page(page);

		copied = iov_iter_copy_from_user_atomic(page, i, offset, bytes);
		flush_dcache_page(page);

		status = a_ops->write_end(file, mapping, pos, bytes, copied,
						page, fsdata);
---
</pre>
</font>
</pre>

<h3><a name="iops">inode ops</a></h3>
<pre>
<font size="2">
<li> lookup (just for inode of dir)
<pre>
walk_component
  -> lookup_fast
    -> __d_lookup // lookup the dentry in dentry_hashtable
  -> lookup_slow  // if lookup_fast fails
    -> __lookup_slow
	  -> inode->i_op->lookup(parent_inode, dentry, flags); // dentry include the name of the child
</pre>
<li> 
</font>
</pre>



<h2><a name="fs_misc">fs misc</a></h2>
<hr style="height:5px;border:none;border-top:1px solid black;" />
<h3><a name="The_truth_of_page_lock">The truth of page lock</a></h3>
<p>
<font size="2">
block_read_full_page(), __block_write_full_page() and
__block_write_full_page()_all will create buffer_heads for page.

<pre>
create_page_buffers()
    -> create_empty_buffers()
        -> attach_page_buffers()
            -> SetPagePrivate()
            -> set_page_private()
</pre>


The truth of the page lock and bh lock<br/>

In the process of read operations
<pre>
do_generic_file_read()
    -> page_cache_sync_readahead() // if page is not present
        -> ondemand_readahead()
            -> ra_submit()
                -> __do_page_cache_readahead()
                    -> read_pages()
                        -> mapping->a_ops->readpages()
                           ext4_mpage_readpages()
                            -> add_to_page_cache_lru()
                                -> __set_page_locked() // page is locked ------> Here
                            -> block_read_full_page() // if page has buffers
                                -> lock_buffer() // the buffer_head is locked -----> Here
                                -> mark_buffer_async_read()
                                    //bh->b_end_io = end_buffer_async_read
                                -> submit_bh()
    -> !PageUptodate() && !trylock_page() , go to page_not_up_to_date
    -> lock_page_killable()
    -> if PageUptodate(), unlock_page() and goto page_ok
</pre>
Where the page and buffer_head are unlocked ?
<pre>
static void end_buffer_async_read(struct buffer_head *bh, int uptodate)
{
    unsigned long flags;
    struct buffer_head *first;
    struct buffer_head *tmp;
    struct page *page;
    int page_uptodate = 1;

    BUG_ON(!buffer_async_read(bh));

    page = bh->b_page;
    if (uptodate) {
        set_buffer_uptodate(bh);
    } else {
        clear_buffer_uptodate(bh);
        buffer_io_error(bh, ", async page read");
        SetPageError(page);
    }

    /*
     * Be _very_ careful from here on. Bad things can happen if
     * two buffer heads end IO at almost the same time and both
     * decide that the page is now completely done.
     */
    first = page_buffers(page);
    local_irq_save(flags);
    bit_spin_lock(BH_Uptodate_Lock, &first->b_state);
    clear_buffer_async_read(bh);
    unlock_buffer(bh);
    tmp = bh;
    do {
        if (!buffer_uptodate(tmp))
            page_uptodate = 0;
        if (buffer_async_read(tmp)) {
            BUG_ON(!buffer_locked(tmp)); //This could prove that the buffer_head is locked during the read process
            goto still_busy;
        }
        tmp = tmp->b_this_page;
    } while (tmp != bh);
    bit_spin_unlock(BH_Uptodate_Lock, &first->b_state);
    local_irq_restore(flags);

    /*
     * If none of the buffers had errors and they are all
     * uptodate then we can set the page uptodate.
     */
    if (page_uptodate && !PageError(page))
        SetPageUptodate(page);
    unlock_page(page);
    return;

still_busy:
    bit_spin_unlock(BH_Uptodate_Lock, &first->b_state);
    local_irq_restore(flags);
    return;
}
</pre>
If all the buffer in that page are uptodate, the page will be set uptodate and
unlocked.<br/>


<strong> We could see that the page and buffer_head both are locked during the process
of read operations. The lock ensure the page exclusive because the device need 
write data into the page through DMA.</strong>

What's about the write operations ?<br/>

The write process is divided into two parts.<br/>

1> write the user data into page cache
<pre>
generic_perform_write()
    -> a_ops->write_begin()
       ext4_write_begin()
        -> grab_cache_page_write_begin()
            -> pagecache_get_page()
                -> find_get_entry()
                If get 
                -> lock_page() //page is locked
                otherwise
                -> add_to_page_cache_lru()
                    -> __set_page_locked() //page is locked'
        -> unlock_page()
        -> ext4_journal_start() // About why does unlock_page() before the
    ext4_journal_start(), please refer to the comment in ext4_write_begin()
        -> lock_page() // the page is relocked
        -> <font color="red">wait_for_stable_page()</font>

    -> iov_iter_copy_from_user_atomic()

    -> a_ops->write_begin()
        -> block_write_end()
            -> __block_commit_write()
                -> set_buffer_uptodate()
                -> mark_buffer_dirty()
                -> SetPageUptodate() // if no partial
        -> unlock_page() // page is unlocked
</pre>
<strong>The page lock will ensure the page exclusive from other operations when the
user data is being copied into it.</strong>

2> write back the dirty page 
<pre>
ext4_writepages()
    -> blk_start_plug()
    -> write_cache_pages() //Go here when in journal mode because this mode
    does not support delayed allocation. We use this branch to demonstrate the
    page and bh lock because I really didn't find the where does the lock_page
    locate.
        -> lock_page() -------> Here
        -> wait_on_page_writeback() when PageWriteback() // keep the write back atomic
        -> clear_page_dirty_for_io()
        -> __writepage()
            -> ext4_writepage()
                -> ext4_bio_write_page()
                    -> set_page_writeback() //very important
                    -> set_buffer_async_write()
                    -> io_submit_add_bh()
                        -> ext4_io_submit()
                        -> io_submit_init_bio()
                            // set bi_end_io = ext4_end_bio()
                    -> unlock_page() ----> Here
                    
    -> blk_finish_plug()


ext4_end_bio()
    -> ext4_finish_bio()
        -> clear_buffer_async_write()
        -> end_page_writeback() // !under_io
            -> test_clear_page_writeback()
            -> wake_up_page(page, PG_writeback);

</pre>
<strong>The page is not locked during the write operations. But the writeback flag is
set to ensure the atomicity of the operations on the page </strong>
</font>
</p>


</body>
</html>
