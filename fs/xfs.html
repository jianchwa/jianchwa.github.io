<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>XFS</title>
</head>
<body>
<div>
    <h1>XFS</h1> 
</div>
<p>
<font size="2">
<a href="#iext_BpTree">inode extent B+ tree</a>
<ul>
<li><a href="#iext_BpTree_lookup">lookup</a>
<li><a href="#iext_BpTree_write_delay">write delayed allocate</a>
</ul>
<a href="#per-ag_metadata_buffer">per-ag metadata buffer</a>
<ul>
<li><a href="#pag_metadata_buffer_lock">lock</a>
<li><a href="#pag_metadata_buffer_read_in">read in</a>
</ul>
<a href="#log">log</a>
<ul>
<li><a href="#log_format">format</a>
<li><a href="#checkpoint">checkpoint</a>
<li><a href="#log_in_core">in core log</a>
</ul>
</font>
</p>

<h3><a name="iext_BpTree">inode extent B+ tree</a></h3>

<h4><a name="iext_BpTree_lookup">lookup</a></h4>
<p>
<font size="2">
<pre>
enum {
    NODE_SIZE    = 256,
    KEYS_PER_NODE    = NODE_SIZE / (sizeof(uint64_t) + sizeof(void *)),
    RECS_PER_LEAF    = (NODE_SIZE - (2 * sizeof(struct xfs_iext_leaf *))) /
                sizeof(struct xfs_iext_rec),
};

On a 64-bit system, a node has two keys, a leaf has one recs.


           +--+--+--+--+
           |K0|K1|P0|P1|
           +--+--+--+--+
                  /   \
     +--+--+--+--+     +--+--+--+--+
     |K0|K1|P0|P1|     |K0|K1|P0|P1|
     +--+--+--+--+     +--+--+--+--+
           /     \
    +--+-+-+     +--+-+-+
    |R0|p|n| <-> |R0|p|n|
    +--+-+-+     +--+-+-+
            /
            | startoff    (offset in the file)
    record <  startblock  (disk logical block offset ?)
            | length
            | flags
            \

<font color="red">
Take the smallest value of the lower level node as the KEY.
</font>

// under XFS_ILOCK_SHARED
xfs_bmapi_read
  -> xfs_iread_extents // read in the extents from disk.
  -> xfs_iext_lookup_extent


</pre>

</font>
</p>


<h4><a name="iext_BpTree_write_delay">write delayed allocate</a></h4>
<p>
<font size="2">
<pre>



</pre>
</font>
</p>


<h3><a name="per-ag_metadata_buffer">per-ag metadata buffer</a></h3>
<p>
<font size="2">
<pre>


xfs_perag is maintained in a radix tree on xfs_mount->m_perag_tree

xfs_buf_find

pag->pag_buf_hash protected by pag->pag_buf_lock maintains the xfs_bufs

xfs_buf_t looks like a buffer mechanism of xfs its own.

xfs_buf_t -> xfs_buf_map [block num, size]
          -> b_pages[]   associated pages    (_xfs_buf_alloc)
          -> b_addr       virtual address in kernel (_xfs_buf_map_pages)

</pre>
</font>
</p>

<h4><a name="pag_metadata_buffer_lock">lock</a></h4>
<p>
<font size="2">
<B>Per xfs_buf_t semaphore</B>
<pre>
xfs_buf_get_maps
  -> xfs_buf_find
    if find
    -> xfs_buf_trylock
      -> down_trylock(&bp->b_sema)

   if no ?
  -> _xfs_buf_alloc
    -> sema_init(&bp->b_sema, 0); <font color="blue">/* held, no waiters */</font>
       XB_SET_OWNER(bp);
   if present


</pre>
<B>per-ag buf hash spin_lock</B>
<pre>
Refer to xfs_buf_find
</pre>

</font>
</p>


<h4><a name="pag_metadata_buffer_read_in">read in</a></h4>
<p>
<font size="2">
<pre>
xfs_buf_read_map
  -> xfs_buf_get_maps
  -> if !bp->b_flags & XBF_DONE
     _xfs_buf_read
       -> xfs_buf_submit_wait
         -> _xfs_buf_ioapply
         ---
<font color="red">
    /* we only use the buffer cache for meta-data */
</font>
    op_flags |= REQ_META;

    /*
     * Walk all the vectors issuing IO on them. Set up the initial offset
     * into the buffer and the desired IO size before we start -
     * _xfs_buf_ioapply_vec() will modify them appropriately for each
     * subsequent call.
     */
    offset = bp->b_offset;
    size = BBTOB(bp->b_io_length);
    blk_start_plug(&plug);
    for (i = 0; i < bp->b_map_count; i++) {
        xfs_buf_ioapply_map(bp, i, &offset, &size, op, op_flags);
        if (bp->b_error)
            break;
        if (size <= 0)
            break;    /* all done */
    }
    blk_finish_plug(&plug);
         ---
How to convert a xfs_buf_t to bio ?
xfs_buf_ioapply_map
---
    sector_t    sector =  bp->b_maps[map].bm_bn;
    ...
    size = min_t(int, BBTOB(bp->b_maps[map].bm_len), *count);
    ...
next_chunk:
    atomic_inc(&bp->b_io_remaining);
    nr_pages = min(total_nr_pages, BIO_MAX_PAGES);

    bio = bio_alloc(GFP_NOIO, nr_pages);
    bio_set_dev(bio, bp->b_target->bt_bdev);
    bio->bi_iter.bi_sector = sector;
    bio->bi_end_io = xfs_buf_bio_end_io;
    bio->bi_private = bp;
    bio_set_op_attrs(bio, op, op_flags);

    for (; size && nr_pages; nr_pages--, page_index++) {
        int    rbytes, nbytes = PAGE_SIZE - offset;

        if (nbytes > size)
            nbytes = size;

        rbytes = bio_add_page(bio, bp->b_pages[page_index], nbytes,
                      offset);
        if (rbytes < nbytes)
            break;

        offset = 0;
        sector += BTOBB(nbytes);
        size -= nbytes;
        total_nr_pages--;
    }

    if (likely(bio->bi_iter.bi_size)) {
        if (xfs_buf_is_vmapped(bp)) {
            flush_kernel_vmap_range(bp->b_addr,
                        xfs_buf_vmap_len(bp));
        }
        submit_bio(bio);
        if (size)
            goto next_chunk;
    }
---
</pre>
</font>
</p>



<h3><a name="log">log</a></h3>
<p>
<font size="2">
<pre>
<B>AIL</B>
Active Item List, a LSN-sorted double linked list.
Items are inserted into this list during log buffer IO completion, after which
they are <U>unpind</U> and can be written to disk.
</pre>
<B>CIL</B>
<pre>
Commited Item List, this list tracks log items that have been commited and have
formatted memory buffer attached to them. It tracks objects in transaction
commit order.
</pre>
</font>
</p>

<h4><a name="log_format">format</a></h4>
<p>
<font size="2">
<pre>
xfs_trans_commit
  -> __xfs_trans_commit
    -> xfs_log_commit_cil
     -> xlog_cil_insert_items // down_read cil->xc_ctx_lock
       -> xlog_cil_insert_format_items
         -> iterate xfs_trans->t_items 
            lip->li_ops->iop_format// if xfs_log_item is set XFS_LI_DIRTY

            xfs_inode_item_format.

            Why is there no lock hold here ? [1]
       -> require cil->xc_cil_lock
          iterate tp->t_items and mve the dirty one to cil->xc_cil
          <font color="red">Note:
          list_move_tail(&lip->li_cil, &cil->xc_cil);
          </font>

[1]
For example:
  xfs_fs_commit_blocks
  ---
    <font color="red">
    xfs_ilock(ip, XFS_ILOCK_EXCL);
    </font>
    xfs_trans_ijoin(tp, ip, XFS_ILOCK_EXCL);
    xfs_trans_log_inode(tp, ip, XFS_ILOG_CORE);

    xfs_setattr_time(ip, iattr);
    if (update_isize) {
        i_size_write(inode, iattr->ia_size);
        ip->i_d.di_size = iattr->ia_size;
    }

    xfs_trans_set_sync(tp);
    error = xfs_trans_commit(tp);
  ---
Where to free the XFS_ILOCK_EXCL ?
xfs_trans_commit
  -> __xfs_trans_commit
    -> xfs_trans_free_items
      -> iterate the tp->t_items
         detach item
               clear_bit(XFS_LI_DIRTY, &lip->li_flags);
            list_del_init(&lip->li_trans);
         unlock item
            lip->li_ops->iop_unlock
            xfs_inode_item_unlock
              <font color="red">
              -> xfs_iunlock(ip, iip->ili_lock_flags)
              </font>
    clear_bit(XFS_LI_DIRTY, &lip->li_flags);
    list_del_init(&lip->li_trans);

<B>So we can know, when we do log formating, the inode is locked with XFS_ILOCK_EXCL</B>
The formatted buffer contains all the changes on the log item. This enable us to
relog the item in memory and write it out asynchronously without needing to
relock the object that was modified at the time it gets written into log.

To achieve this, we also need a one-off xfs_log_vec every time. Only then, we
could do the formating and pushing to log separately.

xfs_trans_commit
  -> __xfs_trans_commit
    -> xfs_log_commit_cil
      -> xlog_cil_alloc_shadow_bufs // allocate shadow buffer outside of
                                       cil->xc_ctx_lock to avoid deadlock when memory is low.
                                       more details, please refer to comment of
                                       xlog_cil_alloc_shadow_bufs.

      -> down_read(&cil->xc_ctx_lock)
      -> xlog_cil_insert_items
        -> xlog_cil_insert_format_items
          ---
    list_for_each_entry(lip, &tp->t_items, li_trans) {
        ...
        if (lip->li_lv && shadow->lv_size <= lip->li_lv->lv_size) {
            ...
        } else {
            <font color="blue">
                if lip->li_lv is NULL, use the shadow one.
                actually, <U>before commit this lv, the lip->li_lv will be taken</U>
                <U>down and cleared to NULL by xlog_cil_push under</U>
                <U>cil->xc_ctx_lock.</U>
            </font>
            /**/
            /* switch to shadow buffer! */
            lv = shadow;
            lv->lv_item = lip;
            ...
        }
        ...
        lip->li_ops->iop_format(lip, lv);
    }
          ---
xlog_cil_push
  ---
  ...
    down_write(&cil->xc_ctx_lock);
  ...
    while (!list_empty(&cil->xc_cil)) {
        struct xfs_log_item    *item;

        item = list_first_entry(&cil->xc_cil,
                    struct xfs_log_item, li_cil);
        list_del_init(&item->li_cil);
        if (!ctx->lv_chain)
            ctx->lv_chain = item->li_lv;
        else
            lv->lv_next = item->li_lv;
        lv = item->li_lv;  // <font color="blue">take down the active lv</font>
        item->li_lv = NULL; 
        num_iovecs += lv->lv_niovecs;
    }
  ---

</pre>
</font>
</p>

<h4><a name="checkpoint">checkpoint</a></h4>
<p>
<font size="2">
checkpoint of the log.
<ul>
<li> lock CIL flush
<li> Chain log vectors and buffers together
<li> Remove items from CIL
<li> unlock CIL flush
<li> write log vectors into log
<li> sequence commit records
<li> attach checkpoint context to log buffer
</ul>
<pre>
xlog_cil_push
    
    <font color="blue">//down_write cil->xc_ctx_lock: </font>
    
    list_add(&ctx->committing, &cil->xc_committing) //under cil->xc_push_lock
       
    CIL Head
       |
       V
    Log Item <-> log vector 1    -> memory buffer
       |                -> vector array
       V
    Log Item <-> log vector 2    -> memory buffer
       |                -> vector array
       V
    ......
       |
       V
    Log Item <-> log vector N-1    -> memory buffer
       |                -> vector array
       V
    Log Item <-> log vector N    -> memory buffer
                    -> vector array

    <font color="blue">
     - take the item down from the cil->xc_cil list.
     - take the active lv down from the item->li_lv.
    </font>

    And after the flush the CIL head is empty, and the checkpoint context log
    vector list would look like:

    Checkpoint Context
       |
       V
    log vector 1    -> memory buffer
       |        -> vector array
       |        -> Log Item
       V
    log vector 2    -> memory buffer
       |        -> vector array
       |        -> Log Item
       V
    ......
       |
       V
    log vector N-1    -> memory buffer
       |        -> vector array
       |        -> Log Item
       V
    log vector N    -> memory buffer
            -> vector array
            -> Log Item


    new_ctx->sequence = ctx->sequence + 1;
    new_ctx->cil = cil;
    cil->xc_ctx = new_ctx;

    
    spin_lock(&cil->xc_push_lock);
    cil->xc_current_sequence = new_ctx->sequence;
    spin_unlock(&cil->xc_push_lock);

    <font color="blue">//up_write cil->xc_ctx_lock</font>

    
    <U>xlog_write</U>
    // write log vectors into log


<font color="blue">
    // There could be multiple context running concurrently here.
    // IOW, the checkpoints could be written out of order.
    // <U>But the commit record must be in order strictly.</U>
</font>
    spin_lock(&cil->xc_push_lock);
    list_for_each_entry(new_ctx, &cil->xc_committing, committing) {
        /*
         * Higher sequences will wait for this one so skip them.
         * Don't wait for our own sequence, either.
         */
        if (new_ctx->sequence >= ctx->sequence)
            continue;
        if (!new_ctx->commit_lsn) {
            xlog_wait(&cil->xc_commit_wait, &cil->xc_push_lock);
            goto restart;
        }
    }
    spin_unlock(&cil->xc_push_lock);


<font color="blue">
    // commit the record
</font>
    xfs_log_done
      -> xlog_commit_record
        -> xlog_write // XLOG_REG_TYPE_COMMIT
    
<font color="blue">
    // attach checkpoint context to commit record log buffer
</font>
    ctx->log_cb.cb_func = xlog_cil_committed;
    ctx->log_cb.cb_arg = ctx;
    error = xfs_log_notify(commit_iclog, &ctx->log_cb);

<font color="blue">
    /*
     * now the checkpoint commit is complete and we've attached the
     * callbacks to the iclog we can assign the commit LSN to the context
     * and wake up anyone who is waiting for the commit to complete.
     */
</font>
    spin_lock(&cil->xc_push_lock);
    ctx->commit_lsn = commit_lsn;
    wake_up_all(&cil->xc_commit_wait);
    spin_unlock(&cil->xc_push_lock);


<font color="blue">
    // The commit maybe pushed to disk here.
</font>
    return xfs_log_release_iclog(log->l_mp, commit_iclog);
</pre>

<B>How to ensure the log lvs has been on disk when commit is pushed ?</B>
<pre>
ITOW, the log IOs maybe in a different iclog from the commit record.
 - the iclog on which the log IOs are written maybe synced to disk after the one
   on which commit record is written.
 - the IOs in block layer even storage device could be disordered.
Needn't we to consider the disordering above ?

There are two points here:
1. the log callback mechanism can ensure when the xlog_cil_committed is invoked,
   the previous log IOs must have been completed. (callbacks of different iclog
   must be invoked in order of lsn)
2. do we really need to ensure log IOs has been on disk when do commit record ?
   every iclog has a crc calculated by xlog_cksum.
   it could help us to recognize the corrupted log.

</pre>

</font>
</p>

<h4><a name="log_in_core">in core log</a></h4>
<p>
<font size="2">
<pre>

The iclog is maintained as a ring in log->l_iclog.
<pre>
                           ---
                         /     \
         log->l_iclog-> |       |
                         \     /
                           ---
</pre>
<B>Get an available iclog space</B><br/>
we will try to get an available iclog from the log->l_iclog ring.<br/>
If it is not an active one, this means the log space has been exhausted and has
to wait.<br/>
See xlog_state_get_iclog_space
<pre>
---
    // under log->l_icloglock
    iclog = log->l_iclog;
    if (iclog->ic_state != XLOG_STATE_ACTIVE) {
        XFS_STATS_INC(log->l_mp, xs_log_noiclogs);

        /* Wait for log writes to have flushed */
        xlog_wait(&log->l_flush_wait, &log->l_icloglock);
        goto restart;
    }
---
</pre>
Note: <U>xlog_state_get_iclog_space not only return an available iclog, but reserve space in it.</U>
<pre>
---
<font color="blue">
   // under log->l_icloglock
   // Has confirmed that this iclog has at least 2 * sizeof (xlog_op_header_t) available space.
</font>
   if (len <= iclog->ic_size - iclog->ic_offset) {
<font color="blue">
   // if the available space in this iclog is enough to carry, reserve the space.
</font>
      *continued_write = 0;
      iclog->ic_offset += len; // reserve our requested space
   } else {
<font color="blue">
   // otherwise, invoke xlog_state_switch_iclogs to switch this iclog from ACTIVE to WANT_SYNC.
   // then, noone could get it again. We have hold a reference of it, so it will not be resynced.
</font>
      *continued_write = 1;
      xlog_state_switch_iclogs(log, iclog, iclog->ic_size);
   }
---
   when continued_write is true, the iclog->ic_offset will be modified by
   xlog_write_copy_finish
     -> xlog_state_finish_copy

   then xlog_state_release_iclog will be invoked and iclog will be synced.
</pre>
So we know, <B>one iclog could be used by multiple users.</B>

<B>iclog block number</B><br/>
The iclogs' bp are allocated in xlog_alloc_log via xfs_buf_get_uncached.
So there is no <U>block number of IO</U> there. Where to get it ?
<pre>
xlog_state_get_iclog_space
---
   atomic_inc(&iclog->ic_refcnt);   /* prevents sync */
   log_offset = iclog->ic_offset;
<font color="blueâ€œ>
    // ic_offset is the current number of bytes written to in this iclog.
    // if ic_offset is zero, that says this is the first writting on this iclog.
</font>
   if (log_offset == 0) {
      ticket->t_curr_res -= log->l_iclog_hsize;
      xlog_tic_add_region(ticket,
                log->l_iclog_hsize,
                XLOG_REG_TYPE_LRHEADER);
      head->h_cycle = cpu_to_be32(log->l_curr_cycle);
      head->h_lsn = cpu_to_be64(
         xlog_assign_lsn(log->l_curr_cycle, <font color="red">log->l_curr_block</font>));
      ASSERT(log->l_curr_block >= 0);
   }
---
</pre>
The l_curr_block is turned in xlog_state_switch_iclogs
<pre>
---
   log->l_curr_block += BTOBB(eventual_size)+BTOBB(log->l_iclog_hsize);
   ...
   if (log->l_curr_block >= log->l_logBBsize) {
      log->l_curr_block -= log->l_logBBsize;
      ASSERT(log->l_curr_block >= 0);
      smp_wmb();
      log->l_curr_cycle++;
      if (log->l_curr_cycle == XLOG_HEADER_MAGIC_NUM)
         log->l_curr_cycle++;
   }
---
</pre>
Finally, the block number will be filled into the bp in xlog_sync with  XFS_BUF_SET_ADDR
<br/>

<B>sync a iclog</B><br/>
Every time, when we want to flush out an in core log, its state will be switched
to WANT_SYNC. At the same time, <font color="red"><U>the iclog ring will be turned.</U></font>
<pre>
xlog_state_switch_iclogs 
---
    log->l_iclog = iclog->ic_next;
---
</pre>
Then
<ul>
<li> no one will write on this iclog again.
<li> when the reference count of the iclog reaches zero, it will be synced to disk.
</ul>

When will we want to sync a iclog ?
<ul>
<li> when the current iclog doesn't have enough space
<pre>
    xlog_state_get_iclog_space
     -> xlog_state_switch_iclogs

    xlog_write
     -> xlog_write_copy_finish
       -> xlog_state_want_sync
        -> xlog_state_switch_iclogs
</pre>
<li> force the log to be synced
<pre>
    xfs_log_force
</pre>
</ul>


An iclog will be synced to disk when its reference count is zero.
The reference is released by xlog_state_release_iclog
<pre>
---
    if (!atomic_dec_and_lock(&iclog->ic_refcnt, &log->l_icloglock))
        return 0;
        ...
    if (iclog->ic_state == XLOG_STATE_WANT_SYNC) {
        /* update tail before writing to iclog */
        xfs_lsn_t tail_lsn = xlog_assign_tail_lsn(log->l_mp);
        sync++;
        iclog->ic_state = <font color="red">XLOG_STATE_SYNCING</font>;
        iclog->ic_header.h_tail_lsn = cpu_to_be64(tail_lsn);
        xlog_verify_tail_lsn(log, iclog, tail_lsn);
        /* cycle incremented when incrementing curr_block */
    }
    spin_unlock(&log->l_icloglock);
    ...
    if (sync)
        return <font color="red">xlog_sync(log, iclog);</font>
---
</pre>
<font color="blue">
One question:
    Whether we indeed need to hold the log->l_icloglock for every iclog ?
</font>

<br/>
<br/>
<B>Submit the iclog to disk</B>
<pre>
xlog_state_release_iclog
  -> xlog_sync
    -> bp = iclog->ic_bp;
      XFS_BUF_SET_ADDR(bp, BLOCK_LSN(be64_to_cpu(iclog->ic_header.h_lsn)));
      XFS_BUF_SET_ADDR(bp, XFS_BUF_ADDR(bp) + log->l_logBBstart);
    -> bp->b_flags |= (XBF_ASYNC | XBF_SYNCIO | XBF_WRITE | <font color="red">XBF_FUA</font>);
      -> xlog_bdstrat
        -> xfs_buf_lock(bp)
          -> xfs_buf_submit
</pre>
<br/>
<B>completion path</B>
<pre>
xfs_buf_bio_end_io
  -> xfs_buf_ioend_async
    -> queue work xfs_buf_ioend_work
xfs_buf_ioend_work
  -> xfs_buf_ioend
    -> bp->b_iodone
       xlog_iodone
         -> xlog_state_done_syncing
           -> set iclog->ic_state to <font color="red">XLOG_STATE_DONE_SYNC</font>  //under log->l_icloglock (a global one ?)
           -> xlog_state_do_callback
         -> xfs_buf_unlock
</pre>
<br/>
<B>do callbacks</B><br/>
<U>The callbacks must be performed in order of lsn</U><br/>
The callbacks here not only include invoke callbacks, but also
xlog_state_clean_log.
<ul>
<li> invoke callbacks linked in iclog->ic_callback
<li> xlog_state_clean_log
<pre>
      - iclog->ic_state = XLOG_STATE_ACTIVE
      - iclog->ic_offset = 0
      - ic_header.h_lsn = 0
</pre>
</ul>
Following code segment ensure the order
<pre>
xlog_state_do_callback
---
            if (!(iclog->ic_state & XLOG_STATE_IOERROR)) {
                ...
                lowest_lsn = xlog_get_lowest_lsn(log);
                if (lowest_lsn &&
                    XFS_LSN_CMP(lowest_lsn,
                        be64_to_cpu(iclog->ic_header.h_lsn)) < 0) {
                    iclog = iclog->ic_next;
                    continue;
                }
---
</pre>
</font>
</p>

</body>
</html>
