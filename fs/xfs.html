<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>XFS</title>
</head>
<body>
<div>
    <h1>XFS</h1> 
</div>
<p>
<font size="2">
<a href="#iext_BpTree">inode extent B+ tree</a>
<ul>
<li><a href="#iext_BpTree_lookup">lookup</a>
<li><a href="#iext_BpTree_write_delay">write delayed allocate</a>
</ul>
<a href="#per-ag_metadata_buffer">per-ag metadata buffer</a>
<ul>
<li><a href="#pag_metadata_buffer_lock">lock</a>
<li><a href="#pag_metadata_buffer_read_in">read in</a>
<li><a href="#xfs_buf_lock">xfs buf lock</a>
</ul>
<a href="#log">log</a>
<ul>
<li><a href="#log_formating">log formating</a>
<li><a href="#checkpoint">checkpoint</a>
<li><a href="#log_in_core">in core log</a>
<ul>
<li><a href="#iclog">iclog</a>
<li><a href="#iclog_ring_and_physical_log_space">iclog ring and physical log space</a>
<li><a href="#iclog_format">iclog format</a>
</ul>
<li><a href="#flush_to_disk">flush to disk</a>
<li><a href="#LSN">LSN</a>
<ul>
<li> <a href="#order_of_callbacks">order of callbacks</a>
<li> <a href="#order_of_AIL">order of AIL</a>
</ul>
<li><a href="#log_space">log space</a>
<ul>
<li><a href="#left_log_space">lef log space</a>
<li><a href="#log_space_accounting">log space accounting</a>
<li><a href="#log_space_reserve">log space reserve</a>
</ul>
<li><a href="#relog">relog</a>
<li><a href="#intent_log">intent log</a>
<ul>
<li><a href="#deferred_operation">deferred operations</a>
<li><a href="#intent_log_recover">intent log recover</a>
</ul>
<li><a href="#log_miscs">miscs</a>
</ul>
<a href="#AG_free_space_management">AG free space management</a>
<ul>
<li><a href="#AG_Free_Space_B+trees">AG Free Space B+trees</a>
<li><a href="#xfs_alloc_ag_vextent_size">xfs_alloc_ag_vextent_size</a>
<li><a href="#xfs_alloc_fixup_trees">xfs_alloc_fixup_trees</a>
<li><a href="#AG_free_list">AG free list</a>
<li><a href="#Busy_extent">Busy extent</a>
</ul>
<a href="#BT">B Tree</a>
<ul>
<li><a href="#xfs_btree_ops">xfs_btree_ops</a>
<li><a href="#xfs_btree_lookup">xfs_btree_lookup</a>
</ul>
<a href="#reflink">reflink</a><br/>

</font>
</p>

<h3><a name="iext_BpTree">inode extent B+ tree</a></h3>

<h4><a name="iext_BpTree_lookup">lookup</a></h4>
<p>
<font size="2">
<pre>
enum {
    NODE_SIZE    = 256,
    KEYS_PER_NODE    = NODE_SIZE / (sizeof(uint64_t) + sizeof(void *)),
    RECS_PER_LEAF    = (NODE_SIZE - (2 * sizeof(struct xfs_iext_leaf *))) /
                sizeof(struct xfs_iext_rec),
};

On a 64-bit system, a node has two keys, a leaf has one recs.

           0   2048
           +--+--+--+--+
           |K0|K1|P0|P1|
           +--+--+--+--+
     0   1024     /   \2048 3072
     +--+--+--+--+     +--+--+--+--+
     |K0|K1|P0|P1|     |K0|K1|P0|P1|
     +--+--+--+--+     +--+--+--+--+
    0      /     \1024
    +--+-+-+     +--+-+-+
    |R0|p|n| <-> |R0|p|n|
    +--+-+-+     +--+-+-+
            /
            | startoff    (offset in the file)
    record <  startblock  (disk logical block offset ?)
            | length
            | flags
            \

<font color="red">
Take the smallest value of the lower level node as the KEY.
The value here is file offset.
</font>

// under XFS_ILOCK_SHARED
xfs_bmapi_read
  -> xfs_iread_extents // read in the extents from disk.
  -> xfs_iext_lookup_extent


</pre>

</font>
</p>


<h4><a name="iext_BpTree_write_delay">write delayed allocate</a></h4>
<p>
<font size="2">
<pre>



</pre>
</font>
</p>


<h3><a name="per-ag_metadata_buffer">per-ag metadata buffer</a></h3>
<p>
<font size="2">
<pre>


xfs_perag is maintained in a radix tree on xfs_mount->m_perag_tree

xfs_buf_find

pag->pag_buf_hash protected by pag->pag_buf_lock maintains the xfs_bufs

xfs_buf_t looks like a buffer mechanism of xfs its own.

xfs_buf_t -> xfs_buf_map [block num, size]
          -> b_pages[]   associated pages    (_xfs_buf_alloc)
          -> b_addr       virtual address in kernel (_xfs_buf_map_pages)

</pre>
</font>
</p>

<h4><a name="pag_metadata_buffer_lock">lock</a></h4>
<p>
<font size="2">
<B>Per xfs_buf_t semaphore</B>
<pre>
xfs_buf_get_maps
  -> xfs_buf_find
    if find
    -> xfs_buf_trylock
      -> down_trylock(&bp->b_sema)

   if no ?
  -> _xfs_buf_alloc
    -> sema_init(&bp->b_sema, 0); <font color="blue">/* held, no waiters */</font>
       XB_SET_OWNER(bp);
   if present


</pre>
<B>per-ag buf hash spin_lock</B>
<pre>
Refer to xfs_buf_find
</pre>

</font>
</p>


<h4><a name="pag_metadata_buffer_read_in">read in</a></h4>
<p>
<font size="2">
<pre>
xfs_buf_read_map
  -> xfs_buf_get_maps
  -> if !bp->b_flags & XBF_DONE
     _xfs_buf_read
       -> xfs_buf_submit_wait
         -> _xfs_buf_ioapply
         ---
<font color="red">
    /* we only use the buffer cache for meta-data */
</font>
    op_flags |= REQ_META;

    /*
     * Walk all the vectors issuing IO on them. Set up the initial offset
     * into the buffer and the desired IO size before we start -
     * _xfs_buf_ioapply_vec() will modify them appropriately for each
     * subsequent call.
     */
    offset = bp->b_offset;
    size = BBTOB(bp->b_io_length);
    blk_start_plug(&plug);
    for (i = 0; i < bp->b_map_count; i++) {
        xfs_buf_ioapply_map(bp, i, &offset, &size, op, op_flags);
        if (bp->b_error)
            break;
        if (size <= 0)
            break;    /* all done */
    }
    blk_finish_plug(&plug);
         ---
How to convert a xfs_buf_t to bio ?
xfs_buf_ioapply_map
---
    sector_t    sector =  bp->b_maps[map].bm_bn;
    ...
    size = min_t(int, BBTOB(bp->b_maps[map].bm_len), *count);
    ...
next_chunk:
    atomic_inc(&bp->b_io_remaining);
    nr_pages = min(total_nr_pages, BIO_MAX_PAGES);

    bio = bio_alloc(GFP_NOIO, nr_pages);
    bio_set_dev(bio, bp->b_target->bt_bdev);
    bio->bi_iter.bi_sector = sector;
    bio->bi_end_io = xfs_buf_bio_end_io;
    bio->bi_private = bp;
    bio_set_op_attrs(bio, op, op_flags);

    for (; size && nr_pages; nr_pages--, page_index++) {
        int    rbytes, nbytes = PAGE_SIZE - offset;

        if (nbytes > size)
            nbytes = size;

        rbytes = bio_add_page(bio, bp->b_pages[page_index], nbytes,
                      offset);
        if (rbytes < nbytes)
            break;

        offset = 0;
        sector += BTOBB(nbytes);
        size -= nbytes;
        total_nr_pages--;
    }

    if (likely(bio->bi_iter.bi_size)) {
        if (xfs_buf_is_vmapped(bp)) {
            flush_kernel_vmap_range(bp->b_addr,
                        xfs_buf_vmap_len(bp));
        }
        submit_bio(bio);
        if (size)
            goto next_chunk;
    }
---
</pre>
</font>
</p>

<h3><a name="xfs_buf_lock">xfs buf lock</a></h3>
<p>
<font size="2">
Lock and unlock scenarios.
<ul>
<li> xfs_buf_delwri_submit_buffers
<pre>
LOCK:

xfs_buf_delwri_submit_buffers
---
    blk_start_plug(&plug);
    list_for_each_entry_safe(bp, n, buffer_list, b_list) {
        if (!wait_list) {
            if (xfs_buf_ispinned(bp)) {
                pinned++;
                continue;
            }
            if (!xfs_buf_trylock(bp))
                continue;
        } else {
            xfs_buf_lock(bp);
        }
        ...
        bp->b_flags &= ~(_XBF_DELWRI_Q | XBF_WRITE_FAIL);
        bp->b_flags |= XBF_WRITE;
        if (wait_list) {
            bp->b_flags &= ~XBF_ASYNC;
            list_move_tail(&bp->b_list, wait_list);
        } else {
            bp->b_flags |= XBF_ASYNC;
            list_del_init(&bp->b_list);
        }
        __xfs_buf_submit(bp, false);
    }
    blk_finish_plug(&plug);
---

UNLOCK:

For the XBF_ASYNC case, it will be unlocked by
xfs_buf_ioend
---
    if (bp->b_iodone)
        (*(bp->b_iodone))(bp);
    else if (bp->b_flags & XBF_ASYNC)
        <font color="red">xfs_buf_relse(bp);</font>
    else
        complete(&bp->b_iowait);
---
For the !XBF_ASYNC case,
xfs_buf_delwri_submit
---
    xfs_buf_delwri_submit_buffers(buffer_list, &wait_list);

    /* Wait for IO to complete. */
    while (!list_empty(&wait_list)) {
        bp = list_first_entry(&wait_list, struct xfs_buf, b_list);

        list_del_init(&bp->b_list);
        ...
        error2 = xfs_buf_iowait(bp);
        <font color="red">xfs_buf_relse(bp);</font>
    }

---
</pre>

<li> xfs_buf_find
<pre>
LOCK:

xfs_buf_find
---
    if (!xfs_buf_trylock(bp)) {
        if (flags & XBF_TRYLOCK) {
            xfs_buf_rele(bp);
            XFS_STATS_INC(btp->bt_mount, xb_busy_locked);
            return -EAGAIN;
        }
        xfs_buf_lock(bp);
    }
---

xfs_buf_find  xfs_buf_incore
              xfs_buf_get_map xfs_buf_read_map xfs_buf_readahead_map  [1]
                                               xfs_buf_read           xlog_recover_buffer_pass2 [2]
                                                                      xlog_recover_inode_pass2  [3]
                                                                      xfs_readlink_bmap_ilocked [4]

                                               xfs_trans_read_buf_map[5]

                              xfs_buf_get      xfs_update_secondary_sbs   [6]
                                               xfs_attr_rmtval_set itself [7]
                                               xfs_trans_get_buf_map [8]
UNLOCK:

[1] This is just for readahead.
xfs_buf_readahead_map
---
    if (bdi_read_congested(target->bt_bdev->bd_bdi))
        return;

    xfs_buf_read_map(target, map, nmaps,
             XBF_TRYLOCK|XBF_ASYNC|XBF_READ_AHEAD, ops);
---
Due to the XBF_ASYNC, it will be unlocked by xfs_buf_ioend

[2] unlocked by xlog_recover_buffer_pass2 itself.
[2] unlocked by xlog_recover_inode_pass2 itself.
    For both two case [2] and [3], the bps will be submitted by xfs_buf_delwri_submit.
    Look at xlog_do_recovery_pass and xlog_recover_process_ophdr.

[4] unlocked by xfs_readlink_bmap_ilocked itself.

[5] This is for update transactions.
    The bp will be added to the transaction.
    xfs_trans_read_buf_map
    ---
    bp = xfs_buf_read_map(target, map, nmaps, flags, ops);
    ...
    if (tp) {
        _xfs_trans_bjoin(tp, bp, 1);
    }
    ---
    So finally, the bp will be unlocked by xfs_buf_item_unlock
    xfs_buf_item_unlock
    ---
    bool            hold = !!(bip->bli_flags & XFS_BLI_HOLD);
    ...
    if (!hold)
        xfs_buf_relse(bp);
    ---
    
<font size="1">
    The XFS_BLI_HOLD is not a common case.
    One of the usage of XFS_BLI_HOLD.
    xfs_dir_ialloc
    ---
<font color="blue">
        /*
         * Normally, xfs_trans_commit releases all the locks.
         * We call bhold to hang on to the ialloc_context across
         * the commit.  Holding this buffer prevents any other
         * processes from doing any allocations in this
         * allocation group.
         */
</font>
        xfs_trans_bhold(tp, ialloc_context);
    ---
</font>

    The xfs_buf_item_unlock is invoked in:
    xfs_log_commit_cil
      -> xlog_cil_insert_items
      -> xfs_trans_free_items
        -> lip->li_ops->iop_unlock
           xfs_buf_item_unlock

    In addition, the bp could be unlocked by xfs_trans_brelse.
    xfs_trans_brelse
    ---
    /*
     * If the buffer is dirty within this transaction, we can't
     * release it until we commit.
     */
    if (test_bit(XFS_LI_DIRTY, &bip->bli_item.li_flags))
		return;
    ...
    xfs_buf_relse(bp);
	---
 [6] unlocked by xfs_update_secondary_sbs itself.
 [7] unlocked by xfs_attr_rmtval_set itself
 [8] same with case [5]
</pre>
</ul>

</font>
</p>



<h3><a name="log">log</a></h3>
<p>
<font size="2">

<B>AIL</B>
<pre>
Active Item List, a LSN-sorted double linked list.
Items are inserted into this list during log buffer IO completion, after which
they are <U>unpind</U> and can be written to disk.
</pre>
<B>CIL</B>
<pre>
Commited Item List, this list tracks log items that have been commited and have
formatted memory buffer attached to them. It tracks objects in transaction
commit order.
</pre>
</font>
</p>
<pre>


XFS
          changes on inode                 
                 | 
-----------------^------------------------------------------------------------------------------------------------------------
XFS log          | format[1]
                 v
           lvs on lip->li_lv                xlog_cil_push                                     xfsaild_push_item
           lip is pined                     take lvs down from cil->xc_cil                    li_ops->push
           under XFS_IOLOCK_EXCL            under down_write cil->xc_ctx_lock                 push the modifications to bp
                 down_read cil->xc_ctx_lock                                                   under XFS_IOLOCK_EXCL
           insert lip on cil->xc_cil        write lvs to iclog[2] .---> xlog_cil_committed[3]       ||
           under spin cil->xc_cil_lock      write commit record  /      insert to ail->ail_head     ||  xfs_iflush_done
                                                   ||           /       unpin                       ||   ^ xfs_iunlock
                                                   \/          /                                    \/   |
--------------------------------------------------------------------------------------------------------------------------------
XFS buf                                      submit to disk                                    submit to disk
--------------------------------------------------------------------------------------------------------------------------------


[1]: xfs_log_commit_cil
[2]: xlog_write
[3]: commit record's IO completion callback, see xlog_cil_push

lv   : xfs_log_vec
lip  : xfs_log_item
cil  : xfs_cil
bp   : xfs_buf
</pre>
<h4><a name="log_formating">log formating</a></h4>
<p>
<font size="2">
Why do we need the log format ?
<pre>
XFS logging is a combination of logical and physical logging. Some objects,
such as inodes and dquots, are logged in logical format where the details
logged are made up of the changes to in-core structures rather than on-disk
structures. Other objects - typically buffers - have their physical changes
logged. The reason for these differences is to reduce the amount of log space
required for objects that are frequently logged. Some parts of inodes are more
frequently logged than others, and inodes are typically more frequently logged
than any other object (except maybe the superblock buffer) so keeping the
amount of metadata logged low is of prime importance.
</pre>
<pre>
xfs_trans_commit
  -> __xfs_trans_commit
    -> xfs_log_commit_cil
     -> xlog_cil_insert_items // down_read cil->xc_ctx_lock
       -> xlog_cil_insert_format_items
         -> iterate xfs_trans->t_items 
            lip->li_ops->iop_format// if xfs_log_item is set XFS_LI_DIRTY

            xfs_inode_item_format.

            Why is there no lock hold here ? [1]
       -> require cil->xc_cil_lock
          iterate tp->t_items and mve the dirty one to cil->xc_cil
          <font color="red">Note:
          list_move_tail(&lip->li_cil, &cil->xc_cil);
          </font>

[1]
For example:
  xfs_fs_commit_blocks
  ---
    <font color="red">
    xfs_ilock(ip, XFS_ILOCK_EXCL);
    </font>
    xfs_trans_ijoin(tp, ip, XFS_ILOCK_EXCL);
    xfs_trans_log_inode(tp, ip, XFS_ILOG_CORE);

    xfs_setattr_time(ip, iattr);
    if (update_isize) {
        i_size_write(inode, iattr->ia_size);
        ip->i_d.di_size = iattr->ia_size;
    }

    xfs_trans_set_sync(tp);
    error = xfs_trans_commit(tp);
  ---
</pre>
Where to free the XFS_ILOCK_EXCL ?
<pre>
xfs_trans_commit
  -> __xfs_trans_commit
    -> xfs_trans_free_items
      -> iterate the tp->t_items
         detach item
               clear_bit(XFS_LI_DIRTY, &lip->li_flags);
            list_del_init(&lip->li_trans);
         unlock item
            lip->li_ops->iop_unlock
            xfs_inode_item_unlock
              <font color="red">
              -> xfs_iunlock(ip, iip->ili_lock_flags)
              </font>
    clear_bit(XFS_LI_DIRTY, &lip->li_flags);
    list_del_init(&lip->li_trans);
</pre>

<B>So we can know, when we do log formating, the inode is locked with XFS_ILOCK_EXCL</B><br/>
The formatted buffer contains all the changes on the log item. This enable us to
relog the item in memory and write it out asynchronously without needing to
relock the object that was modified at the time it gets written into log.
<br/>
To achieve this, we also need a one-off xfs_log_vec every time. Only then, we
could do the formating and pushing to log separately.
<pre>
xfs_trans_commit
  -> __xfs_trans_commit
    -> xfs_log_commit_cil
      -> xlog_cil_alloc_shadow_bufs // allocate shadow buffer outside of
                                       cil->xc_ctx_lock to avoid deadlock when memory is low.
                                       more details, please refer to comment of
                                       xlog_cil_alloc_shadow_bufs.

      -> down_read(&cil->xc_ctx_lock)
      -> xlog_cil_insert_items
        -> xlog_cil_insert_format_items
          ---
    list_for_each_entry(lip, &tp->t_items, li_trans) {
        ...
        if (lip->li_lv && shadow->lv_size <= lip->li_lv->lv_size) {
            ...
        } else {
            /* switch to shadow buffer! */
            lv = shadow;
            lv->lv_item = lip;
            ...
        }
        ...
        lip->li_ops->iop_format(lip, lv);
    }
          ---
</pre>

Regarding to the lip->li_lv, we have 3 cases as following:
<ul>
<li> lip->li_lv is NULL, initial sate
<li> lip->li_lv is NULL, the lv is taken away by xlog_cil_push
<pre>
xlog_cil_push
  ---
    <font color="red">down_write(&cil->xc_ctx_lock);</font>
    while (!list_empty(&cil->xc_cil)) {
        struct xfs_log_item    *item;

        item = list_first_entry(&cil->xc_cil,
                    struct xfs_log_item, li_cil);
        list_del_init(&item->li_cil);
        if (!ctx->lv_chain)
            ctx->lv_chain = item->li_lv;
        else
            lv->lv_next = item->li_lv;
        lv = item->li_lv;  // <font color="blue">take down the active lv</font>
        item->li_lv = NULL; 
        num_iovecs += lv->lv_niovecs;
    }
  ---
</pre>
<li> lip->li_lv is not NULL, there has been log in it, <U>overwrite it</U>
</ul>
</font>
</p>

<h4><a name="checkpoint">checkpoint</a></h4>
<p>
<font size="2">
checkpoint of the log.
<ul>
<li> lock CIL flush
<li> Chain log vectors and buffers together
<li> Remove items from CIL
<li> unlock CIL flush
<li> write log vectors into log
<li> sequence commit records
<li> attach checkpoint context to log buffer
</ul>
<pre>
xlog_cil_push
    
    <font color="blue">//down_write cil->xc_ctx_lock: </font>
    
    list_add(&ctx->committing, &cil->xc_committing) //under cil->xc_push_lock
       
    CIL Head
       |
       V
    Log Item <-> log vector 1    -> memory buffer
       |                -> vector array
       V
    Log Item <-> log vector 2    -> memory buffer
       |                -> vector array
       V
    ......
       |
       V
    Log Item <-> log vector N-1    -> memory buffer
       |                -> vector array
       V
    Log Item <-> log vector N    -> memory buffer
                    -> vector array

    <font color="blue">
     - take the item down from the cil->xc_cil list.
     - take the active lv down from the item->li_lv.
    </font>

    And after the flush the CIL head is empty, and the checkpoint context log
    vector list would look like:

    Checkpoint Context
       |
       V
    log vector 1    -> memory buffer
       |        -> vector array
       |        -> Log Item
       V
    log vector 2    -> memory buffer
       |        -> vector array
       |        -> Log Item
       V
    ......
       |
       V
    log vector N-1    -> memory buffer
       |        -> vector array
       |        -> Log Item
       V
    log vector N    -> memory buffer
            -> vector array
            -> Log Item


    new_ctx->sequence = ctx->sequence + 1;
    new_ctx->cil = cil;
    cil->xc_ctx = new_ctx;

    
    spin_lock(&cil->xc_push_lock);
    cil->xc_current_sequence = new_ctx->sequence;
    spin_unlock(&cil->xc_push_lock);

    <font color="blue">//up_write cil->xc_ctx_lock</font>

    
    <U>xlog_write</U>
    // write log vectors into log


<font color="blue">
    // There could be multiple context running concurrently here.
    // IOW, the checkpoints could be written out of order.
    // <U>But the commit record must be in order strictly.</U>
</font>
    spin_lock(&cil->xc_push_lock);
    list_for_each_entry(new_ctx, &cil->xc_committing, committing) {
        /*
         * Higher sequences will wait for this one so skip them.
         * Don't wait for our own sequence, either.
         */
        if (new_ctx->sequence >= ctx->sequence)
            continue;
        if (!new_ctx->commit_lsn) {
            xlog_wait(&cil->xc_commit_wait, &cil->xc_push_lock);
            goto restart;
        }
    }
    spin_unlock(&cil->xc_push_lock);


<font color="blue">
    // commit the record
</font>
    xfs_log_done
      -> xlog_commit_record
        -> xlog_write // XLOG_REG_TYPE_COMMIT
    
<font color="blue">
    // attach checkpoint context to commit record log buffer
</font>
    ctx->log_cb.cb_func = xlog_cil_committed;
    ctx->log_cb.cb_arg = ctx;
    error = xfs_log_notify(commit_iclog, &ctx->log_cb);

<font color="blue">
    /*
     * now the checkpoint commit is complete and we've attached the
     * callbacks to the iclog we can assign the commit LSN to the context
     * and wake up anyone who is waiting for the commit to complete.
     */
</font>
    spin_lock(&cil->xc_push_lock);
    ctx->commit_lsn = commit_lsn;
    wake_up_all(&cil->xc_commit_wait);
    spin_unlock(&cil->xc_push_lock);


<font color="blue">
    // The commit maybe pushed to disk here.
</font>
    return xfs_log_release_iclog(log->l_mp, commit_iclog);
</pre>

<B>How to ensure the log lvs has been on disk when commit is pushed ?</B>
<pre>
ITOW, the log IOs maybe in a different iclog from the commit record.
 - the iclog on which the log IOs are written maybe synced to disk after the one
   on which commit record is written.
 - the IOs in block layer even storage device could be disordered.
Needn't we to consider the disordering above ?

There are two points here:
1. the log callback mechanism can ensure when the xlog_cil_committed is invoked,
   the previous log IOs must have been completed. (callbacks of different iclog
   must be invoked in order of lsn)
2. do we really need to ensure log IOs has been on disk when do commit record ?
   every iclog has a crc calculated by xlog_cksum.
   it could help us to recognize the corrupted log.

</pre>

</font>
</p>

<h4><a name="log_in_core">in core log</a></h4>
<hr style="height:5px;border:none;border-top:2px solid black;" />
<h4><a name="iclog">iclog</a></h4>
<p>
<font size="2">

The iclog is maintained as a ring in log->l_iclog.
<pre>
                           ---
                         /     \
         log->l_iclog-> |       |
                         \     /
                           ---
</pre>
<B>Get an available iclog space</B><br/>
we will try to get an available iclog from the log->l_iclog ring.<br/>
If it is not an active one, this means the log space has been exhausted and has
to wait.<br/>
See xlog_state_get_iclog_space
<pre>
---
    // under log->l_icloglock
    iclog = log->l_iclog;
    if (iclog->ic_state != XLOG_STATE_ACTIVE) {
        XFS_STATS_INC(log->l_mp, xs_log_noiclogs);

        /* Wait for log writes to have flushed */
        xlog_wait(&log->l_flush_wait, &log->l_icloglock);
        goto restart;
    }
---
</pre>
Note: <U>xlog_state_get_iclog_space not only return an available iclog, but reserve space in it.</U>
<pre>
---
<font color="blue">
   // under log->l_icloglock
   // Has confirmed that this iclog has at least 2 * sizeof (xlog_op_header_t) available space.
</font>
   if (len <= iclog->ic_size - iclog->ic_offset) {
<font color="blue">
   // if the available space in this iclog is enough to carry, reserve the space.
</font>
      *continued_write = 0;
      iclog->ic_offset += len; // reserve our requested space
   } else {
<font color="blue">
   // otherwise, invoke xlog_state_switch_iclogs to switch this iclog from ACTIVE to WANT_SYNC.
   // then, noone could get it again. We have hold a reference of it, so it will not be resynced.
</font>
      *continued_write = 1;
      xlog_state_switch_iclogs(log, iclog, iclog->ic_size);
   }
---
   when continued_write is true, the iclog->ic_offset will be modified by
   xlog_write_copy_finish
     -> xlog_state_finish_copy

   then xlog_state_release_iclog will be invoked and iclog will be synced.
</pre>
So we know, <B>one iclog could be used by multiple users.</B>

<B>iclog block number</B><br/>
The iclogs' bp are allocated in xlog_alloc_log via xfs_buf_get_uncached.
So there is no <U>block number of IO</U> there. Where to get it ?
<pre>
xlog_state_get_iclog_space
---
   atomic_inc(&iclog->ic_refcnt);   /* prevents sync */
   log_offset = iclog->ic_offset;
<font color="blue">
    // ic_offset is the current number of bytes written to in this iclog.
    // if ic_offset is zero, that says this is the first writting on this iclog.
</font>
   if (log_offset == 0) {
      ticket->t_curr_res -= log->l_iclog_hsize;
      xlog_tic_add_region(ticket,
                log->l_iclog_hsize,
                XLOG_REG_TYPE_LRHEADER);
      head->h_cycle = cpu_to_be32(log->l_curr_cycle);
      head->h_lsn = cpu_to_be64(
         xlog_assign_lsn(log->l_curr_cycle, <font color="red">log->l_curr_block</font>));
      ASSERT(log->l_curr_block >= 0);
   }
---
</pre>
The l_curr_block is turned in xlog_state_switch_iclogs
<pre>
---
   log->l_curr_block += BTOBB(eventual_size)+BTOBB(log->l_iclog_hsize);
   ...
   if (log->l_curr_block >= log->l_logBBsize) {
      log->l_curr_block -= log->l_logBBsize;
      ASSERT(log->l_curr_block >= 0);
      smp_wmb();
      log->l_curr_cycle++;
      if (log->l_curr_cycle == XLOG_HEADER_MAGIC_NUM)
         log->l_curr_cycle++;
   }
---
</pre>
Finally, the block number will be filled into the bp in xlog_sync with  XFS_BUF_SET_ADDR
<br/>

<B>sync a iclog</B><br/>
Every time, when we want to flush out an in core log, its state will be switched
to WANT_SYNC. At the same time, <font color="red"><U>the iclog ring will be turned.</U></font>
<pre>
xlog_state_switch_iclogs 
---
    log->l_iclog = iclog->ic_next;
---
</pre>
Then
<ul>
<li> no one will write on this iclog again.
<li> when the reference count of the iclog reaches zero, it will be synced to disk.
</ul>

When will we want to sync a iclog ?
<ul>
<li> when the current iclog doesn't have enough space
<pre>
    xlog_state_get_iclog_space
     -> xlog_state_switch_iclogs

    xlog_write
     -> xlog_write_copy_finish
       -> xlog_state_want_sync
        -> xlog_state_switch_iclogs
</pre>
<li> force the log to be synced
<pre>
    xfs_log_force
</pre>
</ul>


An iclog will be synced to disk when its reference count is zero.
The reference is released by xlog_state_release_iclog
<pre>
---
    if (!atomic_dec_and_lock(&iclog->ic_refcnt, &log->l_icloglock))
        return 0;
        ...
    if (iclog->ic_state == XLOG_STATE_WANT_SYNC) {
        /* update tail before writing to iclog */
        xfs_lsn_t tail_lsn = xlog_assign_tail_lsn(log->l_mp);
        sync++;
        iclog->ic_state = <font color="red">XLOG_STATE_SYNCING</font>;
        iclog->ic_header.h_tail_lsn = cpu_to_be64(tail_lsn);
        xlog_verify_tail_lsn(log, iclog, tail_lsn);
        /* cycle incremented when incrementing curr_block */
    }
    spin_unlock(&log->l_icloglock);
    ...
    if (sync)
        return <font color="red">xlog_sync(log, iclog);</font>
---
</pre>
<font color="blue">
One question:
    Whether we indeed need to hold the log->l_icloglock for every iclog ?
</font>

<br/>
<br/>
<B>Submit the iclog to disk</B>
<pre>
xlog_state_release_iclog
  -> xlog_sync
    -> bp = iclog->ic_bp;
      XFS_BUF_SET_ADDR(bp, BLOCK_LSN(be64_to_cpu(iclog->ic_header.h_lsn)));
      XFS_BUF_SET_ADDR(bp, XFS_BUF_ADDR(bp) + log->l_logBBstart);
    -> bp->b_flags |= (XBF_ASYNC | XBF_SYNCIO | XBF_WRITE | <font color="red">XBF_FUA</font>);
      -> xlog_bdstrat
        -> xfs_buf_lock(bp)
          -> xfs_buf_submit
</pre>
<br/>
<B>completion path</B>
<pre>
xfs_buf_bio_end_io
  -> xfs_buf_ioend_async
    -> queue work xfs_buf_ioend_work
xfs_buf_ioend_work
  -> xfs_buf_ioend
    -> bp->b_iodone
       xlog_iodone
         -> xlog_state_done_syncing
           -> set iclog->ic_state to <font color="red">XLOG_STATE_DONE_SYNC</font>  //under log->l_icloglock (a global one ?)
           -> xlog_state_do_callback
         -> xfs_buf_unlock
</pre>
<br/>
<B>do callbacks</B><br/>
<U>The callbacks must be performed in order of lsn</U><br/>
The callbacks here not only include invoke callbacks, but also
xlog_state_clean_log.
<ul>
<li> invoke callbacks linked in iclog->ic_callback
<li> xlog_state_clean_log
<pre>
      - iclog->ic_state = XLOG_STATE_ACTIVE
      - iclog->ic_offset = 0
      - ic_header.h_lsn = 0
</pre>
</ul>
Following code segment ensure the order
<pre>
xlog_state_do_callback
---
            if (!(iclog->ic_state & XLOG_STATE_IOERROR)) {
                ...
                lowest_lsn = xlog_get_lowest_lsn(log);
                if (lowest_lsn &&
                    XFS_LSN_CMP(lowest_lsn,
                        be64_to_cpu(iclog->ic_header.h_lsn)) < 0) {
                    iclog = iclog->ic_next;
                    continue;
                }
---
</pre>
</font>
</p>

<h4><a name="iclog_ring_and_physical_log_space">iclog ring and physical log space</a></h4>
<p>
<font size="2">
iclog ring:
<ul>
<li> l_iclog_bufs
<pre>
xlog_get_iclog_buffer_size
---
    if (mp->m_logbufs <= 0)
        log->l_iclog_bufs = XLOG_MAX_ICLOGS;
    else
        log->l_iclog_bufs = mp->m_logbufs;
---

#define XLOG_MAX_ICLOGS        8
</pre>
<li> l_iclog_size
<pre>
xlog_get_iclog_buffer_size
---
    if (mp->m_logbsize > 0) {
        size = log->l_iclog_size = mp->m_logbsize;
        log->l_iclog_size_log = 0;
        ...
    }
    /* All machines use 32kB buffers by default. */
    log->l_iclog_size = XLOG_BIG_RECORD_BSIZE;
---
</pre>
</ul>

The biggest total iclog size is 8 * 256K == 2M<br/>

But this is not the real physical log space.<br/>

The real physical log space is:
<ul>
<li> l_logBBstart
<li> l_logBBsize
<pre>
Both of them are from:
xfs_mountfs
---
    error = xfs_log_mount(mp, mp->m_logdev_targp,
                  XFS_FSB_TO_DADDR(mp, sbp->sb_logstart),
                  XFS_FSB_TO_BB(mp, sbp->sb_logblocks));
---
</pre>
</ul>

The max and min size of the log space is
<pre>
   XFS_MAX_LOG_BLOCKS blocks  (1024 * 1024)
or XFS_MAX_LOG_BYTES          (2 * 1024 * 1024 * 1024)

   XFS_MIN_LOG_BLOCKS blocks  (512)
   XFS_MIN_LOG_BYTES          (10 * 1024 * 1024)
</pre>


iclog ring:
<pre>
                            <--.
                           ---  \
                       O /     \
         log->l_iclog-> |       |
                         \ N   /
                      \    ---
               rotate  '-->

rotate: it is done by xlog_state_switch_iclogs
O     : older
N     : newer

</pre>
Where to allocate the iclog space ?
<pre>
xlog_write
  -> xlog_state_get_iclog_space
    -> get an XLOG_STATE_ACTIVE iclog and reserve space in it.
    ---
        //under log->l_icloglock
        iclog->ic_offset += len;
    ---
    if the log->l_iclog is not ACTIVE, wait for it.
      -> xlog_wait on log->l_flush_wait

Where to free the iclog space ?

xlog_iodone
  -> xlog_state_done_syncing
    -> xlog_state_do_callback
      -> xlog_state_clean_log
    -> wake_up_all(&log->l_flush_wait);

</pre>

In conclusion, at one moment, there only be <U>2M log IO in-flight</U> at most.

<pre>
      .-->
     /   ---                     
       /     \                   
      | iclog |                  
       \     /                   
         ---  /                  
          <--'                   
|------------------------------------------------|
           physical log space

The iclog ring will roll forward in xlog_state_switch_iclogs.
</pre>

<pre>
XFS transaction subsystem is that most transactions are asynchronous. That is,
they don't commit to disk until either a log buffer is filled (a log buffer can
hold multiple transactions) or a synchronous operation forces the log buffers
holding the transactions to disk. This means that XFS is doing aggregation of
transactions in memory - batching them, if you like - to minimise the impact
of the log IO on transaction throughput.

The limitation on asynchronous transaction throughput is the number and size of
log buffers made available by the log manager. By default there are 8 log
buffers available and the size of each is 32kB - the size can be increased up
to 256kB by use of a mount option.

Effectively, <U>this gives us the maximum bound of outstanding metadata changes
that can be made to the filesystem at any point in time - if all the log
buffers are full and under IO, then no more transactions can be committed until
the current batch completes.</U> It is now common for a single current CPU core to
be to able to issue enough transactions to keep the log buffers full and under
IO permanently. Hence the XFS journalling subsystem can be considered to be IO
bound.
</pre>


[Life Cycle of a iclog]

<pre>
XLOG_STATE_WANT_SYNC
  xlog_state_switch_iclogs
  ---
    iclog->ic_state = XLOG_STATE_WANT_SYNC;
    ...
    /* roll log?: ic_offset changed later */
    log->l_curr_block += BTOBB(eventual_size)+BTOBB(log->l_iclog_hsize);
    ...
    if (log->l_curr_block >= log->l_logBBsize) {
        log->l_curr_block -= log->l_logBBsize;
        smp_wmb();
        log->l_curr_cycle++;
    }
    log->l_iclog = iclog->ic_next;
  ---

XLOG_STATE_SYNCING
  xlog_state_release_iclog
    -> if XLOG_STATE_WANT_SYNC
       set SYNCING
    -> xlog_sync


XLOG_STATE_DONE_SYNC
  xlog_state_done_syncing


XLOG_STATE_DO_CALLBACK
  xlog_state_do_callback

XLOG_STATE_CALLBACK
  xlog_state_do_callback
  //has been the lowest lsn and ready to do callback.

XLOG_STATE_DIRTY
  xlog_state_do_callback
  //after complete the callbacks

XLOG_STATE_ACTIVE
  xlog_state_do_callback
    -> xlog_state_clean_log
</pre>


<h4><a name="iclog_format">iclog format</a></h4>

Every iclog has its header.
<pre>

[1][2]    [3]  
|--Header--|----Payload---|

[1] iclog->ic_data = bp->b_addr
[2] ic_data is xlog_in_core_2_t
  typedef union xlog_in_core2 {
         xlog_rec_header_t    hic_header;
      xlog_rec_ext_header_t    hic_xheader;
      char            hic_sector[XLOG_HEADER_SIZE];
  } xlog_in_core_2_t;
  #define ic_header    ic_data->hic_header
[3] iclog->ic_datap = (char *)iclog->ic_data + log->l_iclog_hsize;

(xlog_alloc_log)

<U>The first u32 of each log sector must contain the cycle number.</U>
Since log item buffers are formatted without regard to this requirement,
the original contents of the first four bytes of each sector in the log
are copied into the corresponding element of this array. After that, the
first four bytes of those sectors are stamped with the cycle number. This
process is reversed at recovery time. If there are more sectors in this
log record than there are slots in this array, the cycle data continues
for as many sectors are needed; each sector is formatted as type xlog_rec_ext_header.

xlog_sync
  -> xlog_pack_data
  ---
    cycle_lsn = CYCLE_LSN_DISK(iclog->ic_header.h_lsn);

    dp = iclog->ic_datap;
    for (i = 0; i < BTOBB(size); i++) {
        if (i >= (XLOG_HEADER_CYCLE_SIZE / BBSIZE))
            break;
        iclog->ic_header.h_cycle_data[i] = *(__be32 *)dp;
        *(__be32 *)dp = cycle_lsn;
        dp += BBSIZE;
    }
  ---

<U>The cycle is started from 1</U> (xlog_alloc_log log->l_curr_cycle = 1)
</pre>

<pre>

       tail      head
       (c 200)   (c 200)
        |         |
        v         v
   |-----------------------------|
        \____ ____/
             v
     non-checkpointed log

non-checkpointed: the metadata associated with the log has not been flushed to
                  disk
<B>We should try to do recovery from the tail.</B>
This tail is recored in iclog header, xlog_rec_header_t.h_tail_lsn.

xlog_state_release_iclog
---
    if (iclog->ic_state == XLOG_STATE_WANT_SYNC) {
        /* update tail before writing to iclog */
        xfs_lsn_t tail_lsn = <font color="red">xlog_assign_tail_lsn(log->l_mp)</font>;
        sync++;
        iclog->ic_state = XLOG_STATE_SYNCING;
        iclog->ic_header.h_tail_lsn = cpu_to_be64(tail_lsn);
        xlog_verify_tail_lsn(log, iclog, tail_lsn);
        /* cycle incremented when incrementing curr_block */
    }
---

This is a perfect position to update the tail lsn of the log.
<U>It could ensure the tail of the log is updated when we overwrite the
checkpointed aera of the log.</U>(the h_tail_lsn is on the iclog which will be flushed
to log.)


Unlike the jbd2, which has a superblock for journal, xfs record the tail of the
log in every LR. It could reduce some extra IO out of LR.
But the trouble is how to find out the tail when do recovery ?

Even if we could search the log to find out xlog_rec_header_t's magic
0xFEEDbabe, there could be multiple xlog_rec_header_t structures to be found and
they may has different tail lsn. And what we need is the last one.
How to find out it ?

Consider we have a cycle number recored at head of every sector.

                 End
                  |
                  v
|200|200|200|200|200|199|199|199|199|

                                 End
                                  |
                                  v
|200|200|200|200|200|200|200|200|200|


What we do is to search backward from the End to find out the xlog_rec_header_t's
magic.

xlog_find_tail
  -> xlog_find_head
  -> xlog_rseek_logrec_hdr

</pre>




<h4><a name="flush_to_disk">flush to disk</a></h4>
<p>
<font size="2">
After commit record IO is completed, the log item associated with the lvs in the
ctx will be added to AIL, at the same time, the log items are unpind.
<pre>
xlog_cil_committed (completion callback of commit record)
  -> xfs_trans_committed_bulk //ctx->lv_chain (xlog_cil_push put the lvs on it)
    -> iterate the lvs
      -> lip->li_ops->iop_committed()
      -> xfs_log_item_batch_insert
        -> insert the log items onto xfs_ail->ail_head sorted by lsn.
        -> lip->li_ops->iop_unpin()
<font color="blue">
Why do we need this pin/unpin here ?
An log item cannot be flushed to disk before the log is completed.
pin/unpin pair is to achieve this.
The log item is pinned in
xlog_cil_insert_format_items
  -> lip->li_ops->iop_format()
  -> xfs_cil_prepare_item
    -> lv->lv_item->li_ops->iop_pin()

This is done under item lock.
For inode, it is xfs_ilock, for example:
xfs_ilock(ip, XFS_ILOCK_EXCL);
xfs_trans_ijoin(tp, ip, XFS_ILOCK_EXCL);

Then unlocked by xfs_inode_item_unlock

In addition, an log item could be reloged during it is on CIL even AIL.
Then it will not be pushed to disk until all of the pin counter is cleared.
</font>
</pre>
When an log item is on AIL, it could be pushed to disk now.
<pre>
xfsaild
  -> xfsaild_push
    -> xfsaild_push_item
      -> lip->li_ops->iop_push
         xfs_inode_item_push
           -> if xfs_ipincount(ip) > 0 //<font color="blue">it could be reloged.</font>
              return XFS_ITEM_PINNED
           -> xfs_ilock_nowait(ip, <font color="red">XFS_ILOCK_SHARED</font>)
           -> xfs_iflush
             -> xfs_imap_to_bp  // get the buffer containing the on-disk inode.
             -> xfs_iflush_int  // flush the dirty part of inode into on-disk inode.
                                // <font color="blue">the on-disk inode is on
                                // the xfs_buf of the inode</font>
               -> xfs_buf_attach_iodone //<font color="red">xfs_iflush_done</font>
           -> xfs_buf_delwri_queue
              insert the bp into <font color="red">xfs_ail->ail_buf_list</font>
    -> xfs_buf_delwri_submit_nowait
      -> xfs_buf_delwri_submit_buffers
      ---
    list_sort(NULL, buffer_list, xfs_buf_cmp);

    blk_start_plug(&plug);
    list_for_each_entry_safe(bp, n, buffer_list, b_list) {
        if (!wait_list) {
            if (xfs_buf_ispinned(bp)) {
                pinned++;
                continue;
            }
            if (!xfs_buf_trylock(bp))
                continue;
        } else {
            xfs_buf_lock(bp);
        }
        ...
        bp->b_flags &= ~(_XBF_DELWRI_Q | XBF_WRITE_FAIL);
        bp->b_flags |= <font color="red">XBF_WRITE | XBF_ASYNC</font>;
        if (wait_list) {
            xfs_buf_hold(bp);
            list_move_tail(&bp->b_list, wait_list);
        } else
            list_del_init(&bp->b_list);

        xfs_buf_submit(bp);
    }
    blk_finish_plug(&plug);
      ---
</pre>
When the IO is completed:
<pre>
xfs_buf_bio_end_io
  -> xfs_buf_ioend_async
    -> queue work xfs_buf_ioend_work
xfs_buf_ioend_work
  -> xfs_buf_ioend
    -> bp->b_iodone
       xfs_iflush_done // attached by xfs_iflush_int
       ---
    list_for_each_entry_safe(blip, n, &bp->b_li_list, li_bio_list) {
        if (lip->li_cb != xfs_iflush_done)
            continue;

        list_move_tail(&blip->li_bio_list, &tmp);
        ...
    }
    ...
    list_for_each_entry_safe(blip, n, &tmp, li_bio_list) {
        list_del_init(&blip->li_bio_list);
        iip = INODE_ITEM(blip);
        iip->ili_logged = 0;
        iip->ili_last_fields = 0;
        <font color="red">xfs_ifunlock(iip->ili_inode); </font>
    }
       ---
</pre>
</font>
</p>

<h4><a name="LSN">LSN</a></h4>
<p>
<font size="2">
Where does the <B>lsn</B> come from ?
<pre>
xlog_state_get_iclog_space
---
<font color="blue">
    // <U>Under log->l_icloglock</U>
    // On the 1st write to an iclog, figure out lsn. 
</font>
    if (log_offset == 0) {
        ticket->t_curr_res -= log->l_iclog_hsize;
        xlog_tic_add_region(ticket,
                    log->l_iclog_hsize,
                    XLOG_REG_TYPE_LRHEADER);
        head->h_cycle = cpu_to_be32(log->l_curr_cycle);
        head->h_lsn = cpu_to_be64(
            xlog_assign_lsn(log->l_curr_cycle, log->l_curr_block));
    }
---

log->l_curr_block and log->l_curr_cycle
xlog_state_switch_iclogs //<U>Under log->l_icloglock</U>
---
    log->l_curr_block += BTOBB(eventual_size)+BTOBB(log->l_iclog_hsize);
    ...
    if (log->l_curr_block >= log->l_logBBsize) {
        log->l_curr_block -= log->l_logBBsize;
        ASSERT(log->l_curr_block >= 0);
        smp_wmb();
        log->l_curr_cycle++;
        if (log->l_curr_cycle == XLOG_HEADER_MAGIC_NUM)
            log->l_curr_cycle++;
    }
---
</pre>

So we know the lsn is <B>(cycle << 32 | block )</B>
<pre>
Log space and cycle

             current
                |
                v
  |------------------------------|   cycle 200


    current  original
  - ->|         | - - - - - - - -
      v         v
  |------------------------------|   cycle 201

</pre>
</font>
</p>

<h4><a name="order_of_AIL">order of AIL</a></h4>
<p>
<font size="2">
A xfs_cil_ctx contains following information:
<ul>
<li> ctx->lv_chain  lv list
<li> ctx->start_lsn cycle block pair of the first iclog to which the lvs are written.
<pre>
xlog_cil_push
  -> xlog_write
    -> xlog_state_get_iclog_space
    ->     if (!*start_lsn)
            *start_lsn = be64_to_cpu(iclog->ic_header.h_lsn);
</pre>
</ul>
The ctx->start_lsn here will be used to insert the log items to AIL.
<pre>
xlog_cil_committed
  -> xfs_trans_committed_bulk(ctx->cil->xc_log->l_ailp, ctx->lv_chain,
                    <font color="red">ctx->start_lsn</font>, abort)
    -> xfs_trans_ail_update_bulk
    ---
        for (i = 0; i < nr_items; i++) {
            struct xfs_log_item *lip = log_items[i];
<font color="blue">
            <U>// if the log item has been on AIL, we may need to reposition it.
            // this is for the relogging case</U>.
</font>
            if (test_and_set_bit(XFS_LI_IN_AIL, &lip->li_flags)) {
                /* check if we really need to move the item */
                if (XFS_LSN_CMP(lsn, lip->li_lsn) <= 0)
                    continue;

                xfs_ail_delete(ailp, lip);
                if (mlip == lip)
                    mlip_changed = 1;
            }
<font color="blue">
            // a log item's lsn is set here.
</font>
            <B>lip->li_lsn = lsn;</B>
            list_add(&lip->li_ail, &tmp);
        }
<font color="blue">
        // Queue on the AIL list
</font>
        if (!list_empty(&tmp))
            xfs_ail_splice(ailp, cur, &tmp, lsn);
<font color="blue">
        // If the minimum lsn of the AIL list is changed, it indicates the log
        // space is moved forward, try to wake up the waiters.
</font>
        if (mlip_changed) {
            ...
            xfs_log_space_wake(ailp->ail_mount);
        }
    ---
</pre>

Question:
<pre>
The AIL is sorted by lsn. What's about the order of pushing ?
Look at the two helper interfaces in xfsaild_push:
 - xfs_trans_ail_cursor_first
 - xfs_trans_ail_cursor_next
And also the log->ail_last_pushed_lsn

Basically, the entries in AIL list is pushed by order of lsn.

The lsn should be monotonically increasing. But the pushing of log item on AIL
list may be disordered.
We have known that callback of record commit, xlog_cil_committed, is invoked by
the order of lsn. But please note, this is the lsn of the commit record instead
of the log lvs. The log lvs and commit record may have different lsn and there
maybe other one inserted between them.

Why do we need the ail_last_pushed_lsn ?
AIL list is sorted by lsn which combines log space block and cycle. It is not
the block number of the real IO under the log item....So it is meaningless to
push them continously.
</pre>
</font>
</p>


<h4><a name="log_space">log space</a></h4>
<hr style="height:5px;border:none;border-top:2px solid black;" />

<h5><a name="left_log_space">left log space</a></h5>
<p>
<font size="2">
<pre>
Log space

       tail      head
       (c 200)   (c 200)
        |         |
        v         v
   |-----------------------------|
        \____ ____/
             v
            used

                 head     tail
                 (c 201)  (c 200)
                  |        |
                  v        v
   |-----------------------------|
    \______ ______/        \__ __/
           v                  v
          used               used

<B>Both of the head and tail will be pushed forward.</B>
 - pushing Tail means free
 - pushing Head mean allocate
</pre>
<ul>
<li> Tail
<pre>
The <B>log->l_tail_lsn contains</B> the Tail of the log space.
It is essentially the minimum lsn on the AIL list.
Benifit from the sorting of the AIL, we could use the AIL to get the minimum lsn
easily through the xfs_ail_min.

Here is one of the hook to modify the log->l_tail_lsn.

xlog_state_release_iclog
  -> xlog_assign_tail_lsn // iclog->ic_state == XLOG_STATE_WANT_SYNC
    -> xlog_assign_tail_lsn
      -> xlog_assign_tail_lsn_locked
      ---
        lip = xfs_ail_min(mp->m_ail);
        if (lip)
            tail_lsn = lip->li_lsn;
        else
            tail_lsn = atomic64_read(&log->l_last_sync_lsn); // set by xlog_state_do_callback
        atomic64_set(&log->l_tail_lsn, tail_lsn);    
      ---

Note:
The tail is decided by log->l_last_sync_lsn or the AIL list.
What does this mean ?

log->l_last_sync_lsn is the updated xlog_state_do_callback
xlog_iodone
  -> xlog_state_done_syncing
    -> xlog_state_do_callback
It indicates the IO on the iclog is completed.
So it is reasonable to push the Tail.

Regarding to the AIL list, it means only when metadata is flushed to disk we
could release the log.

</pre>
<li> Head
<pre>
<B>log->l_reserve_head.grant</B> is the Head
xfs_log_reserve
  -> xlog_grant_head_check
    -> xlog_grant_head_wait // <font color="blue">xlog_space_left(log, &head->grant) < need_bytes)</font>
<font color="red"><B>Big defect here:</B></font>
<font color="blue">
The log space maybe changed between xlog_grant_head_check and
xlog_grant_add_space.
</font>
  -> xlog_grant_add_space
---
    int64_t    head_val = atomic64_read(head);
    int64_t new, old;

    do {
        int        tmp;
        int        cycle, space;

        xlog_crack_grant_head_val(head_val, &cycle, &space);

        tmp = log->l_logsize - space;
        if (tmp > bytes)
            space += bytes;
        else {
            space = bytes - tmp;
            cycle++;
        }

        old = head_val;
        new = xlog_assign_grant_head_val(cycle, space);
        head_val = atomic64_cmpxchg(head, old, new);
    } while (head_val != old);
}
---
</pre>
</ul>

</font>
</p>

<h5><a name="log_space_accounting">log space accounting</a></h5>
<p>
<font size="2">

<br/>
Log format
<pre>
Log space:

<lr-hdr><          reg         > <lr-hdr><          reg         > <lr-hdr><          reg         >

Log format of a transaction[0]:

      < start-oph >< oph >< trans-hdr >< oph >< reg1 >< oph >...< commit-oph >
      [1]                                                         [2]

[0]
   The transaction here is not the one in xfs_trans_commit, it is the ctx in xlog_cil_push.
   It could include the lvs of a couple of xfs_trans

   xfs_trans->t_items - li - li - li
                         \    \    \
                          lv   lv   lv

   ctx->lv_chain - lv -lv -lv

[1]
   xlog_cil_push
     -> xlog_write
       -> xlog_write_start_rec
          clear XLOG_TIC_INITED of ctx->ticket

[2]
   xlog_cil_push
     -> xfs_log_done //ctx->ticket->t_flags & XLOG_TIC_INITED == 0
       -> xlog_commit_record


</pre>
Actual log space accounting.
<ul>
<li> actual log
<pre>
diff_len and diff_iovecs from xlog_cil_insert_format_items
<U>diff_len returns the actual log space consumed by the lvs in the transaction</U><br/>
A log item's (lip->li_lv) could have 4 cases:

  [a] initial state
  [b] lv has been taken away by xlog_cil_push
  [c] lv is there, and its size is enough to carry the new logs
  [d] lv is there, and its size is not enough to carry the new logs

For the case a and b,

    diff_len = lv->lv_bytes

For the case c and d

    diff_len = lv->lv_bytes - old_lv->lv_bytes

    The original log is overwritten, so we could occupy the previous reserved
    log space.
</pre>
<li> xlog_op_header_t
<pre>
diff_iovecs from the xlog_cil_insert_format_items is used to account the size of
xlog_op_header_t (one xlog_op_header_t per iovec)
---
    xlog_cil_insert_format_items(log, tp, &len, &diff_iovecs);

    spin_lock(&cil->xc_cil_lock);

    /* account for space used by new iovec headers  */
    iovhdr_res = diff_iovecs * sizeof(xlog_op_header_t);
    len += iovhdr_res;
---
</pre>
<li> log record headers
<pre>
---
    /* do we need space for more log record headers? */

    iclog_space = log->l_iclog_size - log->l_iclog_hsize;
    if (len > 0 && <font color="red">(ctx->space_used / iclog_space !=
                (ctx->space_used + len) / iclog_space)) </font>{
        split_res = (len + iclog_space - 1) / iclog_space;
<font color="blue">
        /* need to take into account split region headers, too */
        XLOG_CONTINUE_TRANS ?
</font>
        split_res *= log->l_iclog_hsize + sizeof(struct xlog_op_header);
        ctx->ticket->t_unit_res += split_res;
        ctx->ticket->t_curr_res += split_res;
        tp->t_ticket->t_curr_res -= split_res;
        ASSERT(tp->t_ticket->t_curr_res >= len);
    }
---

<U>A ctx contains a couple of lvs, these lvs will be written to log one time
and all of lvs in a same ctx will have a same log commit record.</U>

So we could see, ctx->space_used is used here to calculate whether additional
log record header is needed.
</pre>
<li> Common part of the log
<pre>
The actual accounting is done in xlog_cil_push.
ctx->ticket will be used to account this.

It is allocated at:
  xlog_cil_ticket_alloc
    -> xlog_ticket_alloc // unit_bytes is zero, xfs_log_calc_unit_res will return the log space needed for common part
      -> xfs_log_calc_unit_res
    -> tic->t_curr_res = 0

It will first steal the reservation from the first xfs_trans inserted into this
ctx.
xlog_cil_insert_items
---
    if (ctx->ticket->t_curr_res == 0) {
        ctx_res = ctx->ticket->t_unit_res;
        ctx->ticket->t_curr_res = ctx_res;
        tp->t_ticket->t_curr_res -= ctx_res;
    }
---
Then account the actual log space in xlog_cil_push.
The left part will be release in xfs_log_done in
</pre>
</ul>
</font>
</p>

<h5><a name="log_space_reserve">log space reserve</a></h5>
<p>
<font size="2">

The log reservation steps:
<ul>
[1] reserve the maximum log space needed
<pre>
xfs_trans_reserve
  -> xfs_log_reserve
    -> xlog_ticket_alloc
      -> xfs_log_calc_unit_res // calculate the maximum log space needed
  -> xlog_grant_head_check
    -> xlog_ticket_reservation
      -> return tic->t_unit_res * tic->t_cnt
  -> xlog_grant_add_space l_reserve_head
  -> xlog_grant_add_space l_write_head
</pre>
[2] account the actual log space used<br/>
[3] release the redundant log space with xfs_log_done.
<pre>
There are two paths to do this:
  [a] When roll trans
      xlog_regrant_reserve_log_space
      ---
        if (ticket->t_cnt > 0)
            ticket->t_cnt--;

        xlog_grant_sub_space(log, &log->l_reserve_head.grant,
                        ticket->t_curr_res);
        xlog_grant_sub_space(log, &log->l_write_head.grant,
                        ticket->t_curr_res);
        ticket->t_curr_res = ticket->t_unit_res;
      ---
<font color="red">
        Just release tic->t_curr_res
</font>
  [b] When commit trans
      xlog_ungrant_log_space
      ---
        if (ticket->t_cnt > 0)
            ticket->t_cnt--;
    
        bytes = ticket->t_curr_res;
        if (ticket->t_cnt > 0) {
            ASSERT(ticket->t_flags & XLOG_TIC_PERM_RESERV);
<font color="red">            bytes += ticket->t_unit_res*ticket->t_cnt; </font>
        }

        xlog_grant_sub_space(log, &log->l_reserve_head.grant, bytes);
        xlog_grant_sub_space(log, &log->l_write_head.grant, bytes);
      ---
<font color="red">
        Not only release tic->t_curr_res, but also the left permanent
        reservation tic->t_unit_res * tic->t_cnt
</font>
</pre>    
</ul>
        
Let's look at some examples:
<pre>
            bash-1440  [005] ....   101.856925: xlog_ticket_alloc: input 254080 calc 267016 cnt 2
            bash-1440  [005] ....   101.857868: xlog_ungrant_log_space: curr 256644 unit 267016 cnt 1
            bash-1440  [005] ....   101.858341: xlog_ticket_alloc: input 326016 calc 340524 cnt 8
            bash-1440  [005] ....   101.858373: xlog_regrant_reserve_log_space: curr 340524 unit 340524 cnt 7
            bash-1440  [005] ....   101.858381: xlog_ungrant_log_space: curr 340524 unit 340524 cnt 6
     kworker/4:1-186   [004] ....   105.789587: xlog_ticket_alloc: input 0 calc 9268 cnt 1
     kworker/4:1-186   [004] ...1   105.789720: xlog_verify_iclog_lsn.isra.20: iclog b 120 c 1 tail b 120 c 1
     kworker/4:1-186   [004] ....   105.789730: xlog_ungrant_log_space: curr 8704 unit 9268 cnt 0
              cp-1546  [003] ....   110.256017: xlog_ticket_alloc: input 254080 calc 267016 cnt 2
              cp-1546  [003] ....   110.256160: xlog_ungrant_log_space: curr 256624 unit 267016 cnt 1
              cp-1546  [003] ....   110.256640: xlog_ticket_alloc: input 178936 calc 190824 cnt 8
              cp-1546  [003] ....   110.257872: xlog_ungrant_log_space: curr 190012 unit 190824 cnt 7
              cp-1546  [003] ....   110.258000: xlog_ticket_alloc: input 760 calc 10028 cnt 0
     kworker/6:1-68    [006] ....   110.258781: xlog_ungrant_log_space: curr 10028 unit 10028 cnt 0
              cp-1546  [003] ....   110.259266: xlog_ticket_alloc: input 5752 calc 15020 cnt 0
              cp-1546  [003] ....   110.259303: xlog_ungrant_log_space: curr 15020 unit 15020 cnt 0
              cp-1546  [003] ....   110.259326: xlog_ticket_alloc: input 178936 calc 190824 cnt 8
              cp-1546  [003] ....   110.259358: xlog_regrant_reserve_log_space: curr 190720 unit 190824 cnt 7
              cp-1546  [003] ....   110.259402: xlog_regrant_reserve_log_space: curr 190620 unit 190824 cnt 6
              cp-1546  [003] ....   110.259429: xlog_regrant_reserve_log_space: curr 190768 unit 190824 cnt 5
              cp-1546  [003] ....   110.259433: xlog_ungrant_log_space: curr 190824 unit 190824 cnt 4
              cp-1546  [003] ....   110.259441: xlog_ticket_alloc: input 5752 calc 15020 cnt 0
              cp-1546  [003] ....   110.259448: xlog_ungrant_log_space: curr 15020 unit 15020 cnt 0
              cp-1546  [003] ....   110.259448: xlog_ungrant_log_space: curr 15020 unit 15020 cnt 0
     kworker/4:1-186   [004] ....   136.508683: xlog_ticket_alloc: input 0 calc 9268 cnt 1
     kworker/4:1-186   [004] ...1   136.508693: xlog_verify_iclog_lsn.isra.20: iclog b 128 c 1 tail b 120 c 1
     kworker/4:1-186   [004] ....   136.508702: xlog_ungrant_log_space: curr 8704 unit 9268 cnt 0
<font color="red">
     kworker/4:2-370   [004] ....   197.948672: xlog_ticket_alloc: input 4224 calc 13492 cnt 1
     kworker/4:2-370   [004] ....   197.948727: xlog_ungrant_log_space: curr 3792 unit 13492 cnt 0
</font>
</pre>
Most of time, the reserved log space will be surplus.

</font>
</p>

<h4><a name="intent_log">intent log</a></h4>
<hr style="height:5px;border:none;border-top:2px solid black;" />
<p>
<font size="2">
Log Items
<pre>
    Transaction Headers
    Intent to Free an Extent                (EFI) \
    Completion of Intent to Free an Extent  (EFD)  |
    Reverse Mapping Updates Intent          (RUI)  |
    Completion of Reverse Mapping Updates   (RUD)  |
    Reference Count Updates Intent          (CUI)   > <font color="red">Itent Log</font>
    Completion of Reference Count Updates   (CUD)  |
    File Block Mapping Intent               (BUI)  |
    Completion of File Block Mapping Updates(BUD) /
    Inode Updates
    Inode Data Log Item
    Buffer Log Item
    Buffer Data Log Item
    Update Quota File
    Quota Update Data Log Item
    Disable Quota Log Item
    Inode Creation Log Item

+--------------+
|xfs defer ops |
+--------------+--------+
|       xfs log         |
+-----------------------+----------------+
|                xfs buf                 |
-----------------------------------------+
               submit_bio
</pre>
</font>
<p>


<h5><a name="deferred_operation">deferred operations</a></h5>
<p>
<font size="2">
<B>What is the deferred operation mechanism for ?</B>
<pre>
<U>The deferred operations in XFS is a kind of "intent logging mechanism".</U>
intent logging means the logs record operation intent.
If a failure occurs, then when the system is recovering, it can use the intent log to
detect what operations were still in process during the failure, and use the intent log
to help recover from the failure, usually by either undoing a partially completed operation,
or by redoing one that might need to be completed
</pre>

<pre>
                     XFS_DEFER_OPS_TYPE_REFCOUNT
                     |       XFS_DEFER_OPS_TYPE_RMAP,
                     |       |
xfs_trans->t_dfops - dfp0 - dfp1
                     |
                     |
                     dfp_work - ri0 - ri1 - ri2
                                type = XFS_REFCOUNT_INCREASE
                                startblock
                                blockcount
dfp  xfs_defer_pending
ri   xfs_refcount_intent
</pre>

<B>defer operations callbacks</B>
<pre>
All of the defer operation callbacks are stores in defer_op_types[]
</pre>
<pre>
Take refcount deferred_operation as example.
</pre>
<ul>
[1] create_intent
<pre>
xfs_refcount_update_create_intent
  -> xfs_cui_log_item
    -> xfs_log_item_init //<font color="blue">XFS_LI_CUI xfs_cui_item_ops</font>
  -> xfs_trans_add_item
</pre>
[2] diff_items
[3] log_item
<pre>
xfs_refcount_update_log_item
---
<font color="blue">
    //Fill up the xfs_phys_extent which will be written to intent log.
</font>
    ext = &cuip->cui_format.cui_extents[next_extent];
    ext->pe_startblock = refc->ri_startblock;
    ext->pe_len = refc->ri_blockcount;
    xfs_trans_set_refcount_flags(ext, refc->ri_type);
---
</pre>
[4] create_done
<pre>
xfs_refcount_update_create_done
  -> xfs_trans_get_cud
    -> xfs_cud_init
      -> xfs_log_item_init//<font color="blue">XFS_LI_CUD xfs_cud_item_ops</font>
  -> xfs_trans_add_item
</pre>
[5] finish_item
<pre>
xfs_refcount_update_finish_item
  -> xfs_trans_log_finish_refcount_update
    -> xfs_refcount_finish_one
<font color="blue">
    // Looks like do the real work here.
</font>
</pre>
[6] cleanup_fn
</ul>

</pre>
<B>Code path</B>
<pre>

xfs_refcount_increase_extent
  -> __xfs_refcount_add
    -> xfs_defer_add

xfs_defer_finish
  -> xfs_defer_finish_noroll
    -> xfs_defer_create_intents
<font color="blue">
        //Create intent log for the items linked in this transaction.
</font>
       ---
        list_for_each_entry(dfp, &tp->t_dfops, dfp_list) {
            dfp->dfp_intent = dfp->dfp_type-><font color="red">create_intent(tp,
                    dfp->dfp_count) <B>[1]</B></font>;
            list_sort(tp->t_mountp, &dfp->dfp_work,
                    dfp-><font color="red">dfp_type->diff_items <B>[2]</B></font>);
            list_for_each(li, &dfp->dfp_work)
                dfp-><font color="red">dfp_type->log_item(tp, dfp->dfp_intent, li) <B>[3]</B></font>;
        }
       ---
    -> xfs_defer_trans_roll <font color="blue"><U>//commit the logs to cil</U></font>
    ->
    ---
<font color="blue">
        <U>//Create an intent-done log</U>
</font>
        dfp->dfp_done = dfp-><font color="red">dfp_type->create_done <B>[4]</B></font>(*tp, dfp->dfp_intent,
                dfp->dfp_count);
        cleanup_fn = dfp->dfp_type->finish_cleanup;

        /* Finish the work items. */
        state = NULL;
        list_for_each_safe(li, n, &dfp->dfp_work) {
            list_del(li);
            dfp->dfp_count--;
            error = dfp-><font color="red">dfp_type->finish_item <B>[5]</B></font>(*tp, li,
                    dfp->dfp_done, &state);
            ...
            }
        if (cleanup_fn)
            <font color="red">cleanup_fn <B>[6]</B></font>(*tp, state, error);
    ---
</pre>



</font>
</p>
  

<h5><a name="intent_log_recover">intent log recover</a></h5>
<p>
<font size="2">
The basic principle is:<br/>
<ul>
<li> Both intent log item and log done are there, needn't to replay the intent log item.
<pre>
xfs_mountfs
  -> xfs_log_mount
    -> xlog_alloc_log
    -> xlog_recover
      -> xlog_do_recover
        -> xlog_do_log_recovery
          -> xlog_do_recovery_pass // analyze log record
            -> xlog_recover_process // check log record crc
              -> xlog_recover_process_data // analyze oph
                -> xlog_recover_process_ophdr
                  -> xlog_recovery_process_trans
                    -> xlog_recover_commit_trans // when get a complete transaction (ctx in xlog_cil_push)
                      -> xlog_recover_items_pass2
                        -> xlog_recover_commit_pass2
xlog_recover_rui_pass2
<font color="blue">
It allocates an in-core rui, copies the extents from the format
structure into it, and adds the rui to the AIL with the given
LSN.
</font>
xlog_recover_rud_pass2
<font color="blue">
This routine is called when an RUD format structure is found in a committed
transaction in the log. Its purpose is to cancel the corresponding RUI if it
was still in the log. To do this it searches the AIL for the RUI with an id
equal to that in the RUD format structure. If we find it we drop the RUD
reference, which removes the RUI from the AIL and frees it.
</font>


Look at xfs_defer_finish_noroll
---
    while (!list_empty(&dop_pending) || !list_empty(&(*tp)->t_dfops)) {
        /* log intents and pull in intake items */
        xfs_defer_create_intents(*tp);
        list_splice_tail_init(&(*tp)->t_dfops, &dop_pending);

        /*
         * Roll the transaction.
         */
        error = xfs_defer_trans_roll(tp);
        if (error)
            goto out;

        /* Log an intent-done item for the first pending item. */
        dfp = list_first_entry(&dop_pending, struct xfs_defer_pending,
                       dfp_list);
        dfp->dfp_done = dfp->dfp_type->create_done(*tp, dfp->dfp_intent,
                dfp->dfp_count);
        cleanup_fn = dfp->dfp_type->finish_cleanup;

        /* Finish the work items. */
        state = NULL;
        list_for_each_safe(li, n, &dfp->dfp_work) {
            list_del(li);
            dfp->dfp_count--;
            error = dfp->dfp_type->finish_item(*tp, li,
                    dfp->dfp_done, &state);
            ...
        }
            ...
    }
---

The real work of a intent log is done in dfp->dfp_type->finish_item().
<U>The logs (inode and buf items) is committed to a same transaction with the
intent log done.</U> So if the log done is there, associated log of the real work
must be there. We needn't replay the intent log any more.

</pre>
<li> If only intent log item, replay it.
<pre>
xfs_mountfs
  -> xfs_log_mount //<font color="blue">analyze log here</font>
  -> xfs_log_mount_finish
    -> xlog_recover_finish
      -> xlog_recover_process_intents
    ---
    spin_lock(&ailp->ail_lock);
    lip = xfs_trans_ail_cursor_first(ailp, &cur, 0);
    while (lip != NULL) {
        if (!xlog_item_is_intent(lip)) {
            break;
        }

        switch (lip->li_type) {
        case XFS_LI_EFI:
            error = xlog_recover_process_efi(log->l_mp, ailp, lip);
            break;
        case XFS_LI_RUI:
            error = xlog_recover_process_rui(log->l_mp, ailp, lip);
            break;
        case XFS_LI_CUI:
            error = xlog_recover_process_cui(parent_tp, ailp, lip);
            break;
        case XFS_LI_BUI:
            error = xlog_recover_process_bui(parent_tp, ailp, lip);
            break;
        }
        if (error)
            goto out;
        lip = xfs_trans_ail_cursor_next(ailp, &cur);
    }
out:
    xfs_trans_ail_cursor_done(&cur);
    spin_unlock(&ailp->ail_lock);

    ---
</pre>
</ul>
</font>
</p>





<h4><a name="relog">relog</a></h4>
<hr style="height:5px;border:none;border-top:2px solid black;" />
<p>
<font size="2">
What is relogging ?
<pre>
XFS allows multiple separate modifications to a single object to be carried
in the log at any given time. This allows the log to avoid needing to flush
each change to disk before recording a new change to the object.


    lvs on lip          take down the lvs     commit record is completed     AIL handling
    lip is pined  ->    write to iclog    ->  insert to AIL, unpin        -> if not pinned, flush to disk
                        submit to disk                                        
    (format)            (xlog_write)          (xlog_cil_committed)
                        \____________________ ______________________/
                                             V
                                      Log IO is ongoing

The relogging here is used to implement a <B>long-running, multiple-committing</B> transactions.
</pre>
The multiple-committing is very obvious here.<br/>
<B>But what is the long-running ?</B><br/>
<pre>
IMO. it should mean do logging an object repeatedly for long time.
</pre>
<U>We always need to roll the transaction with xfs_trans_roll to move log items forward
in the log to avoid to be blocked by ourselves when <font color="red">log wraps around</font>.</U>
<br/>
xfs_trans_roll commits the previous transaction to log and allocates a new one
for next.
<pre>
<U>But if we relog repeatedly, the log item will be pined again and cannot be flushed to
disk, then this log item cannot be removed from the AIL list and free the log
space. We may hang forever to wait the log space.</U>

Refer to <a href="#order_of_AIL">order of AIL</a>
No such issue. When an log item is relogged, it will get an new lsn and
repositioned on the AIL list based on this new lsn. The original log space will
be release finally.
</pre>


</font>
</p>

<h3><a name="log_miscs">miscs</a></h3>
<hr style="height:5px;border:none;border-top:2px solid black;" />
<p>
<font size="2">
<B>In one trasaction, there can only be one modification on one log item.</B>
<pre>
The one transaction here is the one xfs_cil_ctx in xlog_cil_push

It will take all of the lvs down from the log items queued on cil->xc_cil
under down_write of cil->xc_ctx_lock.

xfs_log_commit_cil
  -> xlog_cil_insert_items // under down_read of cil->xc_ctx_lock
    -> xlog_cil_insert_format_items

If the lip->lv is not NULL, format the log there. Otherwise, use the shadow lv
and this lv will be written to log in another transaction.
</pre>

<B>ctx</B>
<pre>
xlog_cil_push write the lvs to log in unit of ctx.
This ctx has a commit record.

When the iclog where the commit record of a ctx is written to log,
xlog_cil_committed will be invoked.

xlog_cil_committed
  -> xfs_trans_committed_bulk
    -> xfs_log_item_batch_insert
    ---
    for (i = 0; i < nr_items; i++) {
        struct xfs_log_item *lip = log_items[i];

        lip->li_ops->iop_unpin(lip, 0);
    }
    ---
</pre>
</font>
</p>


<h2><a name="AG_free_space_management">AG free space management</a></h2>
<hr style="height:5px;border:none;border-top:2px solid black;" />

<h3><a name="AG_Free_Space_B+trees">AG Free Space B+trees</a></h3>
<p>
<font size="2">
The XFS filesystem tracks free space in an allocation group using two B+trees. One B+tree tracks space by block
number, the second by the size of the free space block. This scheme allows XFS to <U>find quickly free space near a
given block or of a given size</U>.<br/>
All block numbers, indexes, and counts are <B>AG relative</B>.
<pre>
agf->agf_roots[] -.
                  |
                  v
           +-----------+
           |   Header  |        
           +--+--+--+--+        key:[startblock,blockcount]
           |K1|K2|K3|K4|        1:[12,16] 2:[184586,3] 3:[225579,1] 4:[511629,1]
           +--+--+--+--+        
           |P1|P2|P3|P4|        1:2 2:83347 3:6 4:4
           +--+--+--+--+        
            |   |
       .----'   '-------.  
       v                v
       2              83347
 +-----------+    +-----------+               
 |   Header  |    |   Header  |               
 +--+--+--+--+    +--+--+--+--+               
 |R0|R1|R2|R3|    |R0|R1|R2|R3|
 +--+--+--+--+    +--+--+--+--+
</pre>
</font>
</p>



<h3><a name="xfs_alloc_ag_vextent_size">xfs_alloc_ag_vextent_size</a></h3>
<p>
<font size="2">
Allocate a variable extent anywhere in the allocation group agno.<br/>
Extent's length (returned in len) will be between minlen and maxlen.<br/>
<pre>
xfs_alloc_ag_vextent_size
---
<font color="blue">
     //Allocate and initialize a cursor for the by-size btree.
</font>
    cnt_cur = xfs_allocbt_init_cursor(args->mp, args->tp, args->agbp,
        args->agno, <font color="red">XFS_BTNUM_CNT</font>);

<font color="blue">
    //Look for an entry >= maxlen+alignment-1 blocks.
    //The result will be the <U>nearest</U> one. 
</font>
    if ((error = xfs_alloc_lookup_ge(cnt_cur, 0,
            args->maxlen + args->alignment - 1, &i)))
        goto error0;

    if (!i) {
        ...
    } else {
        /*
         * Search for a non-busy extent that is large enough.
         */
        for (;;) {
            error = xfs_alloc_get_rec(cnt_cur, &fbno, &flen, &i);
            busy = xfs_alloc_compute_aligned(args, fbno, flen,
                    &rbno, &rlen, &busy_gen);
<font color="blue">
            //xfs_extent_busy_trim
            //regarding to busy extent, please refer to <a href="#Busy_extent">Busy extent</a>
</font>

            if (rlen >= args->maxlen)
                break;

<font color="blue">
            // shift to right the rec
</font>
            error = xfs_btree_increment(cnt_cur, 0, &i);
            if (i == 0) {
                /*
                 * Our only valid extents must have been busy.
                 * Make it unbusy by forcing the log out and
                 * retrying.
                 */
                xfs_btree_del_cursor(cnt_cur,
                             XFS_BTREE_NOERROR);
                trace_xfs_alloc_size_busy(args);
                xfs_extent_busy_flush(args->mp,
                            args->pag, busy_gen);
                goto restart;
            }
        }
    }
    ...
<font color="blue">
    // Allocate and initialize a cursor for the by-block tree.
</font>
    bno_cur = xfs_allocbt_init_cursor(args->mp, args->tp, args->agbp,
        args->agno, <font color="red">XFS_BTNUM_BNO</font>);
    if ((error = xfs_alloc_fixup_trees(cnt_cur, bno_cur, fbno, flen,
            rbno, rlen, XFSA_FIXUP_CNT_OK)))
        goto error0;
    xfs_btree_del_cursor(cnt_cur, XFS_BTREE_NOERROR);
    xfs_btree_del_cursor(bno_cur, XFS_BTREE_NOERROR);
    cnt_cur = bno_cur = NULL;
    args->len = rlen;
    args->agbno = rbno;
---
</pre>
</font>
</p>

<h3><a name="xfs_alloc_fixup_trees">xfs_alloc_fixup_trees</a></h3>

<p>
<font size="2">
Update the bnobt and cntbt, remove the allocated record from the two trees.<br/>
<pre>


       fb
  |---------| freespace record we get from freespace btree
  |f0|---|f1|    result record we need

The steps to fix the cntbt and bnobt:
 a. delete fb from the cntbt
 b. insert f0 and f1 into cntbt
 c. update fb to f0 (cut fb down to f0)
 d. insert f1

</pre>

</font>
</p>


<h3><a name="AG_free_list">AG free list</a></h3>
<p>
<font size="2">
The AG Free List is located in the 4th sector of each AG and is known as the AGFL. It is an array of AG relative
block pointers for reserved space for growing the free space B+trees.
<pre>

(bmap    refcount    rmap    inode)    data block
 the above 4 btrees itself need              
 block to construct the btree             
                            |  xfs_alloc_vextent
                            v
                     bnobt     cntbt
                 the above 2 btrees' block
                 are allocated from agfl
                            |  xfs_alloc_fix_freelist 
                            v
                           agfl
                     
During operations on free space btree (bnobt and cntbt), new free block maybe
needed, if no available free blocks, we may encounter deadlock, especially, free
space.

</pre>

There are two blocks associated with agfl.
<ul>
<li> agf (AG Free Space Block)
<pre>
    - agf_flfirst
    - agf_fllast
    - agf_flcount

    2nd sector of an AG.
    XFS_AGF_DADDR
</pre>
<li> agfl (AG Free list)
<pre>
    4th sector of an AG
    XFS_AGFL_DADDR
</pre>
</ul>

Two basic interfaces of agfl
<ul>
<li> xfs_alloc_get_freelist
<pre>
---
<font color="blue">
    // Get bp of agfl
</font>
    error = xfs_alloc_read_agfl(mp, tp, be32_to_cpu(agf->agf_seqno), &agflbp);
    ...
    agfl_bno = XFS_BUF_TO_AGFL_BNO(mp, agflbp);
<font color="blue">
    // Get the first free block from the agfl
</font>
    bno = be32_to_cpu(agfl_bno[be32_to_cpu(agf->agf_flfirst)]);
<font color="blue">
    // Push the ring of agfl
</font>
    be32_add_cpu(&agf->agf_flfirst, 1);
    xfs_trans_brelse(tp, agflbp);
    if (be32_to_cpu(agf->agf_flfirst) == xfs_agfl_size(mp))
        agf->agf_flfirst = 0;

    pag = xfs_perag_get(mp, be32_to_cpu(agf->agf_seqno));
    ASSERT(!pag->pagf_agflreset);
    be32_add_cpu(&agf->agf_flcount, -1);
    xfs_trans_agflist_delta(tp, -1);
    pag->pagf_flcount--;
    xfs_perag_put(pag);

    logflags = XFS_AGF_FLFIRST | XFS_AGF_FLCOUNT;
    ...
<font color="blue">
    // We only modify the agf, so just log it.
</font>
    xfs_alloc_log_agf(tp, agbp, logflags);
    *bnop = bno;
---
</pre>
<li> xfs_alloc_put_freelist
<pre>
---
<font color="blue">
    // Get bp of agfl
</font>
    if (!agflbp && (error = xfs_alloc_read_agfl(mp, tp,
            be32_to_cpu(agf->agf_seqno), &agflbp)))
        return error;
    be32_add_cpu(&agf->agf_fllast, 1);
    if (be32_to_cpu(agf->agf_fllast) == xfs_agfl_size(mp))
        agf->agf_fllast = 0;

    pag = xfs_perag_get(mp, be32_to_cpu(agf->agf_seqno));
    ASSERT(!pag->pagf_agflreset);
    be32_add_cpu(&agf->agf_flcount, 1);
    xfs_trans_agflist_delta(tp, 1);
    pag->pagf_flcount++;

    logflags = XFS_AGF_FLLAST | XFS_AGF_FLCOUNT;
    ...
    xfs_perag_put(pag);

    xfs_alloc_log_agf(tp, agbp, logflags);
<font color="blue">
    // Fill in the free block into the agfl
</font>
    agfl_bno = XFS_BUF_TO_AGFL_BNO(mp, agflbp);
    blockp = &agfl_bno[be32_to_cpu(agf->agf_fllast)];
    *blockp = cpu_to_be32(bno);
    startoff = (char *)blockp - (char *)agflbp->b_addr;

<font color="blue">
    // we need to log both agf and agfl.
</font>
    xfs_alloc_log_agf(tp, agbp, logflags);

    xfs_trans_buf_set_type(tp, agflbp, XFS_BLFT_AGFL_BUF);
    xfs_trans_log_buf(tp, agflbp, startoff,
              startoff + sizeof(xfs_agblock_t) - 1);
---
</pre>
</ul>
<br/>
<B>Is xfs_alloc_fix_freelist exclusive</B> ?
<pre>
The lock of agf buffer could ensure the exclusivity.
It will be locked by
xfs_alloc_read_agf
  -> xfs_read_agf
    -> xfs_trans_read_buf
      -> xfs_trans_read_buf_map
        -> xfs_buf_read_map
          -> xfs_buf_get_map
            -> xfs_buf_find
              -> xfs_buf_lock
<font color="blue">
Before the transaction is committed or rolled, if the bp has been in the
 transaction, it will not be locked again.
</font>
This is done by,
xfs_trans_read_buf_map
  -> xfs_trans_buf_item_match

Where to unlock ?
Every time we do allocation/free, agf, at least agf->agf_freeblks,
will be modified. So it must be logged and the lock will be freed
after log formating.

<U>Actually, the whole free space allocation and free are under the lock of
agf's bp.</U>

</pre>

<B>defer agfl block frees</B>
<pre>
f8f2835a9cf300079835e1adb1d90f85033be04c (xfs: defer agfl block frees when dfops is available)


The AGFL fixup code executes before every block allocation/free and rectifies the
AGFL based on the current, dynamic allocation requirements of the fs. The AGFL must
hold a minimum number of blocks to satisfy a worst case split of the free space
btrees caused by the impending allocation operation.

Since the AGFL caches individual blocks, AGFL reduction typically involves multiple,
single block frees. We've had reports of transaction overrun problems during certain
workloads that boil down to AGFL reduction freeing multiple blocks and consuming more
space in the log than was reserved for the transaction (note, it is tic->t_unit_res).

One way to address this problem is to release surplus blocks from the AGFL immediately
but defer the free of those blocks (similar to how file-mapped blocks are unmapped from
the file in one transaction and freed via a deferred operation) until the transaction is
rolled.

Let's look at how to achieve this,

__xfs_trans_commit
  -> xfs_defer_finish_noroll
  ---
    while (!list_empty(&dop_pending) || !list_empty(&(*tp)->t_dfops)) {
        /* log intents and pull in intake items */
        xfs_defer_create_intents(*tp);  <font color="red">[1]<B></B></font>
        list_splice_tail_init(&(*tp)->t_dfops, &dop_pending);
<font color="blue">
        // Roll the transaction.
        // The transaction will re-get a log space reservation.
</font>
        error = xfs_defer_trans_roll(tp); <font color="red"><B>[2]</B></font>
        ...
                                                    <font color="red"><B>[3]</B></font>
        dfp->dfp_done = dfp->dfp_type-><font color="red">create_done</font>(*tp, dfp->dfp_intent, dfp->dfp_count);
        cleanup_fn = dfp->dfp_type->finish_cleanup;
        /* Finish the work items. */
        state = NULL;
        list_for_each_safe(li, n, &dfp->dfp_work) {
            list_del(li);
            dfp->dfp_count--;
<font color="blue">
            // Do the real work behind the intend
</font>
                                     <font color="red"><B>[4]</B></font>
            error = dfp->dfp_type-><font color="red">finish_item</font>(*tp, li, dfp->dfp_done, &state);
            ...
        }
        ...
    }
  ---

Given there are 4 intent log item in this transaction and all of them are btree
updating. If log all of the operations in one transaction committing, the
tic->t_unit_res maybe used up. But when handle them in defer work, things are
different.

 a. create intend log for all of them.
 b. roll the trasaction
 c. create intend done log for the 1st one
 d. do the real work, such as updating btree and agf
 e. roll the transaction
 ...

We could see, every time the transaction is rolled, <U>it just commit one btree
updating operation and the log space needed is predictable.</U>
</pre>
</font>
</p>

<h3><a name="Busy_extent">Busy extent</a></h3>
<p>
<font size="2">
Busy block/extent entry.  Indexed by a rbtree in perag to mark blocks that
have been freed but whose transactions aren't committed to disk yet.<br/>
<pre>

xlog_cil_committed
---
    xfs_extent_busy_sort(&ctx->busy_extents);
    xfs_extent_busy_clear(mp, &ctx->busy_extents,
                 (mp->m_flags & XFS_MOUNT_DISCARD) && !abort);
---

</pre>

Think of following operations sequence:
<pre>
 1. free extent E_a from F_a in transaction T_a
 2. allocate extent E_a to F_b in transaction T_b again.
 3. write on the E_a of F_b. 

The issue here is: if the data written on E_a reaches disk before the log of T_a
and T_b, and power is lost at the moment. When do recovery, we cannot replay the
log of free extent E_a from F_a, but the blocks associated to extent E_a has
been corrupted.
</pre>
How does ext4 handle this ?
<pre>
ext4_free_blocks
---
<font color="red">
    /*
     * We need to make sure we don't reuse the freed block until after the
     * transaction is committed. We make an exception if the inode is to be
     * written in writeback mode since writeback mode has weak data
     * consistency guarantees.
     */
</font>
    if (ext4_handle_valid(handle) &&
        ((flags & EXT4_FREE_BLOCKS_METADATA) ||
         !ext4_should_writeback_data(inode))) {
<font color="blue">
        // if order or journal mode
</font>
        ...
        mb_clear_bits(bitmap_bh->b_data, bit, count_clusters);
        ext4_mb_free_metadata(handle, &e4b, new_entry);
        ---
            spin_lock(&sbi->s_md_lock);
<font color="red">
            list_add_tail(&new_entry->efd_list, &sbi->s_freed_data_list);
</font>
            sbi->s_mb_free_pending += clusters;
            spin_unlock(&sbi->s_md_lock);
        ---
    }
---

ext4_journal_commit_callback
 -> ext4_process_freed_data
   -> ext4_mb_load_buddy
     -> ext4_mb_load_buddy_gfp
       -> find_or_create_page
       -> ext4_mb_init_cache // if not Uptodate
<font color="blue">
        // struct inode *inode = sbi->s_buddy_cache;
</font>

jbd2_journal_commit_transaction
---
    if (journal->j_commit_callback)
        journal->j_commit_callback(journal, commit_transaction);
---
</pre>
</font>
</p>

<h2><a name="BT">B Tree</a></h2>
<hr style="height:5px;border:none;border-top:2px solid black;" />

<h3><a name="xfs_btree_ops">xfs_btree_ops</a></h3>
<p>
<font size="2">
xfs_btree_ops
<ul>
<li> init_ptr_from_cur, initialise start pointer from cursor
<pre>
xfs_btree_lookup
---
    cur->bc_ops->init_ptr_from_cur(cur, &ptr);
    pp = &ptr;
---

xfs_allocbt_init_ptr_from_cur
---
    ptr->s = agf->agf_roots[cur->bc_btnum];
<font color="blue">
    //agf here is the on-disk one.
</font>
---
</pre>
<li> init_key_from_rec, get key from the record in level 0 node.
<pre>
We need to iterate the btree and compare the keys.
If level > 0, there are specific key fields there.
But for level 0, only record there. So we need this callback to get key value
from the record.

xfs_btree_lookup
  -> xfs_lookup_get_search_key
---
    if (level == 0) {
        cur->bc_ops->init_key_from_rec(kp,
                xfs_btree_rec_addr(cur, keyno, block));
        return kp;
    }

    return xfs_btree_key_addr(cur, keyno, block);
---
</pre>
<li> key_diff, compare the key
</ul>

</font>
</p>

<h3><a name="xfs_btree_lookup">xfs_btree_lookup</a></h3>
<p>
<font size="2">
xfs_btree_lookup will iterate the btree and fill the entries fillowing:
<ul>
<li> xfs_btree_cur->bc_bufs, the bp of the block we want at every level
<li> xfs_btree_cur->bc_ptrs, the key of the entry in every level's node.
</ul>
<pre>
    /* initialise start pointer from cursor */
    cur->bc_ops->init_ptr_from_cur(cur, &ptr);
    pp = &ptr;
    for (level = cur->bc_nlevels - 1, diff = 1; level >= 0; level--) {
        error = xfs_btree_lookup_get_block(cur, level, pp, &block);
        if (diff == 0) {
            /*
             * If we already had a key match at a higher level, we
             * know we need to use the first entry in this block.
             */
            keyno = 1;
        } else {
            int    high;    /* high entry number */
            int    low;    /* low entry number */

            /* Set low and high entry numbers, 1-based. */
            low = 1;
            high = xfs_btree_get_numrecs(block);
            ...
            while (low <= high) {
                union xfs_btree_key    key;
                union xfs_btree_key    *kp;

                XFS_BTREE_STATS_INC(cur, compare);

                /* keyno is average of low and high. */
                keyno = (low + high) >> 1;

                /* Get current search key */
                kp = xfs_lookup_get_search_key(cur, level,
                        keyno, block, &key);

                /*
                 * Compute difference to get next direction:
                 *  - less than, move right
                 *  - greater than, move left
                 *  - equal, we're done
                 */
                diff = cur->bc_ops->key_diff(cur, kp);
                if (diff < 0)
                    low = keyno + 1;
                else if (diff > 0)
                    high = keyno - 1;
                else
                    break;
            }
        }
<font color="blue">
        Except for the diff==0, the search loop exit is due to low == high.

        Before that, keyno = (high + low)/2
        if diff < 0, low = keyno + 1
        if diff > 0, high = keyno -1

        We could get: low + 2 = high

        Take the bnobt as example, the final result of loop above is:
        
        KEY   low     keyno   high
        PTR   ptr_l   ptr_k   ptr_h
        BNO   bno_l   bno_k   bno_h    (start block number of the range)
                   ^        ^
                  [1]      [2]

        diff = bno_k - bno_want

        if diff > 0, bno_want is at [1], it is included in ptr_l, res = keyno - 1
        if diff < 0, bno_want is at [2], it is included in ptr_k, res = keyno
</font>
        if (level > 0) {
            if (diff > 0 && --keyno < 1)
                keyno = 1;
            ...
            cur->bc_ptrs[level] = keyno;
        }
    }
<font color="blue">
    For intermediate node, it must include the key we want.
    But for leaf node, things is different.

    low      keyno      high
         ^           ^
         '           '
      diff > 0    diff < 0

    LE    diff > 0, res = keyno - 1
          diff < 0, res = keyno
    
    EQ    diff > 0, res = keyno
          diff < 0, res = keyno + 1
          (<font color="red">EQ and GE return same records, but the stat is different</font>)
    GE    diff > 0, res = keyno
          diff < 0, res = keyno + 1
          In case of keyno + 1 may exceed current node, 
          
                    [K1]<font color="blue">[K2]</font><font color="red">[K3]</font>[K4]
                    [P1][<font color="blue">P2]</font><font color="red">[P3]</font>[P4]
                         |   |
                     .---'   '----------.
                     v                  v
              [R1][R2][R3][<font color="blue">R4]</font>   <font color="red">[R1]</font>[R2][R3][R4]
                           ^      
                           keyno
          we need invoke xfs_btree_increment to shift the path right (blue -> red)
    
</font>
    if (dir != XFS_LOOKUP_LE && diff < 0) {
        keyno++;
        xfs_btree_get_sibling(cur, block, &ptr, <font color="red">XFS_BB_RIGHTSIB</font>);
        if (dir == XFS_LOOKUP_GE &&
<font color="red">            keyno > xfs_btree_get_numrecs(block) &&</font>
            !xfs_btree_ptr_is_null(cur, &ptr)) {
            int    i;

            cur->bc_ptrs[0] = keyno;
            error = xfs_btree_increment(cur, 0, &i);
            ...
            *stat = 1;
            return 0;
        }
    } else if (dir == XFS_LOOKUP_LE && diff > 0)
        keyno--;
    cur->bc_ptrs[0] = keyno;

    /* Return if we succeeded or not. */
    if (keyno == 0 || keyno > xfs_btree_get_numrecs(block))
        *stat = 0;
    else if (dir != XFS_LOOKUP_EQ || diff == 0)
        *stat = 1;
    else
        *stat = 0;
    return 0;

</pre>
</font>
</p>


    
<h2><a name="reflink">reflink</a></h2>
<p>
<font size="2">
refcount B+ tree
<pre>
To support the sharing of file data blocks (reflink), each allocation group has
its own reference count B+ tree. 

Each record in the reference count B+tree has the following structure:
struct xfs_refcount_rec {
    __be32 rc_startblock;
    __be32 rc_blockcount;
    __be32 rc_refcount;
};

This data structure tracks reference counts for all shared physical blocks.
 - If a block is free, it will be tracked in the free space B+trees.
 - If a block is owned by a single file, it appears in neither the free space nor the reference count B+trees.
 - If a block is shared, it will appear in the reference count B+tree with a reference count >= 2.
</pre>
<B>Set reflink flag and do the map</B>
<pre>
xfs_reflink_remap_range
  -> xfs_reflink_set_inode_flag
  -> xfs_reflink_remap_blocks
    -> xfs_bmapi_read <font color="blue">//Read extent from the source file</font>
    -> xfs_reflink_remap_extent
    ---
    error = xfs_trans_alloc(mp, &M_RES(mp)->tr_write, resblks, 0, 0, &tp);

    xfs_ilock(ip, XFS_ILOCK_EXCL);
    xfs_trans_ijoin(tp, ip, 0);

    while (rlen) {
        error = __xfs_bunmapi(tp, ip, destoff, &rlen, 0, 1,
                &firstfsb, &dfops);
    
        <font color="blue">/* Update the refcount tree */</font>
        error = xfs_refcount_increase_extent(mp, &dfops, &uirec);
    
        <font color="blue">/* Map the new blocks into the data fork. */</font>
        error = xfs_bmap_map_extent(mp, &dfops, ip, &uirec);
    }

    error = xfs_trans_commit(tp);
    xfs_iunlock(ip, XFS_ILOCK_EXCL);
    
    ---
</pre>

<B>Check and break shared blocks, reserve space on COW fork</B>
<pre>
xfs_file_iomap_begin
  -> xfs_file_iomap_begin_delay
  //under XFS_ILOCK_EXCL
    -> xfs_iext_lookup_extent <font color="blue">// Look up extent in XFS_DATA_FORK. </font>
       if got.br_startoff <= offset_fsb && <font color="blue">offset_fsb is in the range of the got extent</font>
          xfs_is_reflink_inode
          -> xfs_reflink_reserve_cow
          ---
<font color="blue">
          // try to find the associated exten in the XFS_COW_FORK first.
          // if found, just trim the imap to the extent got from COW fork.
</font>
          if (!xfs_iext_lookup_extent(ip, ifp, imap->br_startoff, &icur, &got))
                eof = true;
          if (!eof && got.br_startoff <= imap->br_startoff) {
                xfs_trim_extent(imap, got.br_startoff, got.br_blockcount);
                                                                                  
                *shared = true;
                return 0;
          }
<font color="blue">
          //Find the associated extent in the refcount B+ tree.
</font>
          error = xfs_reflink_trim_around_shared(ip, imap, shared, &trimmed);
          if (error)
              return error;
                                                                                  
          /* Not shared?  Just report the (potentially capped) extent. */
          if (!*shared)
              return 0;
                                                                                  
<font color="blue">
          /*
           * Fork all the shared blocks from our write offset until the end of
           * the extent.
           */
</font>
          ...
          error = xfs_bmapi_reserve_delalloc(ip, XFS_COW_FORK, imap->br_startoff,
                  imap->br_blockcount, 0, &got, &icur, eof);
          ---
</pre>

<B>Do the real allocation</B>
<pre>
xfs_do_writepage
  -> xfs_writepage_map
---

<font color="blue">
        // If this inode is reflinked, map the extent on the extent cow fork.
        // It will firstly check whether there is reservation mapping in the cow
        // fork, if yes, allocate real space for it.
</font>
        if (xfs_is_reflink_inode(XFS_I(inode))) {
            error = xfs_map_cow(wpc, inode, offset, &new_type);
        }
        ...
        if (!wpc->imap_valid) {
            error = xfs_map_blocks(inode, offset, &wpc->imap,
                         wpc->io_type);
            ...
        }
        if (wpc->imap_valid) {
            lock_buffer(bh);
            if (wpc->io_type != XFS_IO_OVERWRITE)
                xfs_map_at_offset(inode, bh, &wpc->imap, offset);
                  -> xfs_map_buffer
                    -> bh->b_blocknr will be set
        }
    ...
    -> xfs_submit_ioend
      -> <font color="red">xfs_reflink_convert_cow</font>
      -> submit_bio
---
</pre>
</font>
</p>

       
</body>
</html>
